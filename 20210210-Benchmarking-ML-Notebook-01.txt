#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
#
# </meta:header>
#
#




Target:

	Test Forrest Classifier ML Notebook using Cinder Volumes as tmp Storage for Spark & Hadoop and with local disk 
        In this test we run two separate deploys, with the only difference being where Spark & Hadoop store tmp data
        We then compare the timings of our sample notebook for each.

 
Result:

	Ongoing test..


# ----------------------------[Test 1a: stvoutsin/issue-288  /  Cinder Volume for tmp  / 500 trees]----------------------------




# -----------------------------------------------------
# Setup an Ansible deploy 
#[user@desktop]

    cloudname=gaia-test

    sed -i '
        s/^\(AGLAIS_CLOUD\)=.*$/\1='${cloudname:?}'/
        ' "${HOME}/aglais.env"

# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name kubernator21 \
        --hostname kubernator \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/common:/common:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/openstack:/openstack:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/hadoop-yarn:/hadoop-yarn:ro,z" \
        atolmis/ansible-client:latest \
        bash




# -----------------------------------------------------
# Delete everything.
#[root@kubernator]

    /openstack/bin/delete-all.sh \
        "${cloudname:?}"

    >   ....
    >   ....


# -----------------------------------------------------
# Default locations for config and status.
#[root@kubernator]

    configyml=/tmp/aglais-config.yml
    statusyml=/tmp/aglais-status.yml


# -----------------------------------------------------
# Create our Aglais configuration.
#[root@kubernator]

cat > "${configyml:?}" << EOF
aglais:
    version: 1.0
    spec:
        openstack:
            cloudname: ${cloudname:?}
EOF


# -----------------------------------------------------
# Create everything.
# This first deploy creates a cluster that uses the Cinder Volumes for temp storage for Spark & Hadoop


#[root@kubernator]

    /hadoop-yarn/bin/create-all.sh

    >   ....
    >   ....



PLAY RECAP **************************************************************************************************************************************************************************************************
gateway                    : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master01                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker01                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker02                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker03                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker04                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker05                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker06                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
zeppelin                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   






# ------------------------------------------------------------------------------------------------------	
# Open prototype Zeppelin service and test the Forrest Classifier Notebook 
# https://github.com/wfau/aglais-testing/blob/main/notebooks/Good_astrometric_solutions_via_ML_Random_Forrest_classifier.json

# Set the numTrees to 500
# rf = RandomForestClassifier(featureSubsetStrategy = 'sqrt', featuresCol = 'features', labelCol = 'label', numTrees = 500, impurity = 'gini', seed=42)




# -----------------------------------	
# Check Spark Admin UI

# Shows 4 active worker nodes and one driver

# Active Worker nodes:

	worker02
	worker04
	worker05
	worker06

# Driver node:
  
	zeppelin



# -----------------------------------------------------
# Observe disk space usage on driver node (Zeppelin)
# (Notebook execution just started)
# fedora@zeppelin
 
ssh zeppelin


df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs         11G     0   11G   0% /dev
tmpfs            11G     0   11G   0% /dev/shm
tmpfs            11G  556K   11G   1% /run
tmpfs            11G     0   11G   0% /sys/fs/cgroup
/dev/vda1        20G  4.7G   15G  25% /
/dev/vdb        512G   17M  510G   1% /data-01
tmpfs           2.2G     0  2.2G   0% /run/user/1000
ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
ceph-fuse        10T  5.5T  4.6T  55% /user/nch
ceph-fuse       1.0T   30G  995G   3% /user/zrq
ceph-fuse       1.0T     0  1.0T   0% /user/stv



# -----------------------------------------	
# Observe disk space usage on worker node
# (Notebook execution just started)
# fedora@worker02
 

ssh worker02

df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs         11G     0   11G   0% /dev
tmpfs            11G     0   11G   0% /dev/shm
tmpfs            11G  540K   11G   1% /run
tmpfs            11G     0   11G   0% /sys/fs/cgroup
/dev/vda1        20G  3.2G   16G  17% /
/dev/vdb        512G  465M  510G   1% /data-01
tmpfs           2.2G     0  2.2G   0% /run/user/1000
ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
ceph-fuse        10T  5.5T  4.6T  55% /user/nch
ceph-fuse       1.0T   30G  995G   3% /user/zrq
ceph-fuse       1.0T     0  1.0T   0% /user/stv


# Check Admin UI again, for more info on what is happening with disk
# Disk used: 0.0 B on each executor

# .... Wait for notebook to complete

# Notebook finished successfully

# Check df again


# -----------------------------------------	
# Observe disk space usage on Zeppelin
# fedora@zeppelin

ssh zeppelin

df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs         11G     0   11G   0% /dev
tmpfs            11G     0   11G   0% /dev/shm
tmpfs            11G  588K   11G   1% /run
tmpfs            11G     0   11G   0% /sys/fs/cgroup
/dev/vda1        20G  4.8G   15G  26% /
/dev/vdb        512G   17M  510G   1% /data-01
tmpfs           2.2G     0  2.2G   0% /run/user/1000
ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
ceph-fuse        10T  5.5T  4.6T  55% /user/nch
ceph-fuse       1.0T   30G  995G   3% /user/zrq
ceph-fuse       1.0T     0  1.0T   0% /user/stv


# -----------------------------------------	
# Observe disk space usage on worker node
# fedora@worker02
 
ssh worker02

df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs         11G     0   11G   0% /dev
tmpfs            11G     0   11G   0% /dev/shm
tmpfs            11G  548K   11G   1% /run
tmpfs            11G     0   11G   0% /sys/fs/cgroup
/dev/vda1        20G  3.2G   16G  18% /
/dev/vdb        512G  802M  510G   1% /data-01
tmpfs           2.2G     0  2.2G   0% /run/user/1000
ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
ceph-fuse        10T  5.5T  4.6T  55% /user/nch
ceph-fuse       1.0T   30G  995G   3% /user/zrq
ceph-fuse       1.0T     0  1.0T   0% /user/stv





# ------------------------- Results --------------------------


# For comparison, benchmarking purposes, I'm using two cells that take the most time to complete
# Results (duration) below




	
# Cell #20201013-132418_278702125
# ------------------------------------------------------
%spark.pyspark

# clear any previously cached data in the context (cells may be executed in any order, and out-dated by changes from here onwards)
sqlContext.clearCache()

# a conservative selection of everything that COULD be within 100pc, including things with measured 
# distances putting them outside the 100pc horizon when their true distances are within, and also including 
# loads of spurious chaff with the wheat of course, plus bad things with significant, unphysical parallaxes:
raw_sources_df = spark.sql('SELECT source_id, random_index, phot_g_mean_mag, phot_bp_rp_excess_factor, bp_rp, g_rp, parallax, ra, dec, b, ' + photometric_consistency_indicators + features_select_string + ' FROM gaia_source WHERE ABS(parallax) > 8.0')

# cache it for speedy access below (all subsequent samples are derived from this):
raw_sources_df.cache()

# register as SQL-queryable
raw_sources_df.createOrReplaceTempView('raw_sources')

raw_sources_df.count()
# EDR3: 1,724,028 sources in 10min 21sec
# (cf. GCNS: 1,211,740 sources with varpi > 8mas plus 512,288 sources with varpi < -8 = 1,724,028 in total) 


	>  Took 43 mins





# Cell 20201013-152110_1282917873
# ------------------------------------------------------


%spark.pyspark

# This cell does the business, given the data and training sets. Follows the example Python code at 
# https://spark.apache.org/docs/2.4.7/api/python/pyspark.ml.html#pyspark.ml.classification.RandomForestClassifier

from pyspark.ml.classification import RandomForestClassifier

# instantiate a trained RF classifier, seeded for repeatability at this stage:
rf = RandomForestClassifier(featureSubsetStrategy = 'sqrt', featuresCol = 'features', labelCol = 'label', numTrees = 500, impurity = 'gini', seed=42)
model = rf.fit(training_df)


	> Took 17 mins









# ----------------------------[Test 1b: stvoutsin/issue-288  /  Cinder Volume for tmp  / 500 trees]----------------------------

# In notebook, change numTrees to 5000, run notebook, and observe disk usage
# instantiate a trained RF classifier, seeded for repeatability at this stage:
# Change this:  rf = RandomForestClassifier(featureSubsetStrategy = 'sqrt', featuresCol = 'features', labelCol = 'label', numTrees = 500, impurity = 'gini', seed=42)
# to this:  rf = RandomForestClassifier(featureSubsetStrategy = 'sqrt', featuresCol = 'features', labelCol = 'label', numTrees = 5000, impurity = 'gini', seed=42)

# Run..



# Observe Spark Admin UI (A few minutes after execution has started)


# In "Executors" Tab, we see the following:

# 5 Active tasks on 4 worker nodes
# 6.6 GB disk used on worker02


# In "Storage" Tab, we see the following:

# Two RDDs entry that used storage:



# RDD #1
----------
# RDD Name:
RDD Storage Info for *(1) Project [source_id#2L, random_index#3L, phot_g_mean_mag#69, phot_bp_rp_excess_factor#85, bp_rp#86, g_rp#88, parallax#9, ra#5, dec#7, b#96, ((1.15436 + (0.033772 * cast(bp_rp#86 as double))) + ((0.032277 * cast(bp_rp#86 as double)) * cast(bp_rp#86 as double))) AS fgbp_grp_0p5#198, (....

# Storage Level: 
Memory Deserialized 1x Replicated	 

# Cached Partitions:
5720

# Fraction Cached:
100%

# Size in Memory: 
290.3 MB

Size on Disk 
0.0 B


# RDD #2
----------
# RDD Name:
MapPartitionsRDD

# Storage Level: 
Disk Serialized 1x Replicated	

# Cached Partitions:
5721

# Fraction Cached:
100%

# Size in Memory: 
5.9 GB

Size on Disk 
6.1 GB





# -----------------------------------------	
# Check disk usage on worker02
# fedora@worker02

df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs         11G     0   11G   0% /dev
tmpfs            11G     0   11G   0% /dev/shm
tmpfs            11G  548K   11G   1% /run
tmpfs            11G     0   11G   0% /sys/fs/cgroup
/dev/vda1        20G  3.3G   16G  18% /
/dev/vdb        512G   20G  491G   4% /data-01
tmpfs           2.2G     0  2.2G   0% /run/user/1000
ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
ceph-fuse        10T  5.5T  4.6T  55% /user/nch
ceph-fuse       1.0T   30G  995G   3% /user/zrq
ceph-fuse       1.0T     0  1.0T   0% /user/stv



ls -al /data-01/spark/
total 16
drwxrwsr-x. 1 fedora fedora  0 Feb 10 14:35 .
drwxr-xr-x. 1 root   root   18 Feb 10 14:35 ..



ls -al /data-01/hdfs/data
total 4
drwx------. 1 fedora fedora 60 Feb 10 14:44 .
drwxrwsr-x. 1 fedora fedora  8 Feb 10 14:35 ..
drwxrwxr-x. 1 fedora fedora 90 Feb 10 14:44 current
-rw-rw-r--. 1 fedora fedora 14 Feb 10 14:44 in_use.lock
drwxr-xr-x. 1 fedora fedora 54 Feb 10 17:40 nm-local-dir



ls -al /data-01/hdfs/data/nm-local-dir/
total 0
drwxr-xr-x. 1 fedora fedora 54 Feb 10 17:46 .
drwx------. 1 fedora fedora 60 Feb 10 14:44 ..
drwxr-xr-x. 1 fedora fedora  0 Feb 10 14:44 filecache
drwx------. 1 fedora fedora 60 Feb 10 15:29 nmPrivate
drwxr-xr-x. 1 fedora fedora 12 Feb 10 15:29 usercache


du  --block-size=1G -csh  /data-01/hdfs/data/nm-local-dir/usercache/fedora/appcache/application_1612968295863_0001/ | sort -n -r | head -n 20
24G	total
24G	/data-01/hdfs/data/nm-local-dir/usercache/fedora/appcache/application_1612968295863_0001/


# So it looks like the the setting in spark is ignored here, 
# The tmp directory setting in core-site.xml of Hadoop is used to write out temp files to /data-01/hdfs/data

# If we do end up using Cinder Volumes, this will have to change to something like /data-01/hadoop/tmp to make it more clear


# Notebook still running after 1 hour.. Leave and come back to it later 



# ----------------------------[Test 2: wfau/master Local disk for tmp]-------------------------


