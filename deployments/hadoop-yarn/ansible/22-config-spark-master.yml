#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

# The Zeppelin node is acting as our Spark Master.
- name: "Configure Spark masters"
  hosts: zeppelin
  gather_facts: false
  vars_files:
    - config/ansible.yml
    - config/spark.yml
    - config/hadoop.yml
    - /tmp/ansible-vars.yml
  vars:
    sptemplink: "{{ hostvars[inventory_hostname].paths.sptemplink }}"
    sptempdest: "{{ hostvars[inventory_hostname].paths.sptempdest }}"

  tasks:

    - name: "Create Spark temp directory"
      include_tasks: "tasks/create-linked.yml"
      vars:
        linkpath: "{{sptemplink}}"
        linkdest: "{{sptempdest}}"
        linkuser: "{{spuser}}"

    #
    # Documentation
    # https://spark.apache.org/docs/3.0.0-preview2/running-on-yarn.html#configuration
    # https://spark.apache.org/docs/3.0.0-preview2/configuration.html
    # https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-2/
    #
    - name: "Configure [{{sphome}}/conf/spark-defaults.conf]"
      become: true
      blockinfile:
        path: "{{sphome}}/conf/spark-defaults.conf"
        create: true
        marker: "# {mark} Ansible managed Spark configuration"
        insertbefore: "EOF"
        block: |

            {{sparkconfig}}

            spark.local.dir            {{sptemplink}}
            spark.eventLog.dir         hdfs://{{hdhost}}:9000/spark-log
    #
    # https://spark.apache.org/docs/3.0.0-preview2/configuration.html#environment-variables
    # - name: "Update [/etc/profile.d/spark.sh]"
    #   become: true
    #   blockinfile:
    #     dest:  '/etc/profile.d/spark.sh'
    #     state: present
    #     create: yes
    #     owner: 'root'
    #     group: 'root'
    #     mode:  'u=rw,g=r,o=r'
    #     insertafter: 'EOF'
    #     marker: '# {mark} Ansible managed Hadoop config'
    #     block: |
    #       export YARN_CONF_DIR={{hdhome}}/etc/hadoop
    #       export HADOOP_CONF_DIR={{hdhome}}/etc/hadoop

    #
    # https://spark.apache.org/docs/3.0.0-preview2/configuration.html#environment-variables
    # Note: When running Spark on YARN in cluster mode, environment variables need to be set using the spark.yarn.appMasterEnv.[EnvironmentVariableName] property in your conf/spark-defaults.conf file.
    # Environment variables that are set in spark-env.sh will not be reflected in the YARN Application Master process in cluster mode.
    - name: "Configure [{{sphome}}/conf/spark-defaults.conf]"
      become: true
      blockinfile:
        path: "{{sphome}}/conf/spark-defaults.conf"
        create: true
        marker: "# {mark} Ansible managed Spark environment"
        insertbefore: "EOF"
        block: |
            # https://spark.apache.org/docs/3.0.0-preview2/configuration.html#inheriting-hadoop-cluster-configuration
            spark.yarn.appMasterEnv.YARN_CONF_DIR={{hdhome}}/etc/hadoop
            spark.yarn.appMasterEnv.HADOOP_CONF_DIR={{hdhome}}/etc/hadoop
#
# TODO History server.
# https://spark.apache.org/docs/3.0.0-preview2/monitoring.html#viewing-after-the-fact
#
# TODO Metrics.
# https://spark.apache.org/docs/3.0.0-preview2/monitoring.html#metrics
#
