#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

all:

    vars:

    # Hadoop vars

        hdname: "hadoop-3.1.3"
        hdbase: "/opt"
        hdhome: "/opt/hadoop"

        hdconf: "{{hdhome}}/etc/hadoop"
        hdhost: "master01"
        hduser: "fedora"

    # HDFS vars

        hdfsconf: "/var/hdfs/conf"
        hdfsuser: "fedora"

  # Spark vars
        spname: "spark-2.4.7"
        spfull: "spark-2.4.7-bin-hadoop2.7"
        spbase: "/opt"
        sphome: "/opt/spark"
        sphost: "master01"
        spuser: "fedora"

        sparkconfig: |

            # https://spark.apache.org/docs/latest/configuration.html
            # https://spark.apache.org/docs/latest/running-on-yarn.html
            # https://stackoverflow.com/questions/37871194/how-to-tune-spark-executor-number-cores-and-executor-memory

            spark.master                 yarn


            # https://www.c2fo.io/c2fo/spark/aws/emr/2016/07/06/apache-spark-config-cheatsheet/
            # https://github.com/AndresNamm/SparkDebugging/tree/master/ExecutorSizing

            # Amount of memory to use for the driver process (where SparkContext is initialized).
            # Zeppelin node has 45G memory
            #
            spark.driver.memory               20g
            spark.driver.memoryOverhead      5120
            spark.driver.cores                  5

            # Limit of total size of serialized results of all partitions for each Spark action.
            # Setting a proper limit can protect the driver from out-of-memory errors.
            spark.driver.maxResultSize     16g

            # Amount of memory to use for the YARN Application Master
            # (default 512m)
            #spark.yarn.am.memory              39g
            #spark.yarn.am.memoryOverhead      39g

            # Number of cores to use for the YARN Application Master in client mode.
            # (default 1)
            # spark.yarn.am.cores             1

            # The number of cores for each executor.
            # https://stackoverflow.com/a/37871195
            # 5 cores per executor is good concurrency
            spark.executor.cores            5

            # Amount of memory to use per executor process.
            # https://stackoverflow.com/a/37871195
            # 5 executors per worker
            # 45G memory per worker
            # 1G for system
            # 44/5=8.8G memory per executor
            # Overhead = 0.07 * spark.executor.memory
            # (44/5) * (1 - 0.07)
            # (44/5) * 0.93
            # 8.184G per executor
            spark.executor.memory                7g
            spark.executor.memoryOverhead      1024

            # The number of executors for static allocation.
            # https://stackoverflow.com/a/37871195
            # 27 cores per worker
            #  5 cores per executors
            # 27/5 = 5 executors per worker
            #  6 workers on our cluster
            # 6*5=30 total executors
            spark.executor.instances       30


            #spark.default.parallelism       300
            #spark.sql.shuffle.partitions    300



        yarnconfig: |

            <!--+
                | Maximum limit of memory YARN can utilize on this node
                | https://stackoverflow.com/a/43827548
                | 45G per worker
                | -1G for system
                | 44G available
                +-->
            <property>
                <name>yarn.nodemanager.resource.memory-mb</name>
                <value>45056</value>
            </property>

            <!--+
                | Maximum limit of memory to allocate to each container request at the Resource Manager.
                | https://stackoverflow.com/a/43827548
                +-->
            <property>
                <name>yarn.scheduler.maximum-allocation-mb</name>
                <value>9216</value>
            </property>

            <!--+
                | Minimum limit of memory to allocate to each container request at the Resource Manager.
                | https://stackoverflow.com/a/43827548
                +-->
            <property>
                <name>yarn.scheduler.minimum-allocation-mb</name>
                <value>1024</value>
            </property>

            <!--+
                | "the general recommendation is ... set it to be equal to the number of physical cores on the machine"
                | https://serverfault.com/q/896783
            <property>
                <name>yarn.nodemanager.resource.cpu-vcores</name>
                <value>27</value>
            </property>
                +-->

            <!--+
                | The default value for yarn.scheduler.maximum-allocation-vcores is set to twice the number of CPUs.
                | This oversubscription assumes that CPUs are not always running a thread, and hence assigning more cores enables maximum CPU utilization.
                | https://serverfault.com/a/908778
                | (total cores) * 2 * 80%
                | (6 * (27 core per vm)) * 2 * 80%
                |
                | 27 * 2 * (80/100) = 43.2
                +-->
            <property>
                <name>yarn.scheduler.maximum-allocation-vcores</name>
                <value>43</value>
            </property>

            <!--+
            <property>
                <name>yarn.scheduler.minimum-allocation-vcores</name>
                <value>1</value>
            </property>
                +-->

    # Zeppelin vars
        zepname: "zeppelin-0.8.2"
        zepbase: "/home/fedora"
        zephome: "/home/fedora/zeppelin-0.8.2-bin-all"
        zephost: "zeppelin"
        zepuser: "fedora"
        zepmavendest: "/var/local/zeppelin/maven"

    hosts:

        zeppelin:
            login:  'fedora'
            image:  'Fedora-30-1.2'
            flavor: 'gaia.cclake.27vcpu'
            discs:
              - type: 'local'
                format: 'ext4'
                mntpath: "/mnt/local/vdb"
                devname: 'vdb'
              - type: 'cinder'
                size: 1024
                format: 'btrfs'
                mntpath: "/mnt/cinder/vdc"
                devname: 'vdc'
            paths:
                # Empty on Zeppelin, master, worker
                hddatalink: "/var/hadoop/data"
                hddatadest: "/mnt/local/vdb/hadoop/data"
                # Empty on Zeppelin
                hdtemplink: "/var/hadoop/temp"
                hdtempdest: "/mnt/local/vdb/hadoop/temp"
                # Empty on Zeppelin
                hdlogslink: "/var/hadoop/logs"
                hdlogsdest: "/mnt/local/vdb/hadoop/logs"
                # Used on Zeppelin
                sptemplink: "/var/spark/temp"
                sptempdest: "/mnt/cinder/vdc/spark/temp"

        monitor:
            login:  'fedora'
            image:  'Fedora-30-1.2'
            flavor: 'gaia.cclake.2vcpu'
            discs: []

    children:

        masters:
            hosts:
                master[01:01]:
            vars:
                login:  'fedora'
                image:  'Fedora-30-1.2'
                flavor: 'gaia.cclake.2vcpu'
                discs: []
                paths:
                    # Empty on Zeppelin, master, worker
                    hddatalink: "/var/hadoop/data"
                    hddatadest: "/mnt/local/vda/hadoop/data"
                    # Used on master
                    # /var/hadoop/temp/dfs/namesecondary/current/
                    hdtemplink: "/var/hadoop/temp"
                    hdtempdest: "/mnt/local/vda/hadoop/temp"
                    # Used on master
                    hdlogslink: "/var/hadoop/logs"
                    hdlogsdest: "/mnt/local/vda/hadoop/logs"
                    # Used on master
                    # /var/hdfs/meta/namenode/fsimage/current/
                    hdfsmetalink: "/var/hdfs/meta"
                    hdfsmetadest: "/mnt/local/vda/hadoop/meta"

        workers:
            hosts:
                worker[01:06]:
            vars:
                login:  'fedora'
                image:  'Fedora-30-1.2'
                flavor: 'gaia.cclake.27vcpu'
                discs:
                  - type: 'local'
                    format: 'ext4'
                    mntpath: "/mnt/local/vdb"
                    devname: 'vdb'
                  - type: 'cinder'
                    size: 1024
                    format: 'btrfs'
                    mntpath: "/mnt/cinder/vdc"
                    devname: 'vdc'
                paths:
                    # Empty on Zeppelin, master, worker
                    hddatalink: "/var/hadoop/data"
                    hddatadest: "/mnt/local/vda/hadoop/data"
                    # Used on workers
                    # /var/hadoop/temp/nm-local-dir/
                    hdtemplink: "/var/hadoop/temp"
                    hdtempdest: "/mnt/local/vdb/hadoop/temp"
                    # Used on worker
                    hdlogslink: "/var/hadoop/logs"
                    hdlogsdest: "/mnt/local/vdb/hadoop/logs"
                    # Workers only, empty
                    hdfslogslink: "/var/hdfs/logs"
                    hdfslogsdest: "/mnt/local/vdb/hdfs/logs"
                    # Workers only, used
                    hdfsdatalink: "/var/hdfs/data"
                    hdfsdatadest: "/mnt/cinder/vdc/hdfs/data"

