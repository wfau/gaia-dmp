#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

all:

    vars:

        # Flavor sizes
        zeppelinflavor: 'gaia.cclake.13vcpu'
        masterflavor: 'gaia.cclake.2vcpu'
        workerflavor: 'gaia.cclake.13vcpu'

        # Flavour values
        zeppelinmemory: 23040
        zeppelincores: 13

        workermemory: 23040
        workercores: 13
        workercount: 4

        # Calculated limits
        spminmem: 1024
        spmaxmem: "{{workermemory - 1024}}"

        spmincores: 1
        spmaxcores: "{{workercores}}"
        sparkconfig: |

            # https://spark.apache.org/docs/latest/configuration.html
            # https://spark.apache.org/docs/latest/running-on-yarn.html
            # https://stackoverflow.com/questions/37871194/how-to-tune-spark-executor-number-cores-and-executor-memory

            spark.master                 yarn

            # Spark config settings calculated using Cheatsheet.xlsx
            # https://www.c2fo.io/img/apache-spark-config-cheatsheet/C2FO-Spark-Config-Cheatsheet.xlsx

            # https://www.c2fo.io/c2fo/spark/aws/emr/2016/07/06/apache-spark-config-cheatsheet/
            # https://github.com/AndresNamm/SparkDebugging/tree/master/ExecutorSizing

            # Calculated using Cheatsheet.xlsx
            spark.driver.memory                 18944m
            spark.driver.memoryOverhead           3072
            spark.yarn.driver.memoryOverhead      3072
            spark.driver.cores                       5
            spark.driver.maxResultSize           9216m

            spark.executor.memory                4096m
            spark.executor.memoryOverhead         1024
            spark.yarn.executor.memoryOverhead    1024
            spark.executor.cores                     3
            #spark.executor.instances                16

            spark.default.parallelism               96
            #spark.sql.shuffle.partitions           96

            # YARN Application Master settings
            spark.yarn.am.memory                 2048m
            spark.yarn.am.cores                      1

            spark.dynamicAllocation.enabled          true
            spark.shuffle.service.enabled            true
            spark.dynamicAllocation.minExecutors      1
            spark.dynamicAllocation.maxExecutors     16
            spark.dynamicAllocation.initialExecutors  8
            spark.dynamicAllocation.cachedExecutorIdleTimeout    60s
            spark.dynamicAllocation.executorIdleTimeout          60s

    hosts:

        zeppelin:
            login: 'fedora'
            image: 'Fedora-30-1.2'
            flavor: "{{zeppelinflavor}}"
            discs:
                - type: 'local'
                  format: 'ext4'
                  mntpath: "/mnt/local/vdb"
                  devname: 'vdb'
                - type: 'cinder'
                  size: 1024
                  format: 'btrfs'
                  mntpath: "/mnt/cinder/vdc"
                  devname: 'vdc'
            paths:
                # Empty on Zeppelin
                hddatalink: "/var/hadoop/data"
                hddatadest: "/mnt/local/vdb/hadoop/data"
                # Empty on Zeppelin
                hdtemplink: "/var/hadoop/temp"
                hdtempdest: "/mnt/local/vdb/hadoop/temp"
                # Empty on Zeppelin
                hdlogslink: "/var/hadoop/logs"
                hdlogsdest: "/mnt/local/vdb/hadoop/logs"
                # Used on Zeppelin
                sptemplink: "/var/spark/temp"
                sptempdest: "/mnt/cinder/vdc/spark/temp"

        monitor:
            login: 'fedora'
            image: 'Fedora-30-1.2'
            flavor: 'gaia.cclake.2vcpu'
            discs: []

    children:

        masters:
            hosts:
                master[01:01]:
            vars:
                login: 'fedora'
                image: 'Fedora-30-1.2'
                flavor: "{{masterflavor}}"
                discs: []
                paths:
                    # Empty on master
                    hddatalink: "/var/hadoop/data"
                    hddatadest: "/mnt/local/vda/hadoop/data"
                    # Used on master
                    # /var/hadoop/temp/dfs/namesecondary/current/
                    hdtemplink: "/var/hadoop/temp"
                    hdtempdest: "/mnt/local/vda/hadoop/temp"
                    # Used on master
                    hdlogslink: "/var/hadoop/logs"
                    hdlogsdest: "/mnt/local/vda/hadoop/logs"
                    # Used on master
                    # /var/hdfs/meta/namenode/fsimage/current/
                    hdfsmetalink: "/var/hdfs/meta"
                    hdfsmetadest: "/mnt/local/vda/hadoop/meta"

        workers:
            hosts:
                worker[01:04]:
            vars:
                login: 'fedora'
                image: 'Fedora-30-1.2'
                flavor: "{{workerflavor}}"
                discs:
                    - type: 'local'
                      format: 'ext4'
                      mntpath: "/mnt/local/vdb"
                      devname: 'vdb'
                    - type: 'cinder'
                      size: 1024
                      format: 'btrfs'
                      mntpath: "/mnt/cinder/vdc"
                      devname: 'vdc'
                paths:
                    # Used on workers
                    hddatalink: "/var/hadoop/data"
                    hddatadest: "/mnt/local/vdb/hadoop/data"
                    # Used on workers
                    # /var/hadoop/temp/nm-local-dir/
                    hdtemplink: "/var/hadoop/temp"
                    hdtempdest: "/mnt/local/vdb/hadoop/temp"
                    # Used on workers
                    hdlogslink: "/var/hadoop/logs"
                    hdlogsdest: "/mnt/local/vdb/hadoop/logs"
                    # Empty on workers
                    hdfslogslink: "/var/hdfs/logs"
                    hdfslogsdest: "/mnt/local/vdb/hdfs/logs"
                    # Empty on workers
                    hdfsdatalink: "/var/hdfs/data"
                    hdfsdatadest: "/mnt/cinder/vdc/hdfs/data"
