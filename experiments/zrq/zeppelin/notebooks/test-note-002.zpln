{
  "paragraphs": [
    {
      "text": "%md\n## Welcome to my world\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-07T15:31:16+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596813406501_52047383",
      "id": "paragraph_1596689529045_996230152",
      "dateCreated": "2020-08-07T15:16:46+0000",
      "dateStarted": "2020-08-07T15:31:16+0000",
      "dateFinished": "2020-08-07T15:31:16+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:3101",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Welcome to my world</h2>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%python\nprint (1 + 1)\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-07T15:31:16+0000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596813406501_476645663",
      "id": "paragraph_1596689843287_146045104",
      "dateCreated": "2020-08-07T15:16:46+0000",
      "dateStarted": "2020-08-07T15:31:16+0000",
      "dateFinished": "2020-08-07T15:31:17+0000",
      "status": "FINISHED",
      "$$hashKey": "object:3102",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "2\n"
          }
        ]
      }
    },
    {
      "text": "%spark.conf\n\nPYSPARK_PYTHON \"python2\"\nspark.pyspark.python \"python2\"\n\nspark.executor.instances 10\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-07T15:31:17+0000",
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596813406501_460964884",
      "id": "paragraph_1596710199844_1808913724",
      "dateCreated": "2020-08-07T15:16:46+0000",
      "dateStarted": "2020-08-07T15:31:17+0000",
      "dateFinished": "2020-08-07T15:31:17+0000",
      "status": "FINISHED",
      "$$hashKey": "object:3103",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "%spark\n\nval NUM_SAMPLES = 1000000\n\nval count = sc.parallelize(1 to NUM_SAMPLES).filter{\n    _ =>\n        val x = math.random\n        val y = math.random\n        x*x + y*y < 1\n    }.count()\n\nprintln(s\"Pi is roughly ${4.0 * count / NUM_SAMPLES}\")",
      "user": "anonymous",
      "dateUpdated": "2020-08-07T17:02:48+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-sjnrwq.local.zeppelin-project.org:8080/jobs/job?id=17",
              "$$hashKey": "object:4825"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596819754444_1709209169",
      "id": "paragraph_1596819754444_1709209169",
      "dateCreated": "2020-08-07T17:02:34+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:4719",
      "dateFinished": "2020-08-07T17:02:49+0000",
      "dateStarted": "2020-08-07T17:02:48+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Pi is roughly 3.142264\n\u001b[1m\u001b[34mNUM_SAMPLES\u001b[0m: \u001b[1m\u001b[32mInt\u001b[0m = 1000000\n\u001b[1m\u001b[34mcount\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 785566\n"
          }
        ]
      }
    },
    {
      "text": "%spark.pyspark\n\nimport random\nNUM_SAMPLES = 10000000\n\ndef inside(p):\n    x, y = random.random(), random.random()\n    return x*x + y*y < 1\n\ncount = sc.parallelize(\n    range(\n        0,\n        NUM_SAMPLES\n        )\n    ).filter(\n        inside\n        ).count()\n\nguess = 4.0 * count / NUM_SAMPLES\nprint(\"Pi is roughly {}\".format(guess))\n\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-07T15:31:17+0000",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-sjnrwq.local.zeppelin-project.org:8080/jobs/job?id=10",
              "$$hashKey": "object:3757"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596813406502_894937877",
      "id": "paragraph_1596689855731_210489457",
      "dateCreated": "2020-08-07T15:16:46+0000",
      "dateStarted": "2020-08-07T15:31:17+0000",
      "dateFinished": "2020-08-07T15:31:19+0000",
      "status": "FINISHED",
      "$$hashKey": "object:3104",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Pi is roughly 3.1412204\n"
          }
        ]
      }
    },
    {
      "text": "%spark.pyspark\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.endpoint\", \"cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1/AUTH_21b4ae3a2ea44bc5a9c14005ed2963af\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.path.style.access\", \"true\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.list.version\", \"2\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.access.key\", \"none\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.secret.key\", \"none\"\n    )\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-07T17:20:23+0000",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 267.15,
              "optionOpen": false
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596813406502_403380169",
      "id": "paragraph_1596692369388_1831019531",
      "dateCreated": "2020-08-07T15:16:46+0000",
      "dateStarted": "2020-08-07T17:20:23+0000",
      "dateFinished": "2020-08-07T17:20:23+0000",
      "status": "FINISHED",
      "$$hashKey": "object:3105",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "%spark.pyspark\n\ndf = sqlContext.read.parquet(\n    \"s3a://gaia-dr2-parquet/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-07T17:20:09+0000",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "//4040-spark-sjnrwq.local.zeppelin-project.org:8080/jobs/job?id=20",
              "$$hashKey": "object:5213"
            },
            {
              "jobUrl": "//4040-spark-sjnrwq.local.zeppelin-project.org:8080/jobs/job?id=21",
              "$$hashKey": "object:5214"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596813406502_956346881",
      "id": "paragraph_1596735544843_2001843771",
      "dateCreated": "2020-08-07T15:16:46+0000",
      "dateStarted": "2020-08-07T17:20:09+0000",
      "dateFinished": "2020-08-07T17:20:10+0000",
      "status": "FINISHED",
      "$$hashKey": "object:3106",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DF count:  262492\nDF partitions:  10\n"
          }
        ]
      }
    },
    {
      "text": "%spark.pyspark\n\ndf = sqlContext.read.parquet(\n    \"s3a://gaia-dr2-parquet/\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()\n",
      "user": "anonymous",
      "dateUpdated": "2020-08-07T17:20:55+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596820827213_2133669053",
      "id": "paragraph_1596820827213_2133669053",
      "dateCreated": "2020-08-07T17:20:27+0000",
      "status": "ERROR",
      "focus": true,
      "$$hashKey": "object:5265",
      "dateFinished": "2020-08-07T17:21:07+0000",
      "dateStarted": "2020-08-07T17:20:55+0000",
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Py4JJavaError: An error occurred while calling o351.parquet.\n: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://gaia-dr2-parquet/: com.amazonaws.SdkClientException: Failed to parse XML document with handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListObjectsV2Handler: Failed to parse XML document with handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListObjectsV2Handler\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:189)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2265)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2163)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2102)\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1700)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:2995)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:737)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.SdkClientException: Failed to parse XML document with handler class com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser$ListObjectsV2Handler\n\tat com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.parseXmlInputStream(XmlResponsesSaxParser.java:166)\n\tat com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.parseListObjectsV2Response(XmlResponsesSaxParser.java:339)\n\tat com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:128)\n\tat com.amazonaws.services.s3.model.transform.Unmarshallers$ListObjectsV2Unmarshaller.unmarshall(Unmarshallers.java:117)\n\tat com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:62)\n\tat com.amazonaws.services.s3.internal.S3XmlResponseHandler.handle(S3XmlResponseHandler.java:31)\n\tat com.amazonaws.http.response.AwsResponseHandlerAdapter.handle(AwsResponseHandlerAdapter.java:69)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleResponse(AmazonHttpClient.java:1726)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleSuccessResponse(AmazonHttpClient.java:1446)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1368)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5062)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5008)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5002)\n\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:941)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$5(S3AFileSystem.java:1276)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:1269)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2237)\n\t... 22 more\nCaused by: org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.\n\tat com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:203)\n\tat com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.fatalError(ErrorHandlerWrapper.java:177)\n\tat com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:400)\n\tat com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:327)\n\tat com.sun.org.apache.xerces.internal.impl.XMLScanner.reportFatalError(XMLScanner.java:1472)\n\tat com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:994)\n\tat com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:602)\n\tat com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:112)\n\tat com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:505)\n\tat com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:842)\n\tat com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:771)\n\tat com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)\n\tat com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1213)\n\tat com.amazonaws.services.s3.model.transform.XmlResponsesSaxParser.parseXmlInputStream(XmlResponsesSaxParser.java:152)\n\t... 48 more\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o351.parquet.\\n', JavaObject id=o354), <traceback object at 0x7f5f4591f878>)"
          }
        ]
      }
    },
    {
      "user": "anonymous",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1596818468456_367867782",
      "id": "paragraph_1596818468456_367867782",
      "dateCreated": "2020-08-07T16:41:08+0000",
      "status": "READY",
      "focus": true,
      "$$hashKey": "object:4153"
    }
  ],
  "name": "fredrick",
  "id": "2FFXJRSDR",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-SNAPSHOT",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {
    "isRunning": false
  },
  "path": "/fredrick"
}
