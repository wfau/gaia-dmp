#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2019, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

########################################################
## Create 3 VMs on OpenStack
########################################################

fedora@stv-aglais-master (Master Node)
fedora@stv-aglais-worker01-1 (Worker Node)
fedora@stv-aglais-worker01-2 (Worker Node)
fedora@stv-aglais-worker01-3 (Worker Node)


# To connect to nodes:	
ssh fedora@stv-aglais-master
[fedora@stv-aglais-master ~]$ 






## Do the following on each node, worker & master:


########################################################
## Fetch Hadoop Binaries on each node & install java
########################################################

sudo yum install wget
wget http://apache.cs.utah.edu/hadoop/common/current/hadoop-3.1.3.tar.gz
tar -xzf hadoop-3.1.3.tar.gz
mv hadoop-3.1.3 hadoop

sudo yum install java-1.8.0-openjdk



########################################################
## Setup Environment variables on each node
########################################################
cat > "${HOME:?}/.profile" << EOF
PATH=/home/fedora/hadoop/bin:/home/fedora/hadoop/sbin:$PATH
EOF

cat <<EOF >> "${HOME:?}/.bashrc"
export HADOOP_HOME=/home/fedora/hadoop
export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre/
EOF


########################################################
## Setup /etc/hosts file and ~/.ssh/config file
########################################################

sudo su
cat <<EOF >> "/etc/hosts"

127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

10.218.1.49       stv-aglais-master.novalocal
10.218.1.49       master
10.218.1.33       worker01
10.218.1.17       worker02
10.218.1.11       worker03


EOF
  

cat > "${HOME:?}/.ssh/config" << EOF

    Host worker01 Worker01
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host worker02 Worker02  
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host worker03 Worker03 
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host stv-aglais-master.novalocal
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

    Host master
        User fedora
        IdentityFile ~/.ssh/stv-master
        Protocol 2
        ForwardAgent yes
        PasswordAuthentication no

EOF

chmod 600 ~/.ssh/config

exit

## stv-aglais-master.novalocal was included because when starting hdfs, hadoop looks for a $(hostname) for a secondary namenode, which returns 'stv-aglais-master.novalocal'




## ---------------------------------------------------------------------- Setup Hadoop -----------------------------------------------------------------------


########################################################
## Setup Hadoop Configurations
########################################################

cat > "${HOME:?}/hadoop/etc/hadoop/core-site.xml" << EOF

<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://master:9000</value>
        </property>
    </configuration>

EOF


cat > "${HOME:?}/hadoop/etc/hadoop/yarn-site.xml" << EOF

<configuration>
    <property>
            <name>yarn.acl.enable</name>
            <value>0</value>
    </property>

    <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>master</value>
    </property>

    <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
    </property>
<property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>1536</value>
</property>

<property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>1536</value>
</property>

<property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>128</value>
</property>

<property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
</property>
<property>
   <name>yarn.scheduler.capacity.root.support.user-limit-factor</name>  
   <value>2</value>
</property>
<property>
   <name>yarn.nodemanager.disk-health-checker.min-healthy-disks</name>
   <value>0.0</value>
</property>
<property>
   <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
   <value>100.0</value>
</property>

 <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>master:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address</name>
    <value>master:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>master:8088</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>master:8031</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>master:8033</value>
  </property>

</configuration>

EOF




cat > "${HOME:?}/hadoop/etc/hadoop/workers" << EOF

worker01
worker02
worker03

EOF


cat > "${HOME:?}/hadoop/etc/hadoop/mapred-site.xml" << EOF

<configuration>
    <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
    </property>
    <property>
            <name>yarn.app.mapreduce.am.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
            <name>mapreduce.map.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
            <name>mapreduce.reduce.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
<property>
	<name>yarn.app.mapreduce.am.resource.mb</name>
        <value>512</value>
</property>

<property>
	<name>mapreduce.map.memory.mb</name>
        <value>256</value>
</property>

<property>
	<name>mapreduce.reduce.memory.mb</name>
        <value>256</value>
</property>
</configuration>

EOF




cat > "${HOME:?}/hadoop/etc/hadoop/core-site.xml" << EOF

<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://master:9000</value>
        </property>
    </configuration>

EOF

## From master node
## Copy Configuration files to Worker nodes

ssh fedora@stv-aglais-master

for node in worker01 worker02 worker03; do
    scp ~/hadoop/etc/hadoop/* $node:/home/fedora/hadoop/etc/hadoop/;
done



################################################
## Format HDFS Master Node on each node
################################################

sudo mkdir /home/hadoop/
sudo chown -R fedora:root /home/hadoop/
source /home/fedora/hadoop/bin/hdfs namenode -format



################################################
## Firewall
################################################

## On all nodes:
sudo su

    yum install firewalld

## On Worker Nodes:

sudo su
	systemctl start firewalld
	firewall-cmd --add-port=9000/tcp --permanent 
	firewall-cmd --add-port=9000/udp --permanent
	firewall-cmd --add-port=2377/tcp --permanent
	firewall-cmd --add-port=2377/tcp --permanent
	firewall-cmd --add-port=7946/tcp --permanent
	firewall-cmd --add-port=4789/tcp --permanent
	firewall-cmd --add-port=4789/udp --permanent
	firewall-cmd --add-port=9000/tcp --permanent
	firewall-cmd --add-port=9000/udp --permanent
	firewall-cmd --add-port=9866/tcp --permanent
	firewall-cmd --add-port=9866/udp --permanent
	firewall-cmd --add-port=8030/tcp --permanent
	firewall-cmd --add-port=8030/udp --permanent
	firewall-cmd --add-port=8031/tcp --permanent
	firewall-cmd --add-port=8031/udp --permanent
	firewall-cmd --add-port=8032/tcp --permanent
	firewall-cmd --add-port=8032/udp --permanent
	firewall-cmd --add-port=8033/tcp --permanent
	firewall-cmd --add-port=8033/udp --permanent
	firewall-cmd --add-port=8042/tcp --permanent
	firewall-cmd --add-port=8042/udp --permanent
	firewall-cmd --add-port=8050/tcp --permanent
	firewall-cmd --add-port=8050/udp --permanent
	firewall-cmd --add-port=8080/tcp --permanent
	firewall-cmd --add-port=8080/udp --permanent
        firewall-cmd --reload

exit

## This might not be needed
## We can probably also setup rules in Openstack



################################################
## Start and Stop HDFS
################################################

## On Master Node:
start-dfs.sh

# stop-dfs.sh

## Create a directory
hdfs dfs -mkdir /hadoop/


## LS directory
hdfs dfs -ls /hadoop/




################################################
## Monitoring HDFS
################################################ 

## Get HDFS Report
hdfs dfsadmin -report

Name: 10.218.1.11:9866 (worker03)
Hostname: worker03
Decommission Status : Normal
Configured Capacity: 23183421440 (21.59 GB)
DFS Used: 28672 (28 KB)
Non DFS Used: 2726359040 (2.54 GB)
DFS Remaining: 19452526592 (18.12 GB)
DFS Used%: 0.00%
DFS Remaining%: 83.91%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Fri Nov 01 18:57:54 UTC 2019
Last Block Report: Fri Nov 01 18:56:06 UTC 2019
Num of Blocks: 0


Name: 10.218.1.17:9866 (worker02)
Hostname: worker02
Decommission Status : Normal
Configured Capacity: 23183421440 (21.59 GB)
DFS Used: 28672 (28 KB)
Non DFS Used: 2725961728 (2.54 GB)
DFS Remaining: 19452923904 (18.12 GB)
DFS Used%: 0.00%
DFS Remaining%: 83.91%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Fri Nov 01 18:57:52 UTC 2019
Last Block Report: Fri Nov 01 18:56:07 UTC 2019
Num of Blocks: 0


Name: 10.218.1.33:9866 (worker01)
Hostname: worker01
Decommission Status : Normal
Configured Capacity: 23183421440 (21.59 GB)
DFS Used: 28672 (28 KB)
Non DFS Used: 2726342656 (2.54 GB)
DFS Remaining: 19452542976 (18.12 GB)
DFS Used%: 0.00%
DFS Remaining%: 83.91%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Fri Nov 01 18:57:52 UTC 2019
Last Block Report: Fri Nov 01 18:56:07 UTC 2019
Num of Blocks: 0



## Check HDFS GUI

ssh -L  '*:8084:stv-aglais-master:9870' fedora@stv-aglais-master

## channel 3: open failed: connect failed: No route to host

...


## ---------------------------------------------------------------------- Setup Yarn -----------------------------------------------------------------------


## "HDFS is a distributed storage system, and doesnâ€™t provide any services for running and scheduling tasks in the cluster. This is the role of the YARN framework"


################################################
## Start and Stop Yarn
################################################

start-yarn.sh

## stop-yarn.sh

# Starting resourcemanager
# Starting nodemanagers


yarn node -list
2019-11-01 20:08:32,393 INFO client.RMProxy: Connecting to ResourceManager at master/10.218.1.49:8032
Total Nodes:3
         Node-Id	     Node-State	Node-Http-Address	Number-of-Running-Containers
  worker01:38081	        RUNNING	    worker01:8042	                           0
  worker02:41569	        RUNNING	    worker02:8042	                           0
  worker03:44661	        RUNNING	    worker03:8042	                           0


## Submit a sample job

cd /home/hadoop/

hdfs dfs -mkdir /hadoop/books

wget -O alice.txt https://www.gutenberg.org/files/11/11-0.txt
wget -O holmes.txt https://www.gutenberg.org/files/1661/1661-0.txt
wget -O frankenstein.txt https://www.gutenberg.org/files/84/84-0.txt

hdfs dfs -put alice.txt holmes.txt frankenstein.txt /hadoop/books

yarn jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount "/hadoop/books/*" output

..
3891_0001_000002. Got exception: java.net.NoRouteToHostException: No Route to Host from  stv-aglais-master.novalocal/10.218.1.49 to worker01:46681 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost
	at sun.reflect.GeneratedConstructorAccessor37.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy84.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:128)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy85.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:126)
	at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:311)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 19 more
. Failing the application.




## No route to host

## After disabling firewall 
sudo systemctl stop firewalld

yarn jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount "/hadoop/books/*" output


2019-11-01 21:35:22,469 INFO client.RMProxy: Connecting to ResourceManager at master/10.218.1.49:8032
2019-11-01 21:35:22,744 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/fedora/.staging/job_1572643463891_0002
2019-11-01 21:35:22,819 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2019-11-01 21:35:22,956 INFO input.FileInputFormat: Total input files to process : 3
2019-11-01 21:35:23,002 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2019-11-01 21:35:23,037 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2019-11-01 21:35:23,057 INFO mapreduce.JobSubmitter: number of splits:3
2019-11-01 21:35:23,139 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2019-11-01 21:35:23,165 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1572643463891_0002
2019-11-01 21:35:23,165 INFO mapreduce.JobSubmitter: Executing with tokens: []
2019-11-01 21:35:23,285 INFO conf.Configuration: resource-types.xml not found
2019-11-01 21:35:23,285 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2019-11-01 21:35:23,331 INFO impl.YarnClientImpl: Submitted application application_1572643463891_0002
2019-11-01 21:35:23,361 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1572643463891_0002/
2019-11-01 21:35:23,362 INFO mapreduce.Job: Running job: job_1572643463891_0002
2019-11-01 21:35:30,427 INFO mapreduce.Job: Job job_1572643463891_0002 running in uber mode : false
2019-11-01 21:35:30,428 INFO mapreduce.Job:  map 0% reduce 0%
2019-11-01 21:35:36,479 INFO mapreduce.Job:  map 100% reduce 0%
2019-11-01 21:35:41,508 INFO mapreduce.Job:  map 100% reduce 100%
2019-11-01 21:35:41,515 INFO mapreduce.Job: Job job_1572643463891_0002 completed successfully
2019-11-01 21:35:41,598 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=486417
		FILE: Number of bytes written=1843671
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1232492
		HDFS: Number of bytes written=274568
		HDFS: Number of read operations=14
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=3
		Launched reduce tasks=1
		Data-local map tasks=3
		Total time spent by all maps in occupied slots (ms)=21250
		Total time spent by all reduces in occupied slots (ms)=3634
		Total time spent by all map tasks (ms)=10625
		Total time spent by all reduce tasks (ms)=1817
		Total vcore-milliseconds taken by all map tasks=10625
		Total vcore-milliseconds taken by all reduce tasks=1817
		Total megabyte-milliseconds taken by all map tasks=2720000
		Total megabyte-milliseconds taken by all reduce tasks=465152
	Map-Reduce Framework
		Map input records=23880
		Map output records=215164
		Map output bytes=2060450
		Map output materialized bytes=486429
		Input split bytes=326
		Combine input records=215164
		Combine output records=33442
		Reduce input groups=25011
		Reduce shuffle bytes=486429
		Reduce input records=33442
		Reduce output records=25011
		Spilled Records=66884
		Shuffled Maps =3
		Failed Shuffles=0
		Merged Map outputs=3
		GC time elapsed (ms)=325
		CPU time spent (ms)=3690
		Physical memory (bytes) snapshot=1038376960
		Virtual memory (bytes) snapshot=7710470144
		Total committed heap usage (bytes)=743964672
		Peak Map Physical memory (bytes)=289021952
		Peak Map Virtual memory (bytes)=1927327744
		Peak Reduce Physical memory (bytes)=180785152
		Peak Reduce Virtual memory (bytes)=1932062720
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1232166
	File Output Format Counters 
		Bytes Written=274568



## Job Completed Successfully

## Check Results

hdfs dfs -ls output

hdfs dfs -cat output/part-r-00000 | less
