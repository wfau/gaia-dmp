#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2019, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

# -----------------------------------------------------
# Create our cloud YAML file.
#[user@desktop]

cat > "${HOME}/clouds.yaml" << EOF

clouds:

  gaia-test:
    auth:
      auth_url: https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
      application_credential_id:     '$(secret 'stv-gaia-test.CREDENTIAL_ID')'
      application_credential_secret: '$(secret 'stv-gaia-test.CREDENTIAL_SECRET')'
    project_name: 'iris-gaia-test'
    project_domain_id: 'default'
    project_domain_name: 'default'
    region_name: "RegionOne"
    interface: "public"
    identity_api_version: 3
    auth_type: "v3applicationcredential"

  gaia-test-super:
    auth:
      auth_url: https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
      application_credential_id:     '$(secret 'stv-gaia-test-super.CREDENTIAL_ID')'
      application_credential_secret: '$(secret 'stv-gaia-test-super.CREDENTIAL_SECRET')'
    region_name: "RegionOne"
    interface: "public"
    identity_api_version: 3
    auth_type: "v3applicationcredential"

EOF



# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    docker run \
        --rm \
        --tty \
        --user "$(id -u)" \
        --interactive \
        --hostname openstacker \
        --env SSH_AUTH_SOCK=/tmp/ssh_auth_sock \
        --volume "${SSH_AUTH_SOCK}:/tmp/ssh_auth_sock" \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml" \
        atolmis/openstack-client \
        bash


# -----------------------------------------------------
# Create a Gateway node
#[user@openstacker]


# Name: k8s-test-gateway
# Create Security Group:
# Enable ssh + port 80



# -----------------------------------------------------
# List our user's keypairs.
#[user@openstacker]



    openstack \
         --os-cloud gaia-test \
         keypair list


+---------------+-------------------------------------------------+
| Name          | Fingerprint                                     |
+---------------+-------------------------------------------------+
| multi-user    | ce:c9:73:42:19:90:68:d9:a0:07:8b:43:d8:92:94:ea |
| multiple-keys | 68:33:1e:9d:d7:46:20:8d:5a:04:e8:3c:19:b4:00:b4 |
| stv-master    | 7c:9f:16:02:3f:21:7f:48:67:41:32:8f:e5:91:b2:dd |
| STV-RSA       | 3c:49:7e:aa:e1:bc:90:8c:35:52:c6:cb:d2:3b:f9:43 |
+---------------+-------------------------------------------------+




# -----------------------------------------------------
# Get the name of our first keypair.
#[user@openstacker]

keyname=$(
  openstack \
    --os-cloud gaia-test \
    keypair list \
        --format json \
| jq -r '.[2] | .Name'
)

echo "Key name [${keyname:?}]"

Key name [stv-master]



# -----------------------------------------------------
# List the available flavours.
#[user@openstacker]

    openstack \
        --os-cloud gaia-test \
        flavor list


+--------------------------------------+-------------------+--------+------+-----------+-------+-----------+
| ID                                   | Name              |    RAM | Disk | Ephemeral | VCPUs | Is Public |
+--------------------------------------+-------------------+--------+------+-----------+-------+-----------+
| 20061eba-9e88-494c-95a3-41ed77721244 | general.v1.small  |  22528 |   20 |         0 |     6 | True      |
| 406a17e0-afd0-47d3-a6ad-8b19198bdd97 | general.v1.tiny   |   6144 |   12 |         0 |     2 | True      |
| 8a821ef8-20b8-4bbb-990b-91198745e7a7 | general.v1.xlarge | 184320 |   20 |       340 |    28 | True      |
| 996c1c8c-c934-411c-9631-b74eb2829631 | general.v1.medium |  46080 |   20 |        60 |    14 | True      |
| c4c07f5a-260a-4f22-9530-a09a19aa490a | general.v1.large  |  92160 |   20 |       160 |    28 | True      |
+--------------------------------------+-------------------+--------+------+-----------+-------+-----------+




# -----------------------------------------------------
# Get the ID of the small flavor.
#[user@openstacker]

    flavorid=$(
        openstack \
            --os-cloud gaia-test \
            flavor list \
                --format json \
        | jq -r '.[] | select(.Name == "general.v1.small") | .ID'
        )

    echo "Flavor ID [${flavorid:?}]"

    >   Flavor ID [20061eba-9e88-494c-95a3-41ed77721244]


# -----------------------------------------------------
# List the available cluster templates.
#[user@openstacker]

    openstack \
        --os-cloud gaia-test \
        coe cluster template list

	+--------------------------------------+----------------------------+
	| uuid                                 | name                       |
	+--------------------------------------+----------------------------+
	| 1b81ec4d-9545-4ada-b6ec-23469550910a | kubernetes-1.14.6-20200203 |
	| 40963ffb-4439-49f8-8e80-f511fc11c4a9 | kubernetes-1.17.2-20200205 |
	| 4d2d0d0a-5925-4744-8451-fa8edfa5d275 | kubernetes-1.16.6-20200205 |
	| d54167d9-495f-437e-88fe-d182b2a230ea | kubernetes-1.15.9-20200205 |
	+--------------------------------------+----------------------------+



# -----------------------------------------------------
# Get the uuid for the Octavia template.
#[user@openstacker]

    templateuuid=$(
        openstack \
            --os-cloud gaia-test \
            coe cluster template list \
                --format json \
        | jq -r '.[] | select(.name | test("kubernetes-1.17.2-20200205")) | .uuid'
        )

    echo "Template uuid [${templateuuid:?}]"

    > Template uuid [40963ffb-4439-49f8-8e80-f511fc11c4a9]




# -----------------------------------------------------
# Create a new cluster, using unrestricted credentials.
#[user@openstacker]

    clustername=k8s_test_1

    openstack \
        --os-cloud gaia-test-super \
        coe cluster create \
            --keypair "${keyname:?}" \
            --flavor  "${flavorid:?}" \
            --node-count 4 \
            --master-count 2 \
            --master-flavor "${flavorid:?}" \
            --cluster-template "${templateuuid:?}" \
            "${clustername:?}"

    > Request to create cluster 09b00f22-c985-412b-a75c-59aecb4c3bac accepted



# -----------------------------------------------------
# List our clusters.
#[user@openstacker]

    openstack \
        --os-cloud gaia-test \
        coe cluster list

+--------------------------------------+------------+------------+------------+--------------+-----------------+
| uuid                                 | name       | keypair    | node_count | master_count | status          |
+--------------------------------------+------------+------------+------------+--------------+-----------------+
| 09b00f22-c985-412b-a75c-59aecb4c3bac | k8s_test_1 | stv-master |          4 |            2 | CREATE_COMPLETE |
+--------------------------------------+------------+------------+------------+--------------+-----------------+



# -----------------------------------------------------
# Get the details of our cluster.
#[user@openstacker]

    clusteruuid=$(
        openstack \
            --os-cloud gaia-test \
            coe cluster list \
                --format json \
        | jq -r '.[] | select(.name == "'${clustername}'") | .uuid'
        )

    echo "Cluster uuid [${clusteruuid:?}]"

    > Cluster uuid [09b00f22-c985-412b-a75c-59aecb4c3bac]


    openstack \
        --os-cloud gaia-test \
        coe cluster show \
            "${clusteruuid}"


+---------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Field               | Value                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
+---------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| status              | CREATE_COMPLETE                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| cluster_template_id | 40963ffb-4439-49f8-8e80-f511fc11c4a9                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| node_addresses      | ['10.0.0.15', '10.0.0.4', '10.0.0.29', '10.0.0.18']                                                                                                                                                                                                                                                                                                                                                                                                                      |
| uuid                | 09b00f22-c985-412b-a75c-59aecb4c3bac                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| stack_id            | 106580d0-1118-4d1b-9a29-2e4c037f9a2a                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| status_reason       | None                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| created_at          | 2020-04-08T20:09:43+00:00                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| updated_at          | 2020-04-08T20:17:19+00:00                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| coe_version         | v1.17.2                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| labels              | {'auto_healing_controller': 'magnum-auto-healer', 'max_node_count': '4', 'cloud_provider_tag': 'v1.17.0', 'etcd_tag': '3.3.17', 'monitoring_enabled': 'true', 'tiller_enabled': 'true', 'autoscaler_tag': 'v1.15.2', 'master_lb_floating_ip_enabled': 'true', 'auto_scaling_enabled': 'true', 'tiller_tag': 'v2.16.1', 'use_podman': 'true', 'auto_healing_enabled': 'true', 'heat_container_agent_tag': 'train-stable-1', 'kube_tag': 'v1.17.2', 'min_node_count': '1'} |
| faults              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| keypair             | stv-master                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| api_address         | https://128.232.227.138:6443                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| master_addresses    | ['10.0.0.23', '10.0.0.16']                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| create_timeout      | 60                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| node_count          | 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| discovery_url       | https://discovery.etcd.io/948ecd2fcebd3bd05622ac980a5b5b8e                                                                                                                                                                                                                                                                                                                                                                                                               |
| master_count        | 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
## [Admin Node]  Create storage Class
##---------------------------------------------------------

## From the Admin node
## Create default storage class with cinder:

	kind: StorageClass
	apiVersion: storage.k8s.io/v1
	metadata:
	  name: standard
	  annotations:
	    storageclass.kubernetes.io/is-default-class: "true"
	provisioner: kubernetes.io/cinder
	parameters:
	  availability: nova



kubectl --kubeconfig ~/.kube/config apply -f sc.yml

| container_version   | 1.12.6                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| name                | k8s_test_1                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| master_flavor_id    | 20061eba-9e88-494c-95a3-41ed77721244                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| flavor_id           | 20061eba-9e88-494c-95a3-41ed77721244                                                                                                                                                                                                                                                                                                                                                                                                                                     |
+---------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+



# -----------------------------------------------------
# Install some packages
#[user@k8s-master]


sudo rpm-ostree install nano
sudo rpm-ostree install wget
sudo rpm-ostree install git



# -----------------------------------------------------
# Create Admin user
#[user@k8s-master]


cat > "${HOME}/dashboard-adminuser.yaml" << EOF

	apiVersion: rbac.authorization.k8s.io/v1
	kind: ClusterRoleBinding
	metadata:
	  name: admin-user
	roleRef:
	  apiGroup: rbac.authorization.k8s.io
	  kind: ClusterRole
	  name: cluster-admin
	subjects:
	- kind: ServiceAccount
	  name: admin-user
	  namespace: kubernetes-dashboard

EOF

kubectl apply -f dashboard-adminuser.yaml




# -----------------------------------------------------
# Create Storage class
#[user@k8s-master]

## From the Admin node
## Create default storage class with cinder:

cat > "${HOME}/sc.yml" << EOF

	kind: StorageClass
	apiVersion: storage.k8s.io/v1
	metadata:
	  name: standard
	  annotations:
	    storageclass.kubernetes.io/is-default-class: "true"
	provisioner: kubernetes.io/cinder
	parameters:
	  availability: nova

EOF

kubectl apply -f sc.yml

> storageclass.storage.k8s.io/standard created


# -----------------------------------------------------
# Deploy K8s Dashboard
#[user@k8s-master]
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta6/aio/deploy/recommended.yaml

     >	namespace/kubernetes-dashboard created
	serviceaccount/kubernetes-dashboard created
	service/kubernetes-dashboard created
	secret/kubernetes-dashboard-certs created
	secret/kubernetes-dashboard-csrf created
	secret/kubernetes-dashboard-key-holder created
	configmap/kubernetes-dashboard-settings created
	role.rbac.authorization.k8s.io/kubernetes-dashboard created
	clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
	rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
	clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
	deployment.apps/kubernetes-dashboard created
	service/dashboard-metrics-scraper created
	deployment.apps/dashboard-metrics-scraper created



# -----------------------------------------------------
# Start proxy
#[user@k8s-master]
# (Need to leave terminal open for this)
kubectl proxy


# -----------------------------------------------------
# Get secret token
#[user@k8s-master]
kubectl -n kube-system describe secret


..
	Name:         admin-token-jnn8k
	Namespace:    kube-system
	Labels:       <none>
	Annotations:  kubernetes.io/service-account.name: admin
		      kubernetes.io/service-account.uid: 5de34e1b-1cfb-4ba1-a2d1-7b4ef4bff48d

	Type:  kubernetes.io/service-account-token

	Data
	====
	ca.crt:     1050 bytes
	namespace:  11 bytes
	token:      ...eyJhbGciOiJSUzJ6fGhQ


..

## Copy admin account secret and use as token for the GUI




# -----------------------------------------------------
# Port forward 8001 to 8001 on localhost (K8s Dashboard)
#[user@local]
ssh -L '8001:127.0.0.1:8001' k8s-test-1-2zuxnqa5tj2u-master-0



# -----------------------------------------------------
# Open Dashboard in browser
#[user@local]
   
    http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/overview?namespace=default



# -----------------------------------------------------
# Get Spark on Kubernetes Tutorial Github project
# https://github.com/devshlabs/spark-kubernetes
#
#[user@local]

git clone https://github.com/devshlabs/spark-kubernetes


# -----------------------------------------------------
# Build Docker Image for Spark
#
#[user@local]
pushd spark-kubernetes/spark-container
    sudo docker build . -t stvoutsin/spark:2.4.4
popd



# -----------------------------------------------------
# Get Spark on Kubernetes Tutorial Github project on Master node
# https://github.com/devshlabs/spark-kubernetes
#
#[user@@k8s-master]

git clone https://github.com/devshlabs/spark-kubernetes



# -----------------------------------------------------
# Configure Spark/Kubernetes image 
# In this example, we set it to stvoutsin/spark:2.4.4
#[user@@k8s-master]

nano kubernetes/spark-worker-controller.yaml
..

    spec:
      containers:
        - name: spark-worker
          image: stvoutsin/spark:2.4.4
..


nano kubernetes/spark-master-controller.yaml

..
      containers:
        - name: spark-master
          image: stvoutsin/spark:2.4.4

..


# -----------------------------------------------------
# Create Spark K8s Pods
#
#[user@k8s-master]

pushd spark-kubernetes/kubernetes
    kubectl create -f spark-master-controller.yaml
    kubectl create -f spark-master-service.yaml
    kubectl create -f spark-worker-controller.yaml
    kubectl get all

      >	NAME                                READY   STATUS    RESTARTS   AGE
	pod/spark-master-controller-l96t5   1/1     Running   0          3m38s
	pod/spark-worker-controller-4wmtp   1/1     Running   0          3m38s
	pod/spark-worker-controller-g8skg   1/1     Running   0          3m38s

	NAME                                            DESIRED   CURRENT   READY   AGE
	replicationcontroller/spark-master-controller   1         1         1       3m44s
	replicationcontroller/spark-worker-controller   2         2         2       3m44s

	NAME                            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
	service/kubernetes              ClusterIP   10.254.0.1       <none>        443/TCP             44h
	service/spark-master            ClusterIP   10.254.177.162   <none>        7077/TCP,8080/TCP   3m44s
	service/spark-master-headless   ClusterIP   None             <none>        <none>              3m44s


popd



# -----------------------------------------------------
# Run PySpark and sample job
#
#[user@k8s-master]


kubectl exec -it spark-master-controller-v2hjb /bin/bash

pyspark

Python 2.7.9 (default, Jun 29 2016, 13:08:31) 
[GCC 4.9.2] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.4
      /_/

Using Python version 2.7.9 (default, Jun 29 2016 13:08:31)
SparkSession available as 'spark'.


>>> from pyspark import SparkContext
>>> def inside(p):
...     x, y = random.random(), random.random()
...     return x*x + y*y < 1
... 
>>> NUM_SAMPLES = 1000000
>>> count = sc.parallelize(range(0, NUM_SAMPLES)) \
...              .filter(inside).count()
20/04/14 18:25:33 WARN scheduler.TaskSetManager: Stage 0 contains a task of very large size (286 KB). The maximum recommended task size is 100 KB.
>>>                                                                             
>>> 
>>> print("Pi is roughly %f" % (4.0 * count / NUM_SAMPLES))
Pi is roughly 3.141584
>>> 


## Seems to have worked, but very limited output??
## We need to check what actually ran and where? Was a Spark job submitted to the worker node?


