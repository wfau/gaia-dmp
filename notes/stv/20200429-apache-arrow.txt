#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


# Experimenting with Apache Arrow
# -------------------------------

# Motivation for this is that converting a large Dataframe into a Pandas DF is very slow in the current system, and will occasionally fall over due to memory overflow exceptions
# The following snippet is where the conversion to Pandas fails:
 

%spark.pyspark

import pandas as pd
import numpy as np

df = sqlContext.read.parquet(
    "/hadoop/gaia/parquet/gdr2/gaia_source/*.parquet"
    ).select(
    ["designation","source_id","ra","ra_error","dec","dec_error","parallax","parallax_error","parallax_over_error","pmra","pmra_error","pmdec","pmdec_error","l","b"]
    ).where(
    "abs(b) < 30.0 AND parallax > 1.0 and parallax_over_error > 10.0 AND phot_g_mean_flux_over_error > 36.19 AND astrometric_sigma5d_max < 0.3 AND visibility_periods_used > 8 AND (astrometric_excess_noise < 1 OR (astrometric_excess_noise > 1 AND astrometric_excess_noise_sig < 2))"
    )

pandas_df = df.select("*").toPandas()
pandas_df.head()


# Exception triggered:

     >  Job aborted due to stage failure: Task 3901 in stage 15.0 failed 4 times, most recent failure: Lost task 3901.3 in stage 15.0 (TID 41499, stv-dev-worker-8, executor 266): TaskResultLost (result lost from block manager)



# Let's try to see if we can improve the performance with Apache Arrow



# Installing Apache Arrow
# -------------------------------


https://docs.databricks.com/spark/latest/spark-sql/spark-pandas.html
https://arrow.apache.org/install/


Apache Arrow is an in-memory columnar data format used in Apache Spark to efficiently transfer data between JVM and Python processes. This is beneficial to Python developers that work with pandas and NumPy data. However, its usage is not automatic and requires some minor changes to configuration or code to take full advantage and ensure compatibility.

Convert Spark DataFrames to and from pandas DataFrames
Arrow is available as an optimization when converting a Spark DataFrame to a pandas DataFrame using the call toPandas() and when creating a Spark DataFrame from a pandas DataFrame with createDataFrame(pandas_df). To use Arrow when executing these calls, set the Spark configuration spark.sql.execution.arrow.enabled to true. This configuration is disabled by default.



# Repeat on each worker node
# user@worker
# -------------------------------

pip install pyarrow==0.17.*


# Try the following Spark job (Via Zeppelin/Pyspark)
# user@worker
# -------------------------------


%spark.pyspark
import pandas as pd
import numpy as np

# Enable Arrow-based columnar data transfers
spark.conf.set("spark.sql.execution.arrow.enabled", "true")

# Generate a pandas DataFrame
pdf = pd.DataFrame(np.random.rand(100, 3))

# Create a Spark DataFrame from a pandas DataFrame using Arrow
df = spark.createDataFrame(pdf)

# Convert the Spark DataFrame back to a pandas DataFrame using Arrow
result_pdf = df.select("*").toPandas()


/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:
  'JavaPackage' object is not callable
Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.



# After repeating the cell run, the error disappears..(!?)



# Selecting the above gaia query dataframe into a pandas object
# user@worker
# -------------------------------
# Let's Try running the following Zeppelin cell from the notebook example:

%spark.pyspark
#df = pd.DataFrame(df, columns=df.columns)
#print(type(df))
#df.head()
# ... doesn't work for me for the system install of pandas. Try this:
pandas_df = df.select("*").toPandas()
print(type(pandas_df))
pandas_df.head()



	
