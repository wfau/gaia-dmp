#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#



# -------------------------------------------#
# Spark Fine tuning   -                      #
# Understanding Spark Concepts               #
# -------------------------------------------#

https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-1/
https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-2/



# Jobs, Tasks, Stages
# ---------------------------------------
At the top of the execution hierarchy are jobs. Invoking an action inside a Spark application triggers the launch of a Spark job to fulfill it. To decide what this job looks like, Spark examines the graph of RDDs on which that action depends and formulates an execution plan. This plan starts with the farthest-back RDDs—that is, those that depend on no other RDDs or reference already-cached data–and culminates in the final RDD required to produce the action’s results.


The execution plan consists of assembling the job’s transformations into stages. A stage corresponds to a collection of tasks that all execute the same code, each on a different subset of the data. Each stage contains a sequence of transformations that can be completed without shuffling the full data.

The number of tasks in a stage is the same as the number of partitions in the last RDD in the stage



# When does Spark shuffle the data?
# ---------------------------------------
Spark also supports transformations with wide dependencies such as groupByKey and reduceByKey. In these dependencies, the data required to compute the records in a single partition may reside in many partitions of the parent RDD. All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute a shuffle, which transfers data around the cluster and results in a new stage with a new set of partitions.



The following  would execute a single stage (outputs dont depend on data that may be on different partitions):

sc.textFile("someFile.txt").
  map(mapFunc).
  flatMap(flatMapFunc).
  filter(filterFunc).
  count()



This process would break down into three stages. 

val tokenized = sc.textFile(args(0)).flatMap(_.split(' '))
val wordCounts = tokenized.map((_, 1)).reduceByKey(_ + _)
val filtered = wordCounts.filter(_._2 >= 1000)
val charCounts = filtered.flatMap(_._1.toCharArray).map((_, 1)).
  reduceByKey(_ + _)
charCounts.collect()


reduceByKey operations require repartitioning the data by keys



At each stage boundary, data is written to disk by tasks in the parent stages and then fetched over the network by tasks in the child stage. 
Because they incur heavy disk and network I/O, stage boundaries can be expensive and should be avoided when possible.




# Parititions 
# ---------------------------------------
The number of partitions at stage boundaries can often make or break an application’s performance. 
Transformations that may trigger a stage boundary typically accept a numPartitions argument that determines how many partitions to split the data into in the child stage.


When reading from HDFS:
Typically there will be a partition for each HDFS block being read. 


To determine the number of partitions in an RDD, you can always call rdd.partitions().size().

How de we increase the number of partitions?
 - Use the repartition transformation, which will trigger a shuffle.
 - Configure our InputFormat to create more splits.
 - Write the input data out to HDFS with a smaller block size.

If the stage is getting its input from another stage, the transformation that triggered the stage boundary will accept a numPartitions argument, such as

val rdd2 = rdd1.reduceByKey(_ + _, numPartitions = X)




# Things to avoid
# ---------------------------------------
 - Avoid groupByKey when performing an associative reductive operation. 
 - Avoid reduceByKey When the input and output value types are different.
 - Avoid the flatMap-join-groupBy pattern

When the records destined for these aggregation operations do not easily fit in memory, some mayhem can ensue.
First, holding many records in these data structures puts pressure on garbage collection, which can lead to pauses down the line. 
Second, when the records do not fit in memory, Spark will spill them to disk, which causes disk I/O and sorting, and these large shuffles have a large overhead. 


Tip: One way to avoid shuffles when joining two datasets is to take advantage of broadcast variables

Also repartitionAndSortWithinPartitions seems to be a good capability to be aware of



# Exceptions of the rule to avoid reshuffling
# ----------------------------------------------
Exceptions can arise when using the reduce or aggregate action to aggregate data into the driver. When aggregating over a high number of partitions, the computation can quickly become bottlenecked on a single thread in the driver merging all the results together. To loosen the load on the driver, one can first use reduceByKey or aggregateByKey to carry out a round of distributed aggregation that divides the dataset into a smaller number of partitions. The values within each partition are merged with each other in parallel, before sending their results to the driver for a final round of aggregation. Take a look at treeReduce and treeAggregate for examples of how to do that. 





# ----------------------------------------#
# Tuning Resource Allocation              #
# ----------------------------------------#


# Spark Executor Cores, Memory and Number of Executors
# ---------------------------------------------------------
Every Spark executor in an application has the same fixed number of cores and same fixed heap size.

The number of cores can be specified with the --executor-cores flag when invoking spark-submit, spark-shell, and pyspark from the command line, or by setting the spark.executor.cores property in the spark-defaults.conf file or on a SparkConf object.
The cores property controls the number of concurrent tasks an executor can run. 

Similarly, the heap size can be controlled with the --executor-memory flag or the spark.executor.memory property. 
The memory property impacts the amount of data Spark can cache as well as the maximum sizes of the shuffle data structures used for grouping, aggregations, and joins


The --num-executors command-line flag or spark.executor.instances configuration property control the number of executors requested. 

(This can be avoided by using spark.dynamicAllocation.enabled in CDH 5.4/Spark 1.3)


Add the following to our spark configuration file

spark.shuffle.service.enabled   true
spark.dynamicAllocation.enabled true



If your installation has a spark-env.sh script in SPARK_HOME/conf, make sure that it does not have lines such as the following, or that they are commented out:

export SPARK_WORKER_INSTANCES=1 #or some other integer, or
export SPARK_EXECUTOR_INSTANCES=1 #or some me other integer



# Yarn Cores and Memory
# ---------------------------

The relevant YARN properties are:

yarn.nodemanager.resource.memory-mb controls the maximum sum of memory used by the containers on each node.
yarn.nodemanager.resource.cpu-vcores controls the maximum sum of cores used by the containers on each node.



Asking for five executor cores will result in a request to YARN for five virtual cores. The memory requested from YARN is a little more complex for a couple reasons:

--executor-memory/spark.executor.memory controls the executor heap size, but JVMs can also use some memory off heap, for example for interned Strings and direct byte buffers. 
The value of the spark.yarn.executor.memoryOverhead property is added to the executor memory to determine the full memory request to YARN for each executor. 
It defaults to max(384, .07 * spark.executor.memory).
YARN may round the requested memory up a little. YARN’s yarn.scheduler.minimum-allocation-mb and yarn.scheduler.increment-allocation-mb properties control the minimum and increment request values respectively.



# Resource considerations
# -------------------------


- The application master, which is a non-executor container with the special capability of requesting containers from YARN, takes up resources of its own that must be budgeted in. In yarn-client mode, it defaults to a 1024MB and one vcore. In yarn-cluster mode, the application master runs the driver, so it’s often useful to bolster its resources with the --driver-memory and --driver-cores properties.
Running executors with too much memory often results in excessive garbage collection delays. 64GB is a rough guess at a good upper limit for a single executor.

- The HDFS client has trouble with tons of concurrent threads. 
A rough guess is that at most five tasks per executor can achieve full write throughput, so it’s good to keep the number of cores per executor below that number.

- Running tiny executors (with a single core and just enough memory needed to run a single task, for example) throws away the benefits that come from running multiple tasks in a single JVM. For example, broadcast variables need to be replicated once on each executor, so many small executors will result in many more copies of the data.



# Example of an efficient resource allocation
# ----------------------------------------------

Imagine a cluster:

6 x Nodes 
  - 16 cores / node
  - 64GB of RAM / node



An efficient configuration would be:

   --num-executors 17
   --executor-cores 5 
   --executor-memory 19G

Why?


This results in:

    3 executors on each node nodes except for the one with the Application Master (which has 2)
    As for Memory, we want 63/3 = 21 * 0.07 = 1.47. 21 - 1.47 ~= 19


# Another Example from:
# -------------------------
https://github.com/vaquarkhan/vaquarkhan/wiki/How-to-calculate-node-and-executors-memory-in-Apache-Spark

If we have following hardware:

6 x Nodes
   - 16 core / node
   - 64 GB of RAM

" 15 cores per executer can lead to bad HDFS I/O throughput. " 
" Best is to keep under 5 cores per executor "


Calculations:

5 core per executor
-For max HDFS throughput
Cluster has 6*15 =90 cores in total
  (after taking out Hadoop /Yarn daemon cores)
90 cores /5 cores/executor (19/5=18-1)
=18 executors
Each node has 3 executors
63 GB/3 =21 GB ,21*(1 -0.07)
~19 GB
1 executor for AM=> 17 executor


Ans:

17 Executors in total
19 GB memory /executor
5 cores /executor




# Calculating resource allocation values for current prototype
# ------------------------------------------------------------


In the current prototype we have:

7x Nodes:
   - 6 / node
   - 22GB of RAM /node


Calculations:

5 cores per executor
Cluster has 7 * 5 = 35 VCPUs (~ cores) in total after taking out Hadoop / Yarn daemon cores
35 /5 = 7 executors

Each node has 1 executor 
Each node has 21 *(1 - 0.07) ~= 19 GB RAM

We need 1 Executor for the Application Master, so => 6 Executors


Ans:

6 executors
17-19 GB memory / executor (Depending on what we set for yarn.nodemanager.resource.memory-mb in Yarn, needs to be less than that)
5 cores / executor




# Calculating resource allocation for Yarn
# ------------------------------------------------------------

Hadoop configurations can be set in mapred-site.xml & yarn-site.xml

Rough guess for the values to place here:

Hadoop Configuration:

   mapred-site.xml
	yarn.app.mapreduce.am.resource.mb	21000
	mapreduce.map.memory.mb		10000
	mapreduce.reduce.memory.mb	10000

   yarn-site.xml
	yarn.nodemanager.resource.memory-mb	21000
	yarn.scheduler.maximum-allocation-mb	21000
	yarn.scheduler.minimum-allocation-mb	21000


(Not sure about these yet!!)


NOTE: The memory that the executors get needs to be less than yarn.nodemanager.resource.memory-mb + overhead (which is about yarn.nodemanager.resource.memory-mb/10)




Otherwise we may see an error like:
 
  > java.lang.IllegalArgumentException: Required AM memory (20480+2048 MB) is above the max threshold (21000 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'.





# Data formats for storage
# --------------------------

It's recommended to use an extensible binary format like Avro, Parquet, Thrift, or Protobuf (not Json). Pick one of these formats and stick to it..




