#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


# Following Notes from zrq here:



# -----------------------------------------------------
# Create Clouds YAML file
#[user@desktop]

cat > "${HOME}/clouds.yaml" << EOF

clouds:


  gaia-test:
    auth:
      auth_url: https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
      application_credential_id:     '$(secret 'stv-gaia-test.CREDENTIAL_ID')'
      application_credential_secret: '$(secret 'stv-gaia-test.CREDENTIAL_SECRET')'
    region_name: "RegionOne"
    interface: "public"
    identity_api_version: 3
    auth_type: "v3applicationcredential"

  gaia-test-super:
    auth:
      auth_url: https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
      application_credential_id:     '$(secret 'stv-gaia-test.CREDENTIAL_ID')'
      application_credential_secret: '$(secret 'stv-gaia-test.CREDENTIAL_SECRET')'
    region_name: "RegionOne"
    interface: "public"
    identity_api_version: 3
    auth_type: "v3applicationcredential"

EOF



# -----------------------------------------------------
# Create our project config file.
#[user@desktop]

    cat > "${HOME:?}/aglais.env" << 'EOF'

AGLAIS_REPO='git@github.com:stvoutsin/aglais.git'
AGLAIS_HOME="${PROJECTS_ROOT:?}/aglais"
AGLAIS_CODE="${AGLAIS_HOME:?}"
AGLAIS_CLOUD=gaia-test
AGLAIS_USER=stv

EOF




# -----------------------------------------------------
# Edit hosts.yml file 
#[user@desktop]

  source "${HOME}/aglais.settings"
  nano ${AGLAIS_CODE:?}/experiments/zrq/ansible/hosts.yml
	..	
	keypair: ''
	...


# -----------------------------------------------------
# Create a container to work with.
# https://podman.readthedocs.io/en/latest/markdown/podman-run.1.html
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock" \
        --env "clouduser=${AGLAIS_USER:?}" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml" \
        --env "ANSIBLE_CODE=/mnt/ansible" \
        --volume "${AGLAIS_CODE:?}/experiments/zrq/ansible:/mnt/ansible" \
        atolmis/ansible-client:latest \
        bash

	# Success



# -----------------------------------------------------
# Create our Ansible include vars file.
#[root@ansibler]

    cat > /tmp/ansible-vars.yml << EOF
buildtag:  'aglais-$(date '+%Y%m%d')'
cloudname: '${cloudname}'
clouduser: '${clouduser}'
EOF


# -----------------------------------------------------
# Run the scripts from the ansible directory.
#[root@ansibler]

    cd "${ANSIBLE_CODE:?}"



# -----------------------------------------------------
# Run the initial part of our deployment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-01.yml"

PLAY RECAP **************************************************************************************************************************************************************************************************
gateway                    : ok=8    changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
localhost                  : ok=25   changed=18   unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
master01                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master02                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker01                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker02                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker03                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker04                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker05                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker06                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker07                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker08                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   



# -----------------------------------------------------
# Run the Hadoop part of our deployment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-02.yml"

PLAY RECAP **************************************************************************************************************************************************************************************************
gateway                    : ok=8    changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
localhost                  : ok=25   changed=18   unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
master01                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master02                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker01                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker02                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker03                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker04                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker05                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker06                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker07                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker08                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

# -----------------------------------------------------
# Format the HDFS NameNode on master01.
#[root@ansibler]

    ssh master01 \
        '
        hdfs namenode -format
        '


	2020-09-09 19:46:10,729 INFO namenode.FSImageFormatProtobuf: Image file /var/local/hadoop/namenode/fsimage/current/fsimage.ckpt_0000000000000000000 of size 398 bytes saved in 0 seconds .
	2020-09-09 19:46:10,737 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
	2020-09-09 19:46:10,742 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
	2020-09-09 19:46:10,743 INFO namenode.NameNode: SHUTDOWN_MSG: 
	/************************************************************
	SHUTDOWN_MSG: Shutting down NameNode at master01/10.10.1.57
	************************************************************/



# -----------------------------------------------------
# Start the HDFS services.
#[root@ansibler]

    ssh master01 \
        '
        start-all.sh
	'	


	Starting namenodes on [master01]
	Starting datanodes
	Starting secondary namenodes [aglais-20200909-master01.novalocal]
	aglais-20200909-master01.novalocal: Warning: Permanently added 'aglais-20200909-master01.novalocal,fe80::f816:3eff:fe8f:e9f4%eth0' (ECDSA) to the list of known hosts.


# -----------------------------------------------------
# Check the HDFS status.
#[root@ansibler]

    ssh master01 \
        '
        hdfs dfsadmin -report
	'

	Configured Capacity: 2199023255552 (2 TB)
	Present Capacity: 2190360985600 (1.99 TB)
	DFS Remaining: 2190360969216 (1.99 TB)
	DFS Used: 16384 (16 KB)
	DFS Used%: 0.00%
	Replicated Blocks:
		Under replicated blocks: 0
		Blocks with corrupt replicas: 0
		Missing blocks: 0
		Missing blocks (with replication factor 1): 0
		Low redundancy blocks with highest priority to recover: 0
		Pending deletion blocks: 0
	Erasure Coded Block Groups: 
		Low redundancy block groups: 0
		Block groups with corrupt internal blocks: 0
		Missing block groups: 0
		Low redundancy blocks with highest priority to recover: 0
		Pending deletion blocks: 0

	-------------------------------------------------
	Live datanodes (4):

	Name: 10.10.0.119:9866 (worker04)
	Hostname: worker04
	Decommission Status : Normal
	Configured Capacity: 549755813888 (512 GB)
	DFS Used: 4096 (4 KB)
	Non DFS Used: 17297408 (16.50 MB)
	DFS Remaining: 547590242304 (509.98 GB)
	DFS Used%: 0.00%
	DFS Remaining%: 99.61%
	Configured Cache Capacity: 0 (0 B)
	Cache Used: 0 (0 B)
	Cache Remaining: 0 (0 B)
	Cache Used%: 100.00%
	Cache Remaining%: 0.00%
	Xceivers: 1
	Last contact: Wed Sep 09 19:46:50 UTC 2020
	Last Block Report: Wed Sep 09 19:46:26 UTC 2020
	Num of Blocks: 0


	Name: 10.10.1.113:9866 (worker02)
	Hostname: worker02
	Decommission Status : Normal
	Configured Capacity: 549755813888 (512 GB)
	DFS Used: 4096 (4 KB)
	Non DFS Used: 17297408 (16.50 MB)
	DFS Remaining: 547590242304 (509.98 GB)
	DFS Used%: 0.00%
	DFS Remaining%: 99.61%
	Configured Cache Capacity: 0 (0 B)
	Cache Used: 0 (0 B)
	Cache Remaining: 0 (0 B)
	Cache Used%: 100.00%
	Cache Remaining%: 0.00%
	Xceivers: 1
	Last contact: Wed Sep 09 19:46:50 UTC 2020
	Last Block Report: Wed Sep 09 19:46:26 UTC 2020
	Num of Blocks: 0


	Name: 10.10.2.192:9866 (worker01)
	Hostname: worker01
	Decommission Status : Normal
	Configured Capacity: 549755813888 (512 GB)
	DFS Used: 4096 (4 KB)
	Non DFS Used: 17297408 (16.50 MB)
	DFS Remaining: 547590242304 (509.98 GB)
	DFS Used%: 0.00%
	DFS Remaining%: 99.61%
	Configured Cache Capacity: 0 (0 B)
	Cache Used: 0 (0 B)
	Cache Remaining: 0 (0 B)
	Cache Used%: 100.00%
	Cache Remaining%: 0.00%
	Xceivers: 1
	Last contact: Wed Sep 09 19:46:50 UTC 2020
	Last Block Report: Wed Sep 09 19:46:26 UTC 2020
	Num of Blocks: 0


	Name: 10.10.2.223:9866 (worker03)
	Hostname: worker03
	Decommission Status : Normal
	Configured Capacity: 549755813888 (512 GB)
	DFS Used: 4096 (4 KB)
	Non DFS Used: 17297408 (16.50 MB)
	DFS Remaining: 547590242304 (509.98 GB)
	DFS Used%: 0.00%
	DFS Remaining%: 99.61%
	Configured Cache Capacity: 0 (0 B)
	Cache Used: 0 (0 B)
	Cache Remaining: 0 (0 B)
	Cache Used%: 100.00%
	Cache Remaining%: 0.00%
	Xceivers: 1
	Last contact: Wed Sep 09 19:46:50 UTC 2020
	Last Block Report: Wed Sep 09 19:46:26 UTC 2020
	Num of Blocks: 0



# -----------------------------------------------------
# Start the YARN services.
#[root@ansibler]

    ssh master01 \
        '
        start-all.sh
        '

      > WARNING: Attempting to start all Apache Hadoop daemons as fedora in 10 seconds.
	WARNING: This is not a recommended production deployment configuration.
	WARNING: Use CTRL-C to abort.
	Starting namenodes on [master01]
	Starting datanodes
	Starting secondary namenodes [aglais-20200909-master01]
	Starting resourcemanager
	Starting nodemanagers


# -----------------------------------------------------
# Install the Spark binaries.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "20-install-spark.yml"


	> PLAY RECAP **********************************************************************************************************************************************
 	  master01                   : ok=3    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
 	  zeppelin                   : ok=3    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   



# -----------------------------------------------------
# Add the security rules for Spark.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "21-config-spark-security.yml"

      
	> PLAY RECAP **********************************************************************************************************************************************
	localhost                  : ok=6    changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   



# -----------------------------------------------------
# Create our Spark configuration.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "22-config-spark-master.yml"

	> PLAY RECAP **********************************************************************************************************************************************
	gateway                    : ok=2    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
	master01                   : ok=2    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  

# -----------------------------------------------------
# Create our HDFS log directory.
#[root@ansibler]

    ssh master01 \
        '
        hdfs dfs -mkdir /spark-log
        '


# -----------------------------------------------------
# Run the SparkPi example from the Spark install instructtions.
# https://spark.apache.org/docs/3.0.0-preview2/running-on-yarn.html#launching-spark-on-yarn
#[root@ansibler]

    ssh master01 \
        '
        cd "${SPARK_HOME:?}"

        spark-submit \
            --class org.apache.spark.examples.SparkPi \
            --master yarn \
            --deploy-mode cluster \
            --driver-memory 1g \
            --executor-memory 1g \
            --executor-cores 1 \
            examples/jars/spark-examples*.jar \
                10


       > ..
 
	2020-09-09 20:11:34,731 INFO yarn.Client: Application report for application_1599680883948_0001 (state: FINISHED)
	2020-09-09 20:11:34,732 INFO yarn.Client: 
		 client token: N/A
		 diagnostics: N/A
		 ApplicationMaster host: worker03
		 ApplicationMaster RPC port: 43513
		 queue: default
		 start time: 1599682277508
		 final status: SUCCEEDED
		 tracking URL: http://master01:8088/proxy/application_1599680883948_0001/
		 user: fedora
	2020-09-09 20:11:34,752 INFO util.ShutdownHookManager: Shutdown hook called
	2020-09-09 20:11:34,756 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-30283984-2687-4b6c-a967-78dd4c424fb9
	2020-09-09 20:11:34,761 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-c482f3d9-ab95-4287-87a8-b5c92285704f




# -----------------------------------------------------
# Run the Zeppelin install.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-04.yml"
	


# -----------------------------------------------------
# Start the YARN services.
#[root@ansibler]

    ssh zeppelin \
        '
        sudo /opt/zeppelin-0.8.2-bin-all/bin/zeppelin-daemon.sh start
        '


# -----------------------------------------------------
# Try some Spark jobs via the Zeppelin GUI.
# http://128.232.227.203:8080/#/notebook/2FMUJCXDZ


java.lang.RuntimeException: 2020-09-12 09:13:28,644 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-09-12 09:13:28,998 INFO client.RMProxy: Connecting to ResourceManager at master01/10.10.1.57:8032
2020-09-12 09:13:29,228 INFO yarn.Client: Requesting a new application from cluster with 4 NodeManagers
2020-09-12 09:13:29,596 INFO conf.Configuration: resource-types.xml not found
2020-09-12 09:13:29,597 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2020-09-12 09:13:29,610 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
2020-09-12 09:13:29,611 INFO yarn.Client: Will allocate AM container, with 4505 MB memory including 409 MB overhead
2020-09-12 09:13:29,611 INFO yarn.Client: Setting up container launch context for our AM
2020-09-12 09:13:29,614 INFO yarn.Client: Setting up the launch environment for our AM container
2020-09-12 09:13:29,624 INFO yarn.Client: Preparing resources for our AM container
Exception in thread "main" org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode="/user":fedora:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1879)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1863)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1822)
	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:59)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3233)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2426)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2400)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1324)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1321)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1338)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1313)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:674)
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:441)
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:876)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:196)
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1177)
	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1583)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:928)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=root, access=WRITE, inode="/user":fedora:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1879)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1863)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1822)
	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:59)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3233)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy12.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:656)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy13.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2424)
	... 20 more
2020-09-12 09:13:29,669 INFO util.ShutdownHookManager: Shutdown hook called
2020-09-12 09:13:29,670 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-878c1d81-5577-4007-9a23-90a1d1af8710

	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterManagedProcess.start(RemoteInterpreterManagedProcess.java:205)
	at org.apache.zeppelin.interpreter.ManagedInterpreterGroup.getOrCreateInterpreterProcess(ManagedInterpreterGroup.java:64)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getOrCreateInterpreterProcess(RemoteInterpreter.java:111)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:164)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:132)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:299)
	at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:408)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:188)
	at org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:315)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)



# Permission error?

# Try giving the right permissions to the '/user' dir:

    ssh master01 \
        '
         hdfs dfs -chown root:hadoop /user
        '


2020-09-12 09:23:11,514 INFO impl.YarnClientImpl: Submitted application application_1599901109198_0003
2020-09-12 09:23:12,519 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:12,522 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1599902590259
	 final status: UNDEFINED
	 tracking URL: http://master01:8088/proxy/application_1599901109198_0003/
	 user: root
2020-09-12 09:23:13,525 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:14,529 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:15,532 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:16,537 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:17,541 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:18,544 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:19,548 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:20,552 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:21,556 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:22,559 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:23,563 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:24,566 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:25,570 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:26,575 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:27,578 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:28,581 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:29,584 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:30,587 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:31,591 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:32,598 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:33,601 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:34,604 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:35,606 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:36,609 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:37,612 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:38,615 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:39,618 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:40,622 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:41,625 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:42,628 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:43,631 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:44,635 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:45,638 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:46,640 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:47,645 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:48,648 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:49,651 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:50,654 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:51,662 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:52,665 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:53,668 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:54,671 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:55,674 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:56,678 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:57,681 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:58,684 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:23:59,687 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:24:00,690 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:24:01,693 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)
2020-09-12 09:24:02,697 INFO yarn.Client: Application report for application_1599901109198_0003 (state: ACCEPTED)

	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterManagedProcess.start(RemoteInterpreterManagedProcess.java:205)
	at org.apache.zeppelin.interpreter.ManagedInterpreterGroup.getOrCreateInterpreterProcess(ManagedInterpreterGroup.java:64)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getOrCreateInterpreterProcess(RemoteInterpreter.java:111)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:164)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:132)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:299)
	at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:408)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:188)
	at org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:315)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)




# Try the same via the spark-submit commandline tool in Zeppelin
ssh zeppelin

   spark-submit             --class org.apache.spark.examples.SparkPi             --master yarn             --deploy-mode cluster             --driver-memory 1g             --executor-memory 1g             --executor-cores 1             examples/jars/spark-examples*.jar

  .. 

	2020-09-12 09:43:22,662 INFO yarn.Client: Application report for application_1599901109198_0005 (state: ACCEPTED)
	2020-09-12 09:43:23,664 INFO yarn.Client: Application report for application_1599901109198_0005 (state: ACCEPTED)
	2020-09-12 09:43:24,667 INFO yarn.Client: Application report for application_1599901109198_0005 (state: ACCEPTED)
	2020-09-12 09:43:25,669 INFO yarn.Client: Application report for application_1599901109198_0005 (state: ACCEPTED)
	2020-09-12 09:43:26,671 INFO yarn.Client: Application report for application_1599901109198_0005 (state: ACCEPTED)
	2020-09-12 09:43:27,673 INFO yarn.Client: Application report for application_1599901109198_0005 (state: ACCEPTED)
	2020-09-12 09:43:28,675 INFO yarn.Client: Application report for application_1599901109198_0005 (state: ACCEPTED)
	2020-09-12 09:43:29,677 INFO yarn.Client: Application report for application_1599901109198_0005 (state: ACCEPTED)
	2020-09-12 09:43:30,680 INFO yarn.Client: Application report for application_1599901109198_0005 (state: ACCEPTED)
	2020-09-12 09:43:31,682 INFO yarn.Client: Application report for application_1599901109198_0005 (state: ACCEPTED)
	2020-09-12 09:43:32,684 INFO yarn.Client: Application report for application_1599901109198_0005 (state: ACCEPTED)
	2020-09-12 09:43:33,686 INFO yarn.Client: Application report for application_1599901109198_0005 (state: ACCEPTED)

..

exit



# Try restarting Hadoop & Zeppelin

    # Restart Hadoop

    ssh master01 \
        '
        stop-all.sh
        start-all.sh
	'	

    # Restart Zeppelin 
    ssh zeppelin \
        '
        sudo /opt/zeppelin-0.8.2-bin-all/bin/zeppelin-daemon.sh start
        '

# No luck
# Check Hadoop GUI

ssh -L '8088:master01:8088' test-gateway

  http://localhost:8088


