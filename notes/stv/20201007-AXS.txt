sudo yum install wget
wget https://github.com/astronomy-commons/axs/releases/download/v1.0/axs-distribution.tar.gz
tar -xzvf axs-distribution.tar.gz 
sudo mv /opt/spark-2.4.7-bin-hadoop2.7/ /opt/spark-2.4.7-bin-hadoop2.7-backup/
sudo mkdir /opt/spark-2.4.7-bin-hadoop2.7
cd /opt/spark-2.4.7-bin-hadoop2.7
sudo cp -R ~/axs-dist/* .

# Run Axis init
axs-init-config.sh

# Edit configurations manually:
nano /opt/spark-2.4.7-bin-hadoop2.7/conf/
..
	spark.local.dir        /opt/spark/local
	spark.sql.warehouse.dir        file:///opt/spark/warehouse
	spark.files.maxPartitionBytes   471859200
	spark.memory.offHeap.enabled    true
	spark.memory.offHeap.size	4g
	# spark.jars.packages   org.mariadb.jdbc:mariadb-java-client:2.2.3
	spark.jars	/opt/spark/python/axs/AxsUtilities-1.0-SNAPSHOT.jar
	spark.scheduler.minRegisteredResourcesRatio     0.75
	spark.master            yarn
	spark.driver.memory              5g
	spark.yarn.am.memory            5g
	spark.executor.memory          5g
	spark.eventLog.enabled  true
	spark.driver.maxResultSize	8192m
	spark.executor.cores            3
	spark.eventLog.enabled  true
	spark.eventLog.dir	hdfs://master01:9000/spark-log
	# END Ansible managed Spark configuration
	# BEGIN Ansible managed Spark environment
	# https://spark.apache.org/docs/3.0.0-preview2/configuration.html#inheriting-hadoop-cluster-configuration
	spark.yarn.appMasterEnv.YARN_CONF_DIR=/opt/hadoop/etc/hadoop
	spark.yarn.appMasterEnv.HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
..

# Remove spark-env and slaves file
rm /opt/spark-2.4.7-bin-hadoop2.7/slaves
rm /opt/spark-2.4.7-bin-hadoop2.7/spark-env.sh


stop-all.sh
start-all.sh


 spark-submit \
>             --class org.apache.spark.examples.SparkPi \
>             --master yarn \
>             --deploy-mode cluster \
>             --driver-memory 1g \
>             --executor-memory 1g \
>             --executor-cores 1 \
>             examples/jars/spark-examples*.jar \
>                 10
20/10/07 16:19:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/10/07 16:19:22 WARN DependencyUtils: Local jar /opt/spark-2.4.7-bin-hadoop2.7/conf/examples/jars/spark-examples*.jar does not exist, skipping.
Exception in thread "main" java.lang.NoSuchMethodError: org.apache.hadoop.io.retry.RetryPolicies.retryOtherThanRemoteException(Lorg/apache/hadoop/io/retry/RetryPolicy;Ljava/util/Map;)Lorg/apache/hadoop/io/retry/RetryPolicy;
	at org.apache.hadoop.yarn.client.RMProxy.createRetryPolicy(RMProxy.java:255)
	at org.apache.hadoop.yarn.client.RMProxy.createRMProxy(RMProxy.java:91)
	at org.apache.hadoop.yarn.client.ClientRMProxy.createRMProxy(ClientRMProxy.java:72)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceStart(YarnClientImpl.java:187)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:161)
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1134)
	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1526)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:935)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

