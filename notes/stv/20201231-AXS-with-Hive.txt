#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
#
# </meta:header>
#
#


Target:

	Run the Ansible deploy with Zeppelin included customized for AXS

Result:

	Success

TODO:
   Integrate steps into Ansible Scripts (Some of this done locally but not in commits yet)
   Try with newer versions of Hadoop & Spark

# -----------------------------------------------------
#[user@desktop]
# Edit Ansible scripts to change spark location to an AXS distribution, and Hadoop to version 2.7.4

# https://archive.apache.org/dist/hadoop/core/hadoop-2.7.4/hadoop-2.7.4.tar.gz
# https://github.com/stevenstetzler/axs/releases/download/v3.0.0-preview/axs-distribution.tgz




# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "clouduser=${AGLAIS_USER:?}" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/openstack:/openstack:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/hadoop-yarn:/hadoop-yarn:ro,z" \
        atolmis/ansible-client:latest \
        bash


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    /openstack/bin/delete-all.sh \
        "${cloudname:?}"


	> ---- ----
	  Done

# -----------------------------------------------------
# Run the main Ansible deployment.
#[root@ansibler]

    /hadoop-yarn/bin/create-all.sh \
        "${cloudname:?}"



PLAY RECAP **************************************************************************************************************************************************************************************************
gateway                    : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master01                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker01                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker02                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
zeppelin                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   




# -----------------------------------------------------
# Run the SparkPi example from the Spark install instructtions.
# https://spark.apache.org/docs/3.0.0-preview2/running-on-yarn.html#launching-spark-on-yarn
#[root@ansibler]

    ssh master01 \
        '
        cd "${SPARK_HOME:?}"

        spark-submit \
            --class org.apache.spark.examples.SparkPi \
            --master yarn \
            --deploy-mode cluster \
            --driver-memory 1g \
            --executor-memory 1g \
            --executor-cores 1 \
            examples/jars/spark-examples*.jar \
                10
        '

   
20/12/30 21:14:38 INFO yarn.Client: Application report for application_1609361972021_0001 (state: RUNNING)
20/12/30 21:14:39 INFO yarn.Client: Application report for application_1609361972021_0001 (state: FINISHED)
20/12/30 21:14:39 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: worker02
	 ApplicationMaster RPC port: 32905
	 queue: default
	 start time: 1609362865834
	 final status: SUCCEEDED
	 tracking URL: http://master01:8088/proxy/application_1609361972021_0001/
	 user: fedora
20/12/30 21:14:39 INFO util.ShutdownHookManager: Shutdown hook called
20/12/30 21:14:39 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3f84af0c-d1fc-413d-826d-2243a5fdb7db


# Success



# -----------------------------------------------------
# Setup Spark Master & Deploy modes in Zeppelin
#[user@zeppelin]


# There seems to be an issue with how we define the Spark master parameter when using Hadoop 2.7.4 with Spark 3.0.0
# Fix:

Change master=yarn in Zeppelin -> Interpreters -> Spark
Change spark.submit.deployMode=client  in Zeppelin -> Interpreters -> Spark




# -----------------------------------------------------
# Build our own distribution from axs-spark source code
#[fedora@master01]

# For this to work I had to checkout a specific version of the axs-spark, otherwise the maven build did not work
# Additionally because we need Scala 2.11


sudo yum install -y maven

git clone https://github.com/astronomy-commons/axs.git
pushd axs
  git checkout master
popd


git clone https://github.com/astronomy-commons/axs-spark
pushd axs-spark
    git checkout axs-2.4.3
popd 

pushd axs/AxsUtilities
  mvn package # runs maven to compile the AxsUtilities project, pom.xml sets configuration for build
popd

cp -r ./axs/axs ./axs-spark/python/. # adds python components of axs to Spark's PYTHONPATH
cp -r ./axs/AxsUtilities/target/*.jar ./axs-spark/python/axs/. # adds compiled  AXS Jar for use in Spark


# Make distribution
pushd axs-spark
    ./dev/make-distribution.sh --name AXS-Custom-Build --tgz -Phadoop-2.7.4 -Pmesos -Pyarn -Phive -Phive-thriftserver -Pkubernetes
popd




# Create hive-site.xml

cat > "axs-spark/dist/conf/hive-site.xml" << EOF 
<configuration>

<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://localhost:3306/hive?characterEncoding=utf8</value>
  <description>JDBC connect string for a JDBC metastore</description>
</property>
 
<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
  <description>Driver class name for a JDBC metastore</description>
</property>
 
<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hive</value>
  <description>username to use against metastore database</description>
</property>
 
<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>pass</value>
  <description>password to use against metastore database</description>
</property>

</configuration>

EOF


## NOTE: Don't use root for username here, because exceptions will occur at later steps..


cat > "axs-spark/dist/conf/spark-defaults.conf" << EOF 

spark.master            yarn
spark.driver.memory              8g
spark.yarn.am.memory            8g
spark.executor.memory          8g
spark.eventLog.enabled  true
spark.driver.maxResultSize	8192m
spark.local.dir         /opt/spark/local
spark.executor.cores            4
spark.executor.instances    4
spark.yarn.am.cores  4
spark.eventLog.enabled  true
spark.eventLog.dir	hdfs://master01:9000/spark-log
# END Ansible managed Spark configuration
# BEGIN Ansible managed Spark environment
# https://spark.apache.org/docs/3.0.0-preview2/configuration.html#inheriting-hadoop-cluster-configuration
spark.yarn.appMasterEnv.YARN_CONF_DIR=/opt/hadoop/etc/hadoop
spark.yarn.appMasterEnv.HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
# END Ansible managed Spark environment
spark.sql.warehouse.dir=/warehouse


EOF


# Copy new distribution to Spark directory and set permissions
mkdir /home/fedora/axs-spark/dist/local
sudo rm -r /opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/*
sudo cp -r /home/fedora/axs-spark/dist/* /opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/
sudo chown -R fedora:root /opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/*

cp /home/fedora/axs/AxsUtilities/target/AxsUtilities-1.0-SNAPSHOT.jar /opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/jars/


# ----------------------------------------------------
# Copy Spark files to Zeppelin
#[fedora@master01]

ssh zeppelin \
	'
	sudo rm -R /opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/*	
	sudo chown fedora:root /opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/
	'

#[fedora@master01]
scp -r /opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/* zeppelin:/opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview
scp /home/fedora/axs/AxsUtilities/target/AxsUtilities-1.0-SNAPSHOT.jar zeppelin:/opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/jars/


# Set the permissions to the spark directory in Zeppelin
ssh zeppelin \
	'
	sudo chown -R fedora:root /opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/
	'


# ----------------------------------------------------
# Restart Zeppelin & Hadoop
#[fedora@master01]


# Restart Hadoop

stop-all.sh
start-all.sh



# Restart Zeppelin

ssh zeppelin \
	'
	/home/fedora/zeppelin-0.8.2-bin-all/bin/zeppelin-daemon.sh restart
	'


# -----------------------------------------------------
# Try some Spark jobs via the Zeppelin GUI.


%spark.pyspark
import random 
NUM_SAMPLES = 200000

def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1

count = sc.parallelize(range(0, NUM_SAMPLES)) \
             .filter(inside).count()
print ("Pi is roughly %f" % (4.0 * count / NUM_SAMPLES))

> Pi is roughly 3.142360

# Success






# ------------------------------------------------
# Follow instructions to install MySQL
# fedora@zeppelin

sudo yum install -y wget
sudo yum install -y nano

https://www.if-not-true-then-false.com/2010/install-mysql-on-fedora-centos-red-hat-rhel/

sudo dnf install https://dev.mysql.com/get/mysql80-community-release-fc30-1.noarch.rpm
sudo dnf install mysql-community-server
systemctl start mysqld.service ## use restart after update

# Enable Mysql service
sudo systemctl start mysqld.service ## use restart after update
sudo systemctl enable mysqld.service

# Get temporary root pass
sudo grep 'A temporary password is generated for root@localhost' /var/log/mysqld.log |tail -1

 > pass

mysql_secure_installation

..


# ------------------------------------------------
# Create Hive user
# fedora@zeppelin

mysql -u root -p

mysql>
ALTER USER 'root'@'localhost' IDENTIFIED BY 'pass';
CREATE DATABASE hive;
CREATE USER 'hive'@'%' IDENTIFIED BY 'pass';
GRANT ALL PRIVILEGES ON *.* TO 'hive'@'%' WITH GRANT OPTION;
FLUSH privileges;



# ------------------------------------------------
# Get mysql connector and place in spark/jars
# fedora@zeppelin

pushd /opt/spark/jars/
   wget https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.22/mysql-connector-java-8.0.22.jar
popd



# ------------------------------------------------
# Setup Hive schema
# fedora@zeppelin

# https://www.geeksforgeeks.org/apache-hive-installation-and-configuring-mysql-metastore-for-hive/
pushd /home/fedora
    wget https://ftp.cc.uoc.gr/mirrors/apache/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz
    tar -xzvf apache-hive-1.2.2-bin.tar.gz 
popd

mysql>

SOURCE /home/fedora/apache-hive-1.2.2-bin/scripts/metastore/upgrade/mysql/hive-schema-1.2.0.mysql.sql

	> ...

	Query OK, 0 rows affected (0.00 sec)

	Query OK, 0 rows affected (0.00 sec)

	Query OK, 0 rows affected (0.00 sec)

	Query OK, 0 rows affected, 1 warning (0.00 sec)

	Query OK, 0 rows affected (0.00 sec)
	 


# ------------------------------------------------
# Try AXS with pyspark or Zeppelin
# fedora@zeppelin


%spark.pyspark


# Initialise AXS
from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)
from axs import AxsCatalog, Constants
db = AxsCatalog(spark)


# Create Dataframes from Parquet files for Gaia and 2Mass
dfgaia = spark.read.parquet("file:///data/gaia/dr2/*.parquet").where("dec>89")
df2mass = spark.read.parquet("file:////user/nch/PARQUET/TESTS/2MASS/*.parquet").where("dec>89")


# Create new Parquet files in Spark Metastore
db.save_axs_table(dfgaia, "gaia_source", repartition=False, calculate_zone=True)
db.save_axs_table(dfgaia, "twomass", repartition=False, calculate_zone=True)


# Import New Tables into AXS (Metadata database)
db.import_existing_table('gaia_source', '', num_buckets=500, zone_height=Constants.ONE_AMIN,
    import_into_spark=False, update_spark_bucketing=True)

db.import_existing_table('twomass', '', num_buckets=500, zone_height=Constants.ONE_AMIN,
    import_into_spark=False, update_spark_bucketing=True)


# Get list of Tables
for tname in db.list_tables():    
    print(tname)

> gaia_source
> twomass


# Load Tables from AXS
gaia = db.load('gaia_source')
twomass = db.load('twomass')


# Run Crossmatch
gaia_sdss_cm = gaia.crossmatch(twomass, 2*Constants.ONE_ASEC, return_min=False, include_dist_col=True)
gaia_sdss_cm.show()

# Success, rows of results showing up

# Get Count of results
gaia_sdss_cm.count()
1091
