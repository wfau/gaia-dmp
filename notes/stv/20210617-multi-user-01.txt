#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#


# Target

  # Experiment with multiple users running jobs concurrently
  # Experiment with SparkContexts



# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    docker run \
        --rm \
        --tty \
        --interactive \
        --name ansibler2 \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        atolmis/ansible-client:2020.12.02 \
        bash


# -----------------------------------------------------
# Set the target cloud to delete.
#[root@ansibler]

    cloudname=gaia-test


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"


	> Done

	> real	0m55.017s
	> user	0m16.240s
	> sys	0m1.585s


# -----------------------------------------------------
# Set the target cloud to create.
#[root@ansibler]

    cloudname=gaia-prod



# -----------------------------------------------------
# Create everything, using a standard config.
#[root@ansibler]

    time \
        /deployments/hadoop-yarn/bin/create-all.sh \
            "${cloudname:?}" \
            'medium-04'

# Failed


TASK [Update the DNF cache] *********************************************************************************************************************************************************************************
task path: /deployments/hadoop-yarn/ansible/04-update-fedora.yml:36
fatal: [zeppelin]: FAILED! => {"changed": false, "msg": "Failed to synchronize cache for repo 'updates'", "rc": 1, "results": []}
...ignoring
changed: [worker02] => {"changed": true, "msg": "", "rc": 0, "results": ["Installed: kernel-5.6.13-100.fc30.x86_64", "Installed: kernel-core-5.6.13-100.fc30.x86_64", "Installed: kernel-modules-5.6.13-100.fc30.x86_64", "Installed: linux-firmware-20200421-107.fc30.noarch"]}
changed: [master01] => {"changed": true, "msg": "", "rc": 0, "results": ["Installed: kernel-5.6.13-100.fc30.x86_64", "Installed: kernel-core-5.6.13-100.fc30.x86_64", "Installed: kernel-modules-5.6.13-100.fc30.x86_64", "Installed: linux-firmware-20200421-107.fc30.noarch"]}
changed: [worker01] => {"changed": true, "msg": "", "rc": 0, "results": ["Installed: kernel-5.6.13-100.fc30.x86_64", "Installed: kernel-core-5.6.13-100.fc30.x86_64", "Installed: kernel-modules-5.6.13-100.fc30.x86_64", "Installed: linux-firmware-20200421-107.fc30.noarch"]}
changed: [monitor] => {"changed": true, "msg": "", "rc": 0, "results": ["Installed: kernel-5.6.13-100.fc30.x86_64", "Installed: kernel-core-5.6.13-100.fc30.x86_64", "Installed: kernel-modules-5.6.13-100.fc30.x86_64", "Installed: linux-firmware-20200421-107.fc30.noarch"]}
changed: [worker03] => {"changed": true, "msg": "", "rc": 0, "results": ["Installed: kernel-5.6.13-100.fc30.x86_64", "Installed: kernel-core-5.6.13-100.fc30.x86_64", "Installed: kernel-modules-5.6.13-100.fc30.x86_64", "Installed: linux-firmware-20200421-107.fc30.noarch"]}
changed: [worker04] => {"changed": true, "msg": "", "rc": 0, "results": ["Installed: kernel-5.6.13-100.fc30.x86_64", "Installed: kernel-core-5.6.13-100.fc30.x86_64", "Installed: kernel-modules-5.6.13-100.fc30.x86_64", "Installed: linux-firmware-20200421-107.fc30.noarch"]}

TASK [Install monitoring tools] *****************************************************************************************************************************************************************************
task path: /deployments/hadoop-yarn/ansible/04-update-fedora.yml:44
changed: [worker01] => {"changed": true, "msg": "", "rc": 0, "results": ["Installed: htop-2.2.0-4.fc30.x86_64", "Installed: atop-2.4.0-3.fc30.x86_64"]}
changed: [worker02] => {"changed": true, "msg": "", "rc": 0, "results": ["Installed: htop-2.2.0-4.fc30.x86_64", "Installed: atop-2.4.0-3.fc30.x86_64"]}
changed: [monitor] => {"changed": true, "msg": "", "rc": 0, "results": ["Installed: htop-2.2.0-4.fc30.x86_64", "Installed: atop-2.4.0-3.fc30.x86_64"]}
changed: [master01] => {"changed": true, "msg": "", "rc": 0, "results": ["Installed: htop-2.2.0-4.fc30.x86_64", "Installed: atop-2.4.0-3.fc30.x86_64"]}
fatal: [zeppelin]: FAILED! => {"changed": false, "msg": "Failed to synchronize cache for repo 'fedora'", "rc": 1, "results": []}
changed: [worker04] => {"changed": true, "msg": "", "rc": 0, "results": ["Installed: htop-2.2.0-4.fc30.x86_64", "Installed: atop-2.4.0-3.fc30.x86_64"]}
changed: [worker03] => {"changed": true, "msg": "", "rc": 0, "results": ["Installed: htop-2.2.0-4.fc30.x86_64", "Installed: atop-2.4.0-3.fc30.x86_64"]}



PLAY RECAP **************************************************************************************************************************************************************************************************
localhost                  : ok=84   changed=67   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
master01                   : ok=85   changed=54   unreachable=0    failed=0    skipped=6    rescued=0    ignored=0   
monitor                    : ok=46   changed=35   unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
worker01                   : ok=91   changed=57   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
worker02                   : ok=91   changed=57   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
worker03                   : ok=91   changed=57   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
worker04                   : ok=91   changed=57   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
zeppelin                   : ok=9    changed=6    unreachable=0    failed=1    skipped=0    rescued=0    ignored=1   


# Try again

# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"


	> Done

	

# -----------------------------------------------------
# Create everything, using a standard config.
#[root@ansibler]

    time \
        /deployments/hadoop-yarn/bin/create-all.sh \
            "${cloudname:?}" \
            'medium-04'
	

      # Success

    	> real	39m47.583s
	> user	10m28.012s
	> sys	2m47.932s



# -----------------------------------------------------
# Get the public IP address of our Zeppelin node.
#[root@ansibler]

    deployname=$(
        yq read \
            '/tmp/aglais-status.yml' \
                'aglais.status.deployment.name'
        )

    zeppelinid=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            server list \
                --format json \
        | jq -r '.[] | select(.Name == "'${deployname:?}'-zeppelin") | .ID'
        )

    zeppelinip=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            server show \
                --format json \
                "${zeppelinid:?}" \
        | jq -r '.addresses' \
        | sed '
            s/[[:space:]]//
            s/.*=\(.*\)/\1/
            s/.*,\(.*\)/\1/
            '
        )

cat << EOF
Zeppelin ID [${zeppelinid:?}]
Zeppelin IP [${zeppelinip:?}]
EOF


  > Zeppelin ID [0f402cc1-9c0b-4d8f-95e5-156d0115e7f5]
  > Zeppelin IP [128.232.227.136]




# -----------------------------------------------------
# Add the Zeppelin user accounts.
#[root@ansibler]

    ssh zeppelin

        pushd "${HOME}"
        ln -s "zeppelin-0.8.2-bin-all" "zeppelin"

            pushd "zeppelin"
 
                # Install nano
                sudo yum install -y nano

                # Manual edit to add names and passwords
                nano conf/shiro.ini

                # Restart Zeppelin for the changes to take.
                ./bin/zeppelin-daemon.sh restart

            popd
        popd
    exit



# -----------------------------------------------------
# Setup integration with github
#[root@ansibler]


  ssh zeppelin \
        '
        export githubuser=username_encodede
        export githubpass=pass_encoded

        rm -rf /home/fedora/zeppelin-0.8.2-bin-all/notebook
        git clone https://${githubuser:?}:${githubpass:?}@github.com/wfau/aglais-notebooks.git /home/fedora/zeppelin-0.8.2-bin-all/notebook

        cat > "${HOME}/zeppelin-0.8.2-bin-all/notebook/.git/hooks/post-commit" << EOF
        #!/bin/sh
        git push

        EOF

        chmod +x ${HOME}/zeppelin-0.8.2-bin-all/notebook/.git/hooks/post-commit
        /home/fedora/zeppelin-0.8.2-bin-all/bin/zeppelin-daemon.sh restart
        '

        > Cloning into '/home/fedora/zeppelin-0.8.2-bin-all/notebook'...
        >  Zeppelin stop                                              [  OK  ]
        >  Zeppelin start                                             [  OK  ]

        # Success


# -----------------------------------------------------
# Login via Firefox
#[user@desktop]

    firefox --new-window "http://128.232.227.187:8080/" &

   # user1 = admin
   # user2 = gaiauser


# -----------------------------------------------------
# Experiment with multiple users
#[user@desktop]

   
   # Login as user #1

   # Run imported notebooks from Zeppelin:  

     /AglaisPublicExamples/SetUp                                                          
        https://raw.githubusercontent.com/wfau/aglais-notebooks/main/2G7GZKWUH/note.json

        [Success]
   

     /AglaisPublicExamples/Good astrometric solutions via ML Random Forrest classifier   
        https://raw.githubusercontent.com/wfau/aglais-notebooks/main/2G5NU6HTK/note.json


        # Results
	Raw catalogue with selected columns
	1724028
	FINISHED   
	Took 5 min 58 sec. Last updated by admin at June 16 2021, 7:36:57 PM.

	
        Train up the Random Forrest
	FINISHED   
	Took 3 min 37 sec. Last updated by admin at June 16 2021, 7:40:44 PM.

        [Success]


    # While this is running...

    # Login as user #2
    # Repeat above steps of importing and running the two notebooks
   
    # Check Spark UI  & Report
    # 1 application running, using up 95% of resources

    # Notebook of user #2 stuck at pending, while job of user #1 is running

    # .. A few minutes later, the notebook of user #1 is completed, and the notebooks of user #2 start, and complete successfully	    

   
# -----------------------------------------------------
# Experiment with different scopes, separating Contexts
#[user@desktop]


# http://zeppelin.apache.org/docs/0.8.2/usage/interpreter/interpreter_binding_mode.html#which-mode-should-i-use

# Login to Zeppelin as admin user

# Modify Spark interpreter
# Set Interpreter instantiation to "Per User" in "Isolated" Process
# Check the "User Impersonate" option
# Restart Spark Interpreter


# Repeat above test

# At the first cell, we get an exception
  
  > org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: fedora is not allowed to impersonate gaiauser

# Undo impersonate step, and try again


# Run SetUp notebook for user #1
   # New application shows up in Yarn taking up 95% of cluster 
   # SetUp script completed

# Run SetUp notebook for user #2
   # New application shows up, 0% of cluster used, state = ACCEPTED, status = UNDEFINED
   # Stuck at RUNNING 0% 
    
# Restart Spark interpreter for user #1

# Job of user #2 now starts, and completes successfully, taking up the 95% of cluster


# So what seems to be the case, is that as we expected, the first job gets allocated the full cluster, and won't release the resources until the SparkContext is cleared (manually or automatically after some time I assume), which means that with the current configuration, multiple users would not be able to use it concurrently.


# What if we reduce the total allocation to half?


# ----------------------------------



# Set the configurations as follows:

# [fedora@zeppelin]

/opt/spark/spark-defaults.conf
..
spark.executor.instances       5
..


# Run test notebooks again:


# Run SetUp notebook for user #1
   # New application shows up in Yarn taking up 47% of cluster 
   # SetUp script completed
	
# Run SetUp notebook for user #2
   # New application shows up in Yarn taking up 47% of cluster 
   # SetUp script completed


# Run Good astrometric solutions via ML Random Forrest classifier for user #1  

        # Results
	

	Raw catalogue with selected columns
	1724028
	FINISHED   
	Took 8 min 33 sec. Last updated by admin at June 17 2021, 11:50:57 AM.
        

	Train up the Random Forrest
	FINISHED
	Took 5 min 25 sec. Last updated by gaiauser at June 17 2021, 11:56:37 AM.
        [Success]


# While job is running, also start the same (different copy) notebook for user #2

        # Results

	Raw catalogue with selected columns
	1724028
	FINISHED
	Took 7 min 56 sec. Last updated by gaiauser at June 17 2021, 11:50:57 AM.


	Train up the Random Forrest
	FINISHED   
	Took 5 min 31 sec. Last updated by admin at June 17 2021, 11:56:43 AM.

        [Success]


# This works, so in theory we could set the maximum resource allocation per user to  set amount, as a fraction of the total resources.

# However perhaps this is not the most efficient way to have a multi user setup
# Unless we expect multiple concurrent users running jobs at the same time, our resources will be underutilized


# -----------------------------------------------------
# Experiment with Dynamic Allocation
#[user@desktop]

# Modify /opt/spark/spark-defaults.conf

#spark.executor.instances       5
spark.dynamicAllocation.enabled          true
spark.shuffle.service.enabled            true


# Run test notebooks again:


# Run SetUp notebook for user #1
   # New application shows up in Yarn taking up 14.5% of cluster, 25Gb RAM, 2 Container, 2 VCPU
   # Notebook stuck at cell: "Show details of databases and tables"
	
# Run SetUp notebook for user #2
   # New application shows up in Yarn taking up 14.5% of cluster, 25Gb RAM, 2 Container, 2 VCPU (Same as user #1)
   # Notebook stuck at cell: "Show details of databases and tables"


# In both cases, no executors are assigned to the job..(After checking the Spark UI)

# ----------------
# Try setting min / max / initial # of executors for dynamic allocation

# Modify /opt/spark/spark-defaults.conf

#spark.executor.instances       5


spark.dynamicAllocation.enabled          true
spark.shuffle.service.enabled            true
spark.dynamicAllocation.minExecutors     5
spark.dynamicAllocation.maxExecutors    11
spark.dynamicAllocation.initialExecutors  3



# Run test notebooks again:


# Run SetUp notebook for user #1
   # New application shows up in Yarn resource usage alternating between 45 - 87%, and alternating number of containers (7-11) and Memory used (80 - 150 Gb)
   # Notebook stuck at cell: "Show details of databases and tables"


# Check logs of worker node..


2021-06-17 14:18:33,204 ERROR yarn.YarnAllocator: Failed to launch executor 10056 on container container_1623926060683_0008_01_010057
org.apache.spark.SparkException: Exception while starting container container_1623926060683_0008_01_010057 on host worker02
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:125)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator$$anonfun$runAllocatedContainers$1$$anon$2.run(YarnAllocator.scala:546)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.yarn.exceptions.InvalidAuxServiceException: The auxService:spark_shuffle does not exist
	at sun.reflect.GeneratedConstructorAccessor29.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl.instantiateException(SerializedExceptionPBImpl.java:168)
	at org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl.deSerialize(SerializedExceptionPBImpl.java:106)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:205)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:122)
	... 5 more
2021-06-17 14:18:33,204 ERROR yarn.YarnAllocator: Failed to launch executor 10059 on container container_1623926060683_0008_01_010060
org.apache.spark.SparkException: Exception while starting container container_1623926060683_0008_01_010060 on host worker02
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:125)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:65)
	at org.apache.spark.deploy.yarn.YarnAllocator$$anonfun$runAllocatedContainers$1$$anon$2.run(YarnAllocator.scala:546)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.yarn.exceptions.InvalidAuxServiceException: The auxService:spark_shuffle does not exist
	at sun.reflect.GeneratedConstructorAccessor29.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl.instantiateException(SerializedExceptionPBImpl.java:168)
	at org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl.deSerialize(SerializedExceptionPBImpl.java:106)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:205)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:122)
	... 5 more



# https://stackoverflow.com/questions/30921838/auxservicemapreduce-shuffle-does-not-exist-on-hive


yarn settings

  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>

  <property>
    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
    <value>org.apache.spark.network.yarn.YarnShuffleService</value>
  </property>

  <property>
       <name>yarn.app.mapreduce.am.resource.mb</name>
       <value>2048</value>
  </property>




# After some trial & error, it seems the the right settings are:

 <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>spark_shuffle</value>
  </property>

  <property>
    <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
    <value>org.apache.spark.network.yarn.YarnShuffleService</value>
  </property>

  <property>
       <name>yarn.app.mapreduce.am.resource.mb</name>
       <value>2048</value>
  </property>


# We also need to add the folowing class to the hadoop lib directory of each node
# From master, copy /opt/spark/yarnspark-2.4.7-yarn-shuffle.jar -> /opt/hadoop/share/hadoop/yarn/ on each worker & master nodes

# Restart hadoop
# In master:
  stop-all.sh
  start-all.sh


# Repeat step from above, running the notebooks for each user concurrently

# Both notebooks start successfully..
# After checking yarn UI, we see 47.1% usage for each application (two applications started, 1 per user)

# Let's try killing one of the two applications, to see what happens
# Restart Spark interpreter for user #2

# Application for user #1 now jumps to 95% usage, so it looks like the load balancing is working

