#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2015, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

########################################################
## Create 3 VMs
########################################################
Stevedore@Cadelicia (Master Node)
Stevedore@Froeseth (Worker Node)
Stevedore@Abecien (Worker Node)



########################################################
## Create Docker Swarm and connect them
########################################################
ssh Stevedore@Cadelicia
	docker swarm init --advertise-addr 192.168.201.11


	## Firewall Ports for Swarm

	sudo su
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=7946/tcp --permanent
	    firewall-cmd --add-port=4789/tcp --permanent
	    firewall-cmd --add-port=4789/udp --permanent
	    firewall-cmd --add-port=9000/tcp --permanent
	    firewall-cmd --add-port=9000/udp --permanent
	    firewall-cmd --add-port=9866/tcp --permanent
	    firewall-cmd --add-port=9866/udp --permanent
	    firewall-cmd --add-port=8030/tcp --permanent
	    firewall-cmd --add-port=8030/udp --permanent
	    firewall-cmd --add-port=8031/tcp --permanent
	    firewall-cmd --add-port=8031/udp --permanent
	    firewall-cmd --add-port=8032/tcp --permanent
	    firewall-cmd --add-port=8032/udp --permanent
	    firewall-cmd --add-port=8033/tcp --permanent
	    firewall-cmd --add-port=8033/udp --permanent
	    firewall-cmd --add-port=8042/tcp --permanent
	    firewall-cmd --add-port=8042/udp --permanent
	    firewall-cmd --add-port=8050/tcp --permanent
	    firewall-cmd --add-port=8050/udp --permanent
	    iptables -A INPUT -p 50 -j ACCEPT    
	    firewall-cmd --reload
	exit
exit


Stevedore@Froeseth
        docker swarm join --token SWMTKN-1-1dpw6aqhjgu14bwv53smgrb1ftyvnhtd5b5ja946v9g352jah0-012oy90p9zgq3l2x4ybl2vvjy 192.168.201.11:2377

	sudo su
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=7946/tcp --permanent
	    firewall-cmd --add-port=4789/tcp --permanent
	    firewall-cmd --add-port=4789/udp --permanent
	    firewall-cmd --add-port=9000/tcp --permanent
	    firewall-cmd --add-port=9000/udp --permanent
	    firewall-cmd --add-port=9866/tcp --permanent
	    firewall-cmd --add-port=9866/udp --permanent
	    firewall-cmd --add-port=8030/tcp --permanent
	    firewall-cmd --add-port=8030/udp --permanent
	    firewall-cmd --add-port=8031/tcp --permanent
	    firewall-cmd --add-port=8031/udp --permanent
	    firewall-cmd --add-port=8032/tcp --permanent
	    firewall-cmd --add-port=8032/udp --permanent
	    firewall-cmd --add-port=8033/tcp --permanent
	    firewall-cmd --add-port=8033/udp --permanent
	    firewall-cmd --add-port=8042/tcp --permanent
	    firewall-cmd --add-port=8042/udp --permanent
	    firewall-cmd --add-port=8050/tcp --permanent
	    firewall-cmd --add-port=8050/udp --permanent
	    iptables -A INPUT -p 50 -j ACCEPT    
	    firewall-cmd --reload
	exit
exit


Stevedore@Abecien
        docker swarm join --token SWMTKN-1-1dpw6aqhjgu14bwv53smgrb1ftyvnhtd5b5ja946v9g352jah0-012oy90p9zgq3l2x4ybl2vvjy 192.168.201.11:2377

	sudo su
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=2377/tcp --permanent
	    firewall-cmd --add-port=7946/tcp --permanent
	    firewall-cmd --add-port=4789/tcp --permanent
	    firewall-cmd --add-port=4789/udp --permanent
	    firewall-cmd --add-port=9000/tcp --permanent
	    firewall-cmd --add-port=9000/udp --permanent
	    firewall-cmd --add-port=9866/tcp --permanent
	    firewall-cmd --add-port=9866/udp --permanent
	    firewall-cmd --add-port=8030/tcp --permanent
	    firewall-cmd --add-port=8030/udp --permanent
	    firewall-cmd --add-port=8031/tcp --permanent
	    firewall-cmd --add-port=8031/udp --permanent
	    firewall-cmd --add-port=8032/tcp --permanent
	    firewall-cmd --add-port=8032/udp --permanent
	    firewall-cmd --add-port=8033/tcp --permanent
	    firewall-cmd --add-port=8033/udp --permanent
	    firewall-cmd --add-port=8042/tcp --permanent
	    firewall-cmd --add-port=8042/udp --permanent
	    firewall-cmd --add-port=8050/tcp --permanent
	    firewall-cmd --add-port=8050/udp --permanent
	    iptables -A INPUT -p 50 -j ACCEPT    
	    firewall-cmd --reload
	exit
exit

########################################################
## Update Hosts file on each node
########################################################
..

127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

192.168.201.8       Delild
192.168.201.9       Abecien
192.168.201.10      Saewan
192.168.201.11      Cadelicia
192.168.201.12      Froeseth
192.168.201.13      Astoalith
192.168.201.14      Erennon
192.168.201.15      Gworewia

..



## ---------------------------------------------------------------------- Setup Hadoop -----------------------------------------------------------------------



########################################################
## Fetch Hadoop Binaries on each node
########################################################

ssh Stevedore@Cadelicia

	wget http://apache.cs.utah.edu/hadoop/common/current/hadoop-3.1.3.tar.gz
	tar -xzf hadoop-3.1.3.tar.gz
	mv hadoop-3.1.3 hadoop


## Fetch Hadoop Binaries on Worker node
ssh Stevedore@Froeseth

	wget http://apache.cs.utah.edu/hadoop/common/current/hadoop-3.1.3.tar.gz
	tar -xzf hadoop-3.1.3.tar.gz
	mv hadoop-3.1.3 hadoop


## Fetch Hadoop Binaries on Worker node
ssh Stevedore@Abecien

	wget http://apache.cs.utah.edu/hadoop/common/current/hadoop-3.1.3.tar.gz
	tar -xzf hadoop-3.1.3.tar.gz
	mv hadoop-3.1.3 hadoop


########################################################
## Setup Hadoop Configuration
########################################################
ssh Stevedore@Cadelicia

	## Install java
	sudo  dnf install java-11-openjdk.x86_64

	nano .profile
	..
	PATH=/home/Stevedore/hadoop/bin:/home/Stevedore/hadoop/sbin:$PATH
	..


	nano .bashrc
	..
	export HADOOP_HOME=/home/Stevedore/hadoop
	export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
	export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.212.b04-0.fc28.x86_64/jre
	..

## Setup Configuration files


## hadoop/etc/hadoop/core-site.xml 

<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://Cadelicia:9000</value>
        </property>
    </configuration>




## hadoop/etc/hadoop/yarn-site.xml
<configuration>
    <property>
            <name>yarn.acl.enable</name>
            <value>0</value>
    </property>

    <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>192.168.201.11</value>
    </property>

    <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
    </property>
<property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>1536</value>
</property>

<property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>1536</value>
</property>

<property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>128</value>
</property>

<property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
</property>
<property>
   <name>yarn.scheduler.capacity.root.support.user-limit-factor</name>  
   <value>2</value>
</property>
<property>
   <name>yarn.nodemanager.disk-health-checker.min-healthy-disks</name>
   <value>0.0</value>
</property>
<property>
   <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
   <value>100.0</value>
</property>

 <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>Cadelicia:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address</name>
    <value>Cadelicia:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>Cadelicia:8088</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>Cadelicia:8031</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>Cadelicia:8033</value>
  </property>

</configuration>



## hadoop/etc/hadoop/workers

Abecien
Froeseth




## hadoop/etc/hadoop/mapred-site.xml
<configuration>
    <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
    </property>
    <property>
            <name>yarn.app.mapreduce.am.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
            <name>mapreduce.map.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
            <name>mapreduce.reduce.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
<property>
	<name>yarn.app.mapreduce.am.resource.mb</name>
        <value>512</value>
</property>

<property>
	<name>mapreduce.map.memory.mb</name>
        <value>256</value>
</property>

<property>
	<name>mapreduce.reduce.memory.mb</name>
        <value>256</value>
</property>
</configuration>



## hadoop/etc/hadoop/core-site.xml

<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://Cadelicia:9000</value>
        </property>
    </configuration>


## "Memory allocation can be tricky on low RAM nodes because default values are not suitable for nodes with less than 8GB of RAM"

## Copy Configuration files to Worker nodes

for node in Abecien Froeseth; do
    scp ~/hadoop/etc/hadoop/* $node:/home/Stevedore/hadoop/etc/hadoop/;
done



################################################
## Format HDFS Master Node
################################################

mkdir /home/hadoop/
sudo chown -R Stevedore:root /home/hadoop/

source /home/Stevedore/hadoop/bin/hdfs namenode -format


...



2019-10-22 18:30:51,857 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
2019-10-22 18:30:51,857 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-10-22 18:30:51,859 INFO util.GSet: Computing capacity for map NameNodeRetryCache
2019-10-22 18:30:51,859 INFO util.GSet: VM type       = 64-bit
2019-10-22 18:30:51,860 INFO util.GSet: 0.029999999329447746% max memory 876.5 MB = 269.3 KB
2019-10-22 18:30:51,860 INFO util.GSet: capacity      = 2^15 = 32768 entries
2019-10-22 18:30:51,892 INFO namenode.FSImage: Allocated new BlockPoolId: BP-105882924-192.168.201.11-1571765451881
2019-10-22 18:30:51,909 INFO common.Storage: Storage directory /home/hadoop/data/nameNode has been successfully formatted.
2019-10-22 18:30:51,953 INFO namenode.FSImageFormatProtobuf: Saving image file /home/hadoop/data/nameNode/current/fsimage.ckpt_0000000000000000000 using no compression
2019-10-22 18:30:52,144 INFO namenode.FSImageFormatProtobuf: Image file /home/hadoop/data/nameNode/current/fsimage.ckpt_0000000000000000000 of size 396 bytes saved in 0 seconds .
2019-10-22 18:30:52,167 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2019-10-22 18:30:52,176 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid = 0 when meet shutdown.
2019-10-22 18:30:52,178 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at Cadelicia/192.168.201.11
************************************************************/
Connection to cadelicia closed.


################################################
## Start and Stop HDFS
################################################

start-dfs.sh

..

Starting namenodes on [Cadelicia]
Cadelicia: namenode is running as process 8597.  Stop it first.
Starting datanodes
Abecien: WARNING: /home/Stevedore/hadoop/logs does not exist. Creating.
Froeseth: WARNING: /home/Stevedore/hadoop/logs does not exist. Creating.
Starting secondary namenodes [Cadelicia]
Cadelicia: secondarynamenode is running as process 8838.  Stop it first.
[Stevedore@Cadelicia bin]$ 




## Create a directory
hdfs dfs -mkdir /user/hadoop/


## LS directory
hdfs dfs -ls /user/hadoop/




################################################
## Monitoring HDFS
################################################ 

## Get HDFS Report
hdfs dfsadmin -report

Configured Capacity: 66033025024 (61.50 GB)
Present Capacity: 41644593152 (38.78 GB)
DFS Remaining: 41643319296 (38.78 GB)
DFS Used: 1273856 (1.21 MB)
DFS Used%: 0.00%
Replicated Blocks:
	Under replicated blocks: 0
	Blocks with corrupt replicas: 0
	Missing blocks: 0
	Missing blocks (with replication factor 1): 0
	Low redundancy blocks with highest priority to recover: 0
	Pending deletion blocks: 0
Erasure Coded Block Groups: 
	Low redundancy block groups: 0
	Block groups with corrupt internal blocks: 0
	Missing block groups: 0
	Low redundancy blocks with highest priority to recover: 0
	Pending deletion blocks: 0

-------------------------------------------------
Live datanodes (2):

Name: 192.168.201.12:9866 (Froeseth)
Hostname: Froeseth
Decommission Status : Normal
Configured Capacity: 33016512512 (30.75 GB)
DFS Used: 1085440 (1.04 MB)
Non DFS Used: 15318958080 (14.27 GB)
DFS Remaining: 16428544000 (15.30 GB)
DFS Used%: 0.00%
DFS Remaining%: 49.76%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Wed Oct 23 12:07:44 BST 2019
Last Block Report: Wed Oct 23 06:21:49 BST 2019
Num of Blocks: 2


Name: 192.168.201.9:9866 (Abecien)
Hostname: Abecien
Decommission Status : Normal
Configured Capacity: 33016512512 (30.75 GB)
DFS Used: 188416 (184 KB)
Non DFS Used: 6057390080 (5.64 GB)
DFS Remaining: 25214775296 (23.48 GB)
DFS Used%: 0.00%
DFS Remaining%: 76.37%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Wed Oct 23 12:07:44 BST 2019
Last Block Report: Wed Oct 23 07:29:02 BST 2019
Num of Blocks: 1


## Check HDFS GUI

ssh -L '*:8084:Cadelicia:9870' Stevedore@Cadelicia

curl http://localhost:8084
...



## ---------------------------------------------------------------------- Setup Yarn -----------------------------------------------------------------------

"HDFS is a distributed storage system, and doesnâ€™t provide any services for running and scheduling tasks in the cluster. This is the role of the YARN framework"

## Start Yarn

start-yarn.sh
# Starting resourcemanager
# Starting nodemanagers

yarn node -list

## Had to open some additional ports, and add some configuration parameters before I could get the additional worker nodes to show up

2019-10-23 14:11:24,504 INFO client.RMProxy: Connecting to ResourceManager at Cadelicia/192.168.201.11:8032
Total Nodes:2
         Node-Id	     Node-State	Node-Http-Address	Number-of-Running-Containers
  Froeseth:37629	        RUNNING	    Froeseth:8042	                           0
   Abecien:37539	        RUNNING	     Abecien:8042	                           0



