#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


    Magnum cluster
        20200828-01-magnum-cluster.txt
        Using Openstack directly, not Terraform.

    Nginx-controller
        20200807-06-nginx-ingress.txt
            Helm based deploy.

    Dashboard
        20200807-07-dashboard.txt
            Helm based deploy, followed by kubectl additions.

    CephFS-router
        20200820-05-cephfs-router.txt
            Openstack calls to get configuration.
            Terraform deploy.

    CephFS-provider
        20200821-01-cephfs-provider.txt
            Kubectl based deploy.


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --hostname kubernator \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --env "clustername=${CLUSTER_NAME:?}" \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml:z" \
        --volume "${AGLAIS_CODE}/experiments/zrq/kubernetes:/kubernetes:z" \
        atolmis/openstack-client \
        bash

# -----------------------------------------------------
# Get the connection details for our cluster.
#[user@kubernator]

    mkdir -p "${HOME}/.kube"
    openstack \
        --os-cloud "${cloudname:?}" \
        coe cluster config \
            "${clustername:?}" \
                --force \
                --dir "${HOME}/.kube"


    kubectl \
        cluster-info

    >   Kubernetes master is running at https://....
    >   Heapster is running at https://....
    >   CoreDNS is running at https://....


# -----------------------------------------------------
# Set the Manila API version.
# https://stackoverflow.com/a/58806536
#[user@kubernator]

    export OS_SHARE_API_VERSION=2.51


# -----------------------------------------------------
# List our current shares.
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
            share list

    >   +--------------------------------------+----------------+------+-------------+-----------+-----------+------------------+------+-------------------+
    >   | ID                                   | Name           | Size | Share Proto | Status    | Is Public | Share Type Name  | Host | Availability Zone |
    >   +--------------------------------------+----------------+------+-------------+-----------+-----------+------------------+------+-------------------+
    >   | ad1d9ca2-5b1c-4064-8c74-695286de6098 | gaia-dr2-share | 4399 | CEPHFS      | available | True      | cephfsnativetype |      | nova              |
    >   +--------------------------------------+----------------+------+-------------+-----------+-----------+------------------+------+-------------------+


# -----------------------------------------------------
# Get the details of our existing share.
#[user@kubernator]

    shareid=ad1d9ca2-5b1c-4064-8c74-695286de6098

    openstack \
        --os-cloud "${cloudname:?}" \
            share show \
                --format json \
                "${shareid:?}"

    >   {
    >     "access_rules_status": "active",
    >     "availability_zone": "nova",
    >     "create_share_from_snapshot_support": false,
    >     "created_at": "2020-08-21T19:26:12.000000",
    >     "description": "",
    >     "export_locations": "\npath = 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/0d4ce629-d290-4d7d-9d5e-9b94593196a0\nid = c8dd7596-c708-4c99-91e1-67021e99171a\npreferred = False",
    >     "has_replicas": false,
    >     "id": "ad1d9ca2-5b1c-4064-8c74-695286de6098",
    >     "is_public": true,
    >     "mount_snapshot_support": false,
    >     "name": "gaia-dr2-share",
    >     "project_id": "21b4ae3a2ea44bc5a9c14005ed2963af",
    >     "properties": {
    >       "kubernetes.io/created-for/pv/name": "pvc-b64d2646-55ca-4402-b6b3-76f70e47aa2c",
    >       "kubernetes.io/created-for/pvc/namespace": "default",
    >       "kubernetes.io/created-for/pvc/name": "gaia-dr2-volume-claim"
    >     },
    >     "replication_type": null,
    >     "revert_to_snapshot_support": false,
    >     "share_group_id": null,
    >     "share_network_id": null,
    >     "share_proto": "CEPHFS",
    >     "share_type": "5d0f58c5-ed21-4e1f-91bb-fe1a49deb5d8",
    >     "share_type_name": "cephfsnativetype",
    >     "size": 4399,
    >     "snapshot_id": null,
    >     "snapshot_support": false,
    >     "source_share_group_snapshot_member_id": null,
    >     "status": "available",
    >     "task_state": null,
    >     "user_id": "98169f87de174ad4ac98c32e59646488",
    >     "volume_type": "cephfsnativetype"
    >   }

    #
    # Some metadata linking it to the original PersistentVolume
    # and PersistentVolumeClaim.
    #


# -----------------------------------------------------
# List our current share access rules.
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
            share access list \
                "${shareid:?}"

    >   +--------------------------------------+-------------+----------------+--------------+--------+----------------+----------------------------+----------------------------+
    >   | id                                   | access_type | access_to      | access_level | state  | access_key     | created_at                 | updated_at                 |
    >   +--------------------------------------+-------------+----------------+--------------+--------+----------------+----------------------------+----------------------------+
    >   | b6f44adf-2b00-481d-bfb2-99398af5c2de | cephx       | gaia-dr2-share | ro           | active | AQAL........== | 2020-08-28T01:42:34.000000 | 2020-08-28T01:42:35.000000 |
    >   +--------------------------------------+-------------+----------------+--------------+--------+----------------+----------------------------+----------------------------+


    openstack \
        --os-cloud "${cloudname:?}" \
            share access list \
                --format json \
                "${shareid:?}"

    >   [
    >     {
    >       "id": "b6f44adf-2b00-481d-bfb2-99398af5c2de",
    >       "access_type": "cephx",
    >       "access_to": "gaia-dr2-share",
    >       "access_level": "ro",
    >       "state": "active",
    >       "access_key": "AQAL........==",
    >       "created_at": "2020-08-28T01:42:34.000000",
    >       "updated_at": "2020-08-28T01:42:35.000000"
    >     }
    >   ]

    #
    # From the Manila provisioner documentation.
    # (*) now replaced by CSI
    # https://github.com/gman0/cloud-provider-openstack/blob/ef56215b90cac0cf92d1f750f2ab2e88e2ec01d5/docs/using-manila-provisioner.md

        "In order to use an existing Manila share, parameters [osShareID or osShareName] and osShareAccessID must be supplied."

    #
    # Selector-Label Volume Binding
    # https://docs.openshift.com/container-platform/3.11/install_config/persistent_storage/selector_label_binding.html

        To bind a PVC to a specific PV as a cluster administrator:

            Use pvc.spec.volumeName if you know the PV name.

            Use pvc.spec.selector if you know the PV labels.

        By specifying a selector, the PVC requires the PV to have specific labels


        Pre-bind the PV to your PVC using the PVC namespace and name.
        A PV defined as such will bind only to the specified PVC and to nothing else, as shown in the following example:

            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: mktg-ops--kafka--kafka-broker01
            spec:
              capacity:
                storage: 15Gi
              accessModes:
                - ReadWriteOnce
              claimRef:
                  apiVersion: v1
                  kind: PersistentVolumeClaim
                  name: kafka-broker01
                  namespace: default


    #
    # Our original dynamically allocated volume :
    #

        {
            "apiVersion": "v1",
            "kind": "PersistentVolume",
            "metadata": {
                "annotations": {
                    "manila.cloud-provider-openstack.kubernetes.io/ID": "503db06d-0a85-4d58-a771-76caabf66868",
                    "manila.cloud-provider-openstack.kubernetes.io/OSSecretName": "os-trustee",
                    "manila.cloud-provider-openstack.kubernetes.io/OSSecretNamespace": "kube-system",
                    "manila.cloud-provider-openstack.kubernetes.io/ProvisionType": "dynamic",
                    "manila.cloud-provider-openstack.kubernetes.io/ShareSecretName": "manila-16da93cd-e35b-11ea-9386-0a580a640251",
                    "manila.cloud-provider-openstack.kubernetes.io/ShareSecretNamespace": "default",
                    "pv.kubernetes.io/provisioned-by": "manila-provisioner"
                },
            "creationTimestamp": "2020-08-21T03:05:01Z",
            "finalizers": [
                "kubernetes.io/pv-protection"
            ],
            "name": "pvc-10bb7a62-5fdf-4399-99c4-a37f01634f65",
            "resourceVersion": "2126851",
            "selfLink": "/api/v1/persistentvolumes/pvc-10bb7a62-5fdf-4399-99c4-a37f01634f65",
            "uid": "5998da29-9fd3-444d-974a-0ccce5d7f4e8"
        },
        "spec": {
            "accessModes": [
                "ReadWriteOnce"
            ],
            "capacity": {
                "storage": "4399G"
            },
            "cephfs": {
                "monitors": [
                    "10.206.1.5:6789",
                    "10.206.1.6:6789",
                    "10.206.1.7:6789"
                ],
                "path": "/volumes/_nogroup/616d38f4-d4fe-4421-bd03-e8fe5ae3ddb0",
                "secretRef": {
                    "name": "manila-16da93cd-e35b-11ea-9386-0a580a640251",
                    "namespace": "default"
                },
                "user": "pvc-10bb7a62-5fdf-4399-99c4-a37f01634f65"
            },
            "claimRef": {
                "apiVersion": "v1",
                "kind": "PersistentVolumeClaim",
                "name": "gaia-dr2-volume-claim",
                "namespace": "default",
                "resourceVersion": "2126819",
                "uid": "10bb7a62-5fdf-4399-99c4-a37f01634f65"
            },
            "persistentVolumeReclaimPolicy": "Delete",
            "storageClassName": "manila-cephfs-storage-class",
            "volumeMode": "Filesystem"
        },
       "status": {
           "phase": "Bound"
       }
   }


    #
    # Key parts of the spec:
    #

       "spec": {
           "accessModes": [
               "ReadWriteOnce"
           ],
           "capacity": {
               "storage": "4399G"
           },
           "cephfs": {
               "monitors": [
                   "10.206.1.5:6789",
                   "10.206.1.6:6789",
                   "10.206.1.7:6789"
               ],
               "path": "/volumes/_nogroup/616d38f4-d4fe-4421-bd03-e8fe5ae3ddb0",
               "secretRef": {
                   "name": "manila-16da93cd-e35b-11ea-9386-0a580a640251",
                   "namespace": "default"
               },
               "user": "pvc-10bb7a62-5fdf-4399-99c4-a37f01634f65"
           },
           "claimRef": {
               "apiVersion": "v1",
               "kind": "PersistentVolumeClaim",
               "name": "gaia-dr2-volume-claim",
               "namespace": "default",
               "resourceVersion": "2126819",
               "uid": "10bb7a62-5fdf-4399-99c4-a37f01634f65"
           },
           "persistentVolumeReclaimPolicy": "Delete",
           "storageClassName": "manila-cephfs-storage-class",
           "volumeMode": "Filesystem"
       },


    #
    # Some match up, others I'm not sure where they came from.
    # Problem is we don't have complete data - the PersistenVolumeClaim we have in our notes doesn't come from current Manila share.
    # We need to create a new dynamic share and see how the fields match up.
    #


# -----------------------------------------------------
# Create our test claim.
#[user@kubernator]

    cat > /tmp/test-claim.yaml << EOF

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    aglais-tag: test-claim-tag
  name: test-claim
spec:
  storageClassName: manila-cephfs-storage-class
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 4Mi

EOF

    kubectl \
        apply \
            --filename /tmp/test-claim.yaml

    >   persistentvolumeclaim/test-claim created


# -----------------------------------------------------
# Create a test Pod.
#[user@kubernator]

    cat > /tmp/test-pod.yaml << EOF

kind: Pod
apiVersion: v1
metadata:
  name: test-pod-01
  namespace: default
spec:
  volumes:
    - name: test-data
      persistentVolumeClaim:
        claimName: test-claim
    - name: local-data
      emptyDir: {}
  containers:
    - name: test-container-01
      image: 'fedora:latest'
      volumeMounts:
        - name: test-data
          mountPath: /test-data
        - name: local-data
          mountPath: /local-data
      command: ["/bin/sh"]
      args:
        - "-c"
        - >-
          while true; do
          date >> /local-data/date-log.txt;
          sleep 1;
          done
EOF

    kubectl \
        apply \
            --filename /tmp/test-pod.yaml

    >   pod/test-pod-01 created


# -----------------------------------------------------
# Check the available space.
#[user@kubernator]

    kubectl exec \
        --tty \
        --stdin \
        test-pod-01 \
        -- \
            df -h /test-data/

    >   Filesystem                                                                                              Size  Used Avail Use% Mounted on
    >   10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/994a8a17-0220-424c-b028-35482e2143bf  1.0G     0  1.0G   0% /test-data


# -----------------------------------------------------
# Get details of the PersistentVolumeClaim.
#[user@kubernator]

    kubectl \
        --output json \
        get persistentvolumeclaim test-claim

    >   {
    >       "apiVersion": "v1",
    >       "kind": "PersistentVolumeClaim",
    >       "metadata": {
    >           "annotations": {
    >               "kubectl.kubernetes.io/last-applied-configuration": "{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolumeClaim\",\"metadata\":{\"annotations\":{},\"labels\":{\"aglais-tag\":\"test-claim-tag\"},\"name\":\"test-claim\",\"namespace\":\"default\"},\"spec\":{\"accessModes\":[\"ReadWriteMany\"],\"resources\":{\"requests\":{\"storage\":\"4Mi\"}},\"storageClassName\":\"manila-cephfs-storage-class\"}}\n",
    >               "pv.kubernetes.io/bind-completed": "yes",
    >               "pv.kubernetes.io/bound-by-controller": "yes",
    >               "volume.beta.kubernetes.io/storage-provisioner": "manila-provisioner"
    >           },
    >           "creationTimestamp": "2020-08-29T02:32:35Z",
    >           "finalizers": [
    >               "kubernetes.io/pvc-protection"
    >           ],
    >           "labels": {
    >               "aglais-tag": "test-claim-tag"
    >           },
    >           "name": "test-claim",
    >           "namespace": "default",
    >           "resourceVersion": "449141",
    >           "selfLink": "/api/v1/namespaces/default/persistentvolumeclaims/test-claim",
    >           "uid": "41814ac5-00cc-4ba3-ae8b-6eddad01ef0c"
    >       },
    >       "spec": {
    >           "accessModes": [
    >               "ReadWriteMany"
    >           ],
    >           "resources": {
    >               "requests": {
    >                   "storage": "4Mi"
    >               }
    >           },
    >           "storageClassName": "manila-cephfs-storage-class",
    >           "volumeMode": "Filesystem",
    >           "volumeName": "pvc-41814ac5-00cc-4ba3-ae8b-6eddad01ef0c"
    >       },
    >       "status": {
    >           "accessModes": [
    >               "ReadWriteMany"
    >           ],
    >           "capacity": {
    >               "storage": "1G"
    >           },
    >           "phase": "Bound"
    >       }
    >   }


# -----------------------------------------------------
# Get details of the PersistentVolume.
#[user@kubernator]

    volname=$(
        kubectl \
            --output json \
            get persistentvolumeclaim test-claim \
        | jq -r '.spec.volumeName'
        )

    kubectl \
        --output json \
        get persistentvolume \
           "${volname:?}"


    >   {
    >       "apiVersion": "v1",
    >       "kind": "PersistentVolume",
    >       "metadata": {
    >           "annotations": {
    >               "manila.cloud-provider-openstack.kubernetes.io/ID": "d71dae9a-b508-4a54-a5e9-7a91e8548b1e",
    >               "manila.cloud-provider-openstack.kubernetes.io/OSSecretName": "os-trustee",
    >               "manila.cloud-provider-openstack.kubernetes.io/OSSecretNamespace": "kube-system",
    >               "manila.cloud-provider-openstack.kubernetes.io/ProvisionType": "dynamic",
    >               "manila.cloud-provider-openstack.kubernetes.io/ShareSecretName": "manila-e80564ee-e99f-11ea-8c83-0a580a640409",
    >               "manila.cloud-provider-openstack.kubernetes.io/ShareSecretNamespace": "default",
    >               "pv.kubernetes.io/provisioned-by": "manila-provisioner"
    >           },
    >           "creationTimestamp": "2020-08-29T02:32:45Z",
    >           "finalizers": [
    >               "kubernetes.io/pv-protection"
    >           ],
    >           "name": "pvc-41814ac5-00cc-4ba3-ae8b-6eddad01ef0c",
    >           "resourceVersion": "449139",
    >           "selfLink": "/api/v1/persistentvolumes/pvc-41814ac5-00cc-4ba3-ae8b-6eddad01ef0c",
    >           "uid": "f418356d-05e5-4edc-af9d-1fe16d26faed"
    >       },
    >       "spec": {
    >           "accessModes": [
    >               "ReadWriteMany"
    >           ],
    >           "capacity": {
    >               "storage": "1G"
    >           },
    >           "cephfs": {
    >               "monitors": [
    >                   "10.206.1.5:6789",
    >                   "10.206.1.6:6789",
    >                   "10.206.1.7:6789"
    >               ],
    >               "path": "/volumes/_nogroup/994a8a17-0220-424c-b028-35482e2143bf",
    >               "secretRef": {
    >                   "name": "manila-e80564ee-e99f-11ea-8c83-0a580a640409",
    >                   "namespace": "default"
    >               },
    >               "user": "pvc-41814ac5-00cc-4ba3-ae8b-6eddad01ef0c"
    >           },
    >           "claimRef": {
    >               "apiVersion": "v1",
    >               "kind": "PersistentVolumeClaim",
    >               "name": "test-claim",
    >               "namespace": "default",
    >               "resourceVersion": "449101",
    >               "uid": "41814ac5-00cc-4ba3-ae8b-6eddad01ef0c"
    >           },
    >           "persistentVolumeReclaimPolicy": "Delete",
    >           "storageClassName": "manila-cephfs-storage-class",
    >           "volumeMode": "Filesystem"
    >       },
    >       "status": {
    >           "phase": "Bound"
    >       }
    >   }


# -----------------------------------------------------
# Get details of the Manila share.
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
            share list

    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+
    >   | ID                                   | Name                                     | Size | Share Proto | Status    | Is Public | Share Type Name  | Host | Availability Zone |
    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+
    >   | ad1d9ca2-5b1c-4064-8c74-695286de6098 | gaia-dr2-share                           | 4399 | CEPHFS      | available | True      | cephfsnativetype |      | nova              |
    >   | d71dae9a-b508-4a54-a5e9-7a91e8548b1e | pvc-41814ac5-00cc-4ba3-ae8b-6eddad01ef0c |    1 | CEPHFS      | available | False     | cephfsnativetype |      | nova              |
    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+

    shareid=$(
        openstack \
            --os-cloud "${cloudname:?}" \
                share list \
                    --format json \
        | jq -r '.[1].ID'
        )

    openstack \
        --os-cloud "${cloudname:?}" \
            share show \
                --format json \
                "${shareid:?}"


    >   {
    >     "access_rules_status": "active",
    >     "availability_zone": "nova",
    >     "create_share_from_snapshot_support": false,
    >     "created_at": "2020-08-29T02:32:40.000000",
    >     "description": null,
    >     "export_locations": "\npath = 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/994a8a17-0220-424c-b028-35482e2143bf\nid = 5b413a59-20ad-4747-af2d-e1717e5505d9\npreferred = False",
    >     "has_replicas": false,
    >     "id": "d71dae9a-b508-4a54-a5e9-7a91e8548b1e",
    >     "is_public": false,
    >     "mount_snapshot_support": false,
    >     "name": "pvc-41814ac5-00cc-4ba3-ae8b-6eddad01ef0c",
    >     "project_id": "21b4ae3a2ea44bc5a9c14005ed2963af",
    >     "properties": {
    >       "kubernetes.io/created-for/pv/name": "pvc-41814ac5-00cc-4ba3-ae8b-6eddad01ef0c",
    >       "kubernetes.io/created-for/pvc/namespace": "default",
    >       "kubernetes.io/created-for/pvc/name": "test-claim"
    >     },
    >     "replication_type": null,
    >     "revert_to_snapshot_support": false,
    >     "share_group_id": null,
    >     "share_network_id": null,
    >     "share_proto": "CEPHFS",
    >     "share_type": "5d0f58c5-ed21-4e1f-91bb-fe1a49deb5d8",
    >     "share_type_name": "cephfsnativetype",
    >     "size": 1,
    >     "snapshot_id": null,
    >     "snapshot_support": false,
    >     "source_share_group_snapshot_member_id": null,
    >     "status": "available",
    >     "task_state": null,
    >     "user_id": "98169f87de174ad4ac98c32e59646488",
    >     "volume_type": "cephfsnativetype"
    >   }


    openstack \
        --os-cloud "${cloudname:?}" \
            share access list \
                --format json \
                "${shareid:?}"

    >   [
    >     {
    >       "id": "474be0df-773e-4025-a19c-f0953b17124a",
    >       "access_type": "cephx",
    >       "access_to": "pvc-41814ac5-00cc-4ba3-ae8b-6eddad01ef0c",
    >       "access_level": "rw",
    >       "state": "active",
    >       "access_key": "AQBM........==",
    >       "created_at": "2020-08-29T02:32:43.000000",
    >       "updated_at": "2020-08-29T02:32:44.000000"
    >     }
    >   ]


# -----------------------------------------------------
# List our Kubernetes secrets.
#[user@kubernator]

    kubectl \
        get secrets

    >   NAME                                                TYPE                                  DATA   AGE
    >   augusta-20200828-ingress-nginx-admission            Opaque                                3      15h
    >   augusta-20200828-ingress-nginx-token-cntsj          kubernetes.io/service-account-token   3      15h
    >   default-token-vjk7x                                 kubernetes.io/service-account-token   3      25h
    >   kubernetes-dashboard-csrf                           Opaque                                1      15h
    >   kubernetes-dashboard-key-holder                     Opaque                                2      15h
    >   manila-e80564ee-e99f-11ea-8c83-0a580a640409         Opaque                                1      17m
    >   manila-provisioner-account-token-xx2h4              kubernetes.io/service-account-token   3      15h
    >   sh.helm.release.v1.augusta-20200828.v1              helm.sh/release.v1                    1      15h
    >   sh.helm.release.v1.valeria-20200828.v1              helm.sh/release.v1                    1      15h
    >   valeria-20200828-kubernetes-dashboard-certs         Opaque                                0      15h
    >   valeria-20200828-kubernetes-dashboard-token-gbwns   kubernetes.io/service-account-token   3      15h
    >   valeria-account-20200828-token-mmzvn                kubernetes.io/service-account-token   3      15h


    kubectl \
        get secret \
            --output json \
            manila-e80564ee-e99f-11ea-8c83-0a580a640409



    >   {
    >       "apiVersion": "v1",
    >       "data": {
    >           "key": "QVFCTXZrbGY3eW8zRGhBQW9SUE9QSlpjbmlrbEE2cW85MU41SHc9PQ=="
    >       },
    >       "kind": "Secret",
    >       "metadata": {
    >           "creationTimestamp": "2020-08-29T02:32:44Z",
    >           "name": "manila-e80564ee-e99f-11ea-8c83-0a580a640409",
    >           "namespace": "default",
    >           "resourceVersion": "449133",
    >           "selfLink": "/api/v1/namespaces/default/secrets/manila-e80564ee-e99f-11ea-8c83-0a580a640409",
    >           "uid": "da877e4f-ef08-4609-9d92-a5f9377f34e5"
    >       },
    >       "type": "Opaque"
    >   }


# -----------------------------------------------------
# List our Kubernetes storage class.
#[user@kubernator]

    kubectl \
        get storageclass

    >   NAME                          PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
    >   manila-cephfs-storage-class   manila-provisioner   Delete          Immediate           false                  15h


    kubectl \
        get storageclass \
            --output json \
                'manila-cephfs-storage-class'

    >   {
    >       "apiVersion": "storage.k8s.io/v1",
    >       "kind": "StorageClass",
    >       "metadata": {
    >           "creationTimestamp": "2020-08-28T11:40:31Z",
    >           "name": "manila-cephfs-storage-class",
    >           "resourceVersion": "173222",
    >           "selfLink": "/apis/storage.k8s.io/v1/storageclasses/manila-cephfs-storage-class",
    >           "uid": "30362003-c2c3-49d0-90c0-b07e6c422592"
    >       },
    >       "parameters": {
    >           "backend": "cephfs",
    >           "osSecretName": "os-trustee",
    >           "osSecretNamespace": "kube-system",
    >           "protocol": "CEPHFS",
    >           "type": "cephfsnativetype",
    >           "zones": "nova"
    >       },
    >       "provisioner": "manila-provisioner",
    >       "reclaimPolicy": "Delete",
    >       "volumeBindingMode": "Immediate"
    >   }



# -----------------------------------------------------
# -----------------------------------------------------
# Create a Manila share.
# https://docs.openstack.org/python-openstackclient/latest/cli/plugin-commands/manila.html#share-create
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
            share create \
            --format json \
            --name 'magruela-share' \
            --share-type 'cephfsnativetype' \
            --availability-zone 'nova' \
            'CEPHFS' \
            5 \
    | tee /tmp/magruela-share.json

    >   {
    >     "access_rules_status": "active",
    >     "availability_zone": "nova",
    >     "create_share_from_snapshot_support": false,
    >     "created_at": "2020-08-29T13:17:01.000000",
    >     "description": null,
    >     "has_replicas": false,
    >     "id": "a32ea61f-a922-4b9d-959b-a9d2d2e57c4b",
    >     "is_public": false,
    >     "metadata": {},
    >     "mount_snapshot_support": false,
    >     "name": "magruela-share",
    >     "project_id": "21b4ae3a2ea44bc5a9c14005ed2963af",
    >     "replication_type": null,
    >     "revert_to_snapshot_support": false,
    >     "share_group_id": null,
    >     "share_network_id": null,
    >     "share_proto": "CEPHFS",
    >     "share_type": "5d0f58c5-ed21-4e1f-91bb-fe1a49deb5d8",
    >     "share_type_name": "cephfsnativetype",
    >     "size": 5,
    >     "snapshot_id": null,
    >     "snapshot_support": false,
    >     "source_share_group_snapshot_member_id": null,
    >     "status": "creating",
    >     "task_state": null,
    >     "user_id": "98169f87de174ad4ac98c32e59646488",
    >     "volume_type": "cephfsnativetype"
    >   }

    shareid=$(
        jq -r '.id' /tmp/magruela-share.json
        )

    openstack \
        --os-cloud "${cloudname:?}" \
            share show \
                --format json \
                "${shareid:?}" \
    | tee /tmp/magruela-share.json

    >   {
    >     "access_rules_status": "active",
    >     "availability_zone": "nova",
    >     "create_share_from_snapshot_support": false,
    >     "created_at": "2020-08-29T13:17:01.000000",
    >     "description": null,
    >     "export_locations": "\npath = 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121\nid = c005fd86-70ba-4ea6-acb4-db8d0dcb9b20\npreferred = False",
    >     "has_replicas": false,
    >     "id": "a32ea61f-a922-4b9d-959b-a9d2d2e57c4b",
    >     "is_public": false,
    >     "mount_snapshot_support": false,
    >     "name": "magruela-share",
    >     "project_id": "21b4ae3a2ea44bc5a9c14005ed2963af",
    >     "properties": {},
    >     "replication_type": null,
    >     "revert_to_snapshot_support": false,
    >     "share_group_id": null,
    >     "share_network_id": null,
    >     "share_proto": "CEPHFS",
    >     "share_type": "5d0f58c5-ed21-4e1f-91bb-fe1a49deb5d8",
    >     "share_type_name": "cephfsnativetype",
    >     "size": 5,
    >     "snapshot_id": null,
    >     "snapshot_support": false,
    >     "source_share_group_snapshot_member_id": null,
    >     "status": "available",
    >     "task_state": null,
    >     "user_id": "98169f87de174ad4ac98c32e59646488",
    >     "volume_type": "cephfsnativetype"
    >   }


# -----------------------------------------------------
# Extract the parts we need from the locations string.
# https://tldp.org/LDP/abs/html/string-manipulation.html
#[user@kubernator]

    locations=$(
        jq '.export_locations' /tmp/magruela-share.json
        )

    monitors=$(
        echo "${locations:?}" |
        sed '
            s/^.*path = \([^\\]*\).*$/\1/
            s/^\(.*\):\/\(.*\)$/\1/
            s/,/\n/g
            '
            )

    sharepath=$(
        echo "${locations:?}" |
        sed '
            s/^.*path = \([^\\]*\).*$/\1/
            s/^\(.*\):\/\(.*\)$/\2/
            '
            )

    sharesize=$(
        jq '.size' /tmp/magruela-share.json
        )

    for monitor in ${monitors}
    do
        echo "Monitor [${monitor}]"
    done

    >   Monitor [10.206.1.5:6789]
    >   Monitor [10.206.1.6:6789]
    >   Monitor [10.206.1.7:6789]

    echo "
Share path [${sharepath}]
Share size [${sharesize}]
"

    >   Share path [volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121]
    >   Share size [5]


# -----------------------------------------------------
# Create a Kubernetes volume.
# https://docs.openshift.com/container-platform/3.11/install_config/persistent_storage/selector_label_binding.html#selector-label-volume-define
#[user@kubernator]

    cat > /tmp/test-volume.yaml << EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: magruela-volume
  labels:
    aglais-dataset:  test
    aglais-datatype: text
spec:
  capacity:
    storage: $((sharesize - 1))Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  cephfs:
    path: ${sharepath}
    monitors:
$(
    for monitor in ${monitors}
    do
        echo "      - ${monitor}"
    done
)
EOF

    kubectl \
        apply \
            --filename /tmp/test-volume.yaml

    >   persistentvolume/magruela-volume created


    kubectl get \
        --output json \
        persistentvolume magruela-volume

    >   {
    >       "apiVersion": "v1",
    >       "kind": "PersistentVolume",
    >       "metadata": {
    >           "annotations": {
    >               "kubectl.kubernetes.io/last-applied-configuration": "{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolume\",\"metadata\":{\"annotations\":{},\"labels\":{\"aglais-dataset\":\"test\",\"aglais-datatype\":\"text\"},\"name\":\"magruela-volume\"},\"spec\":{\"accessModes\":[\"ReadWriteMany\"],\"capacity\":{\"storage\":\"4Gi\"},\"cephfs\":{\"monitors\":[\"10.206.1.5:6789\",\"10.206.1.6:6789\",\"10.206.1.7:6789\"],\"path\":\"volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121\"},\"persistentVolumeReclaimPolicy\":\"Retain\"}}\n"
    >           },
    >           "creationTimestamp": "2020-08-29T14:15:32Z",
    >           "finalizers": [
    >               "kubernetes.io/pv-protection"
    >           ],
    >           "labels": {
    >               "aglais-dataset": "test",
    >               "aglais-datatype": "text"
    >           },
    >           "name": "magruela-volume",
    >           "resourceVersion": "666509",
    >           "selfLink": "/api/v1/persistentvolumes/magruela-volume",
    >           "uid": "5ef26a70-e1dd-4986-bd1d-8eedeee099cc"
    >       },
    >       "spec": {
    >           "accessModes": [
    >               "ReadWriteMany"
    >           ],
    >           "capacity": {
    >               "storage": "4Gi"
    >           },
    >           "cephfs": {
    >               "monitors": [
    >                   "10.206.1.5:6789",
    >                   "10.206.1.6:6789",
    >                   "10.206.1.7:6789"
    >               ],
    >               "path": "volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121"
    >           },
    >           "persistentVolumeReclaimPolicy": "Retain",
    >           "volumeMode": "Filesystem"
    >       },
    >       "status": {
    >           "phase": "Available"
    >       }
    >   }


# -----------------------------------------------------
# Create a Kubernetes volume claim.
# https://docs.openshift.com/container-platform/3.11/install_config/persistent_storage/selector_label_binding.html#selector-label-volume-define
#[user@kubernator]

    cat > /tmp/test-claim.yaml << EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: magruela-claim
spec:
  accessModes:
  - ReadWriteMany
  resources:
     requests:
       storage: $((sharesize - 2))Gi
  selector:
    matchLabels:
      aglais-dataset:  test
      aglais-datatype: text
EOF

    kubectl \
        apply \
            --filename /tmp/test-claim.yaml

    >   persistentvolumeclaim/magruela-claim created


    kubectl get \
        --output json \
        persistentvolumeclaim magruela-claim

    >   {
    >       "apiVersion": "v1",
    >       "kind": "PersistentVolumeClaim",
    >       "metadata": {
    >           "annotations": {
    >               "kubectl.kubernetes.io/last-applied-configuration": "{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolumeClaim\",\"metadata\":{\"annotations\":{},\"name\":\"magruela-claim\",\"namespace\":\"default\"},\"spec\":{\"accessModes\":[\"ReadWriteMany\"],\"resources\":{\"requests\":{\"storage\":\"3Gi\"}},\"selector\":{\"matchLabels\":{\"aglais-dataset\":\"test\",\"aglais-datatype\":\"text\"}}}}\n",
    >               "pv.kubernetes.io/bind-completed": "yes",
    >               "pv.kubernetes.io/bound-by-controller": "yes"
    >           },
    >           "creationTimestamp": "2020-08-29T14:17:41Z",
    >           "finalizers": [
    >               "kubernetes.io/pvc-protection"
    >           ],
    >           "name": "magruela-claim",
    >           "namespace": "default",
    >           "resourceVersion": "667188",
    >           "selfLink": "/api/v1/namespaces/default/persistentvolumeclaims/magruela-claim",
    >           "uid": "8c842b10-5eef-45fe-903f-bfabb1fc4381"
    >       },
    >       "spec": {
    >           "accessModes": [
    >               "ReadWriteMany"
    >           ],
    >           "resources": {
    >               "requests": {
    >                   "storage": "3Gi"
    >               }
    >           },
    >           "selector": {
    >               "matchLabels": {
    >                   "aglais-dataset": "test",
    >                   "aglais-datatype": "text"
    >               }
    >           },
    >           "volumeMode": "Filesystem",
    >           "volumeName": "magruela-volume"
    >       },
    >       "status": {
    >           "accessModes": [
    >               "ReadWriteMany"
    >           ],
    >           "capacity": {
    >               "storage": "4Gi"
    >           },
    >           "phase": "Bound"
    >       }
    >   }


    kubectl get \
        --output json \
        persistentvolume magruela-volume

    >   {
    >       "apiVersion": "v1",
    >       "kind": "PersistentVolume",
    >       "metadata": {
    >           "annotations": {
    >               "kubectl.kubernetes.io/last-applied-configuration": "{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolume\",\"metadata\":{\"annotations\":{},\"labels\":{\"aglais-dataset\":\"test\",\"aglais-datatype\":\"text\"},\"name\":\"magruela-volume\"},\"spec\":{\"accessModes\":[\"ReadWriteMany\"],\"capacity\":{\"storage\":\"4Gi\"},\"cephfs\":{\"monitors\":[\"10.206.1.5:6789\",\"10.206.1.6:6789\",\"10.206.1.7:6789\"],\"path\":\"volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121\"},\"persistentVolumeReclaimPolicy\":\"Retain\"}}\n",
    >               "pv.kubernetes.io/bound-by-controller": "yes"
    >           },
    >           "creationTimestamp": "2020-08-29T14:15:32Z",
    >           "finalizers": [
    >               "kubernetes.io/pv-protection"
    >           ],
    >           "labels": {
    >               "aglais-dataset": "test",
    >               "aglais-datatype": "text"
    >           },
    >           "name": "magruela-volume",
    >           "resourceVersion": "667186",
    >           "selfLink": "/api/v1/persistentvolumes/magruela-volume",
    >           "uid": "5ef26a70-e1dd-4986-bd1d-8eedeee099cc"
    >       },
    >       "spec": {
    >           "accessModes": [
    >               "ReadWriteMany"
    >           ],
    >           "capacity": {
    >               "storage": "4Gi"
    >           },
    >           "cephfs": {
    >               "monitors": [
    >                   "10.206.1.5:6789",
    >                   "10.206.1.6:6789",
    >                   "10.206.1.7:6789"
    >               ],
    >               "path": "volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121"
    >           },
    >           "claimRef": {
    >               "apiVersion": "v1",
    >               "kind": "PersistentVolumeClaim",
    >               "name": "magruela-claim",
    >               "namespace": "default",
    >               "resourceVersion": "667183",
    >               "uid": "8c842b10-5eef-45fe-903f-bfabb1fc4381"
    >           },
    >           "persistentVolumeReclaimPolicy": "Retain",
    >           "volumeMode": "Filesystem"
    >       },
    >       "status": {
    >           "phase": "Bound"
    >       }
    >   }

    #
    # Yay - Kubernetes volume controller matched the claim to the volume and linked them together.
    # Match based on the labels and selectors we assigned to the volume and claim.
    #

# -----------------------------------------------------
# Create a Pod that mounts the volume ....
#[user@kubernator]

    cat > /tmp/test-pod.yaml << EOF
kind: Pod
apiVersion: v1
metadata:
  name: test-pod-02
  namespace: default
spec:
  volumes:
    - name: magruela-data
      persistentVolumeClaim:
        claimName: magruela-claim
    - name: local-data
      emptyDir: {}
  containers:
    - name: test-container-01
      image: 'fedora:latest'
      volumeMounts:
        - name: magruela-data
          mountPath: /magruela-data
        - name: local-data
          mountPath: /local-data
      command: ["/bin/sh"]
      args:
        - "-c"
        - >-
          while true; do
          date >> /local-data/date-log.txt;
          sleep 1;
          done
EOF

    kubectl \
        apply \
            --filename /tmp/test-pod.yaml

    >   pod/test-pod-02 created


# -----------------------------------------------------
# Check the Pod status.
#[user@kubernator]

    kubectl \
        describe pod \
            test-pod-02

    >   Name:         test-pod-02
    >   Namespace:    default
    >   Node:         tiberius-20200827-4tjt3wg4fxmz-node-0/10.0.0.112
    >   Start Time:   Sat, 29 Aug 2020 15:01:51 +0000
    >   Labels:       <none>
    >   Annotations:  kubectl.kubernetes.io/last-applied-configuration:
    >                   {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"test-pod-02","namespace":"default"},"spec":{"containers":[{"args":["-...
    >   Status:       Pending
    >   IP:
    >   Containers:
    >     test-container-01:
    >       Container ID:
    >       Image:         fedora:latest
    >       Image ID:
    >       Port:          <none>
    >       Host Port:     <none>
    >       Command:
    >         /bin/sh
    >       Args:
    >         -c
    >         while true; do date >> /local-data/date-log.txt; sleep 1; done
    >       State:          Waiting
    >         Reason:       ContainerCreating
    >       Ready:          False
    >       Restart Count:  0
    >       Environment:    <none>
    >       Mounts:
    >         /local-data from local-data (rw)
    >         /magruela-data from magruela-data (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from default-token-vjk7x (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             False
    >     ContainersReady   False
    >     PodScheduled      True
    >   Volumes:
    >     magruela-data:
    >       Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    >       ClaimName:  magruela-claim
    >       ReadOnly:   false
    >     local-data:
    >       Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    >       Medium:
    >       SizeLimit:  <unset>
    >     default-token-vjk7x:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  default-token-vjk7x
    >       Optional:    false
    >   QoS Class:       BestEffort
    >   Node-Selectors:  <none>
    >   Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
    >                    node.kubernetes.io/unreachable:NoExecute for 300s
    >   Events:
    >     Type     Reason       Age                  From                                            Message
    >     ----     ------       ----                 ----                                            -------
    >     Normal   Scheduled    <unknown>            default-scheduler                               Successfully assigned default/test-pod-02 to tiberius-20200827-4tjt3wg4fxmz-node-0
    >     Warning  FailedMount  68s                  kubelet, tiberius-20200827-4tjt3wg4fxmz-node-0  Unable to attach or mount volumes: unmounted volumes=[magruela-data], unattached volumes=[magruela-data local-data default-token-vjk7x]: timed out waiting for the condition
    >     Warning  FailedMount  63s (x9 over 3m11s)  kubelet, tiberius-20200827-4tjt3wg4fxmz-node-0  MountVolume.SetUp failed for volume "magruela-volume" : CephFS: mount failed: mount failed: exit status 32
    >   Mounting command: mount
    >   Mounting arguments: -t ceph -o name=admin,secretfile=/etc/ceph/admin.secret 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121 /var/lib/kubelet/pods/3366e6dd-abbf-4477-9db2-8fbdff5bc7d1/volumes/kubernetes.io~cephfs/magruela-volume
    >   Output: mount: wrong fs type, bad option, bad superblock on 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121,
    >          missing codepage or helper program, or other error


# -----------------------------------------------------
# Delete our Pod, PersistentVolumeClaim and PersistentVolume
#[user@kubernator]

    kubectl \
        delete pod \
            test-pod-02

    kubectl \
        delete persistentvolumeclaim \
            magruela-claim

    kubectl \
        delete persistentvolume \
            magruela-volume

# -----------------------------------------------------
# Guess - add storage class and mode to our PersistentVolume.
#[user@kubernator]

    cat > /tmp/magruela-volume.yaml << EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: magruela-volume
  labels:
    aglais-dataset:  test
    aglais-datatype: text
spec:
  capacity:
    storage: $((sharesize - 1))Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manila-cephfs-storage-class
  volumeMode: Filesystem
  cephfs:
    path: ${sharepath}
    monitors:
$(
    for monitor in ${monitors}
    do
        echo "      - ${monitor}"
    done
)
EOF

    kubectl \
        apply \
            --filename /tmp/magruela-volume.yaml

    >   persistentvolume/magruela-volume created


    kubectl get \
        --output json \
        persistentvolume magruela-volume

    >   {
    >       "apiVersion": "v1",
    >       "kind": "PersistentVolume",
    >       "metadata": {
    >           "annotations": {
    >               "kubectl.kubernetes.io/last-applied-configuration": "{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolume\",\"metadata\":{\"annotations\":{},\"labels\":{\"aglais-dataset\":\"test\",\"aglais-datatype\":\"text\"},\"name\":\"magruela-volume\"},\"spec\":{\"accessModes\":[\"ReadWriteMany\"],\"capacity\":{\"storage\":\"4Gi\"},\"cephfs\":{\"monitors\":[\"10.206.1.5:6789\",\"10.206.1.6:6789\",\"10.206.1.7:6789\"],\"path\":\"volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121\"},\"persistentVolumeReclaimPolicy\":\"Retain\",\"storageClassName\":\"manila-cephfs-storage-class\",\"volumeMode\":\"Filesystem\"}}\n"
    >           },
    >           "creationTimestamp": "2020-08-29T15:19:52Z",
    >           "finalizers": [
    >               "kubernetes.io/pv-protection"
    >           ],
    >           "labels": {
    >               "aglais-dataset": "test",
    >               "aglais-datatype": "text"
    >           },
    >           "name": "magruela-volume",
    >           "resourceVersion": "686444",
    >           "selfLink": "/api/v1/persistentvolumes/magruela-volume",
    >           "uid": "7ae240f7-9696-46d4-b24e-3e2fc38554dc"
    >       },
    >       "spec": {
    >           "accessModes": [
    >               "ReadWriteMany"
    >           ],
    >           "capacity": {
    >               "storage": "4Gi"
    >           },
    >           "cephfs": {
    >               "monitors": [
    >                   "10.206.1.5:6789",
    >                   "10.206.1.6:6789",
    >                   "10.206.1.7:6789"
    >               ],
    >               "path": "volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121"
    >           },
    >           "persistentVolumeReclaimPolicy": "Retain",
    >           "storageClassName": "manila-cephfs-storage-class",
    >           "volumeMode": "Filesystem"
    >       },
    >       "status": {
    >           "phase": "Available"
    >       }
    >   }


# -----------------------------------------------------
# Create our PersistentVolumeClaim.
# https://docs.openshift.com/container-platform/3.11/install_config/persistent_storage/selector_label_binding.html#selector-label-volume-define
#[user@kubernator]

    cat > /tmp/magruela-claim.yaml << EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: magruela-claim
spec:
  accessModes:
  - ReadWriteMany
  resources:
     requests:
       storage: $((sharesize - 2))Gi
  selector:
    matchLabels:
      aglais-dataset:  test
      aglais-datatype: text
EOF

    kubectl \
        apply \
            --filename /tmp/magruela-claim.yaml

    >   persistentvolumeclaim/magruela-claim created


    kubectl get \
        --output json \
        persistentvolumeclaim magruela-claim

    >   {
    >       "apiVersion": "v1",
    >       "kind": "PersistentVolumeClaim",
    >       "metadata": {
    >           "annotations": {
    >               "kubectl.kubernetes.io/last-applied-configuration": "{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolumeClaim\",\"metadata\":{\"annotations\":{},\"name\":\"magruela-claim\",\"namespace\":\"default\"},\"spec\":{\"accessModes\":[\"ReadWriteMany\"],\"resources\":{\"requests\":{\"storage\":\"3Gi\"}},\"selector\":{\"matchLabels\":{\"aglais-dataset\":\"test\",\"aglais-datatype\":\"text\"}}}}\n"
    >           },
    >           "creationTimestamp": "2020-08-29T15:21:49Z",
    >           "finalizers": [
    >               "kubernetes.io/pvc-protection"
    >           ],
    >           "name": "magruela-claim",
    >           "namespace": "default",
    >           "resourceVersion": "687054",
    >           "selfLink": "/api/v1/namespaces/default/persistentvolumeclaims/magruela-claim",
    >           "uid": "61c5a419-9f63-4278-ba42-e19515eb0e3d"
    >       },
    >       "spec": {
    >           "accessModes": [
    >               "ReadWriteMany"
    >           ],
    >           "resources": {
    >               "requests": {
    >                   "storage": "3Gi"
    >               }
    >           },
    >           "selector": {
    >               "matchLabels": {
    >                   "aglais-dataset": "test",
    >                   "aglais-datatype": "text"
    >               }
    >           },
    >           "volumeMode": "Filesystem"
    >       },
    >       "status": {
    >           "phase": "Pending"
    >       }
    >   }


# -----------------------------------------------------
# Create a Pod that mounts the volume ....
#[user@kubernator]

    cat > /tmp/magruela-pod.yaml << EOF
kind: Pod
apiVersion: v1
metadata:
  name: magruela-pod
  namespace: default
spec:
  volumes:
    - name: magruela-data
      persistentVolumeClaim:
        claimName: magruela-claim
    - name: local-data
      emptyDir: {}
  containers:
    - name: magruela-container
      image: 'fedora:latest'
      volumeMounts:
        - name: magruela-data
          mountPath: /magruela-data
        - name: local-data
          mountPath: /local-data
      command: ["/bin/sh"]
      args:
        - "-c"
        - >-
          while true; do
          date >> /local-data/date-log.txt;
          sleep 1;
          done
EOF

    kubectl \
        apply \
            --filename /tmp/magruela-pod.yaml

    >   pod/magruela-pod created


# -----------------------------------------------------
# Check the Pod status.
#[user@kubernator]

    kubectl \
        describe pod \
            magruela-pod

    >   Name:         magruela-pod
    >   Namespace:    default
    >   Node:         <none>
    >   Labels:       <none>
    >   Annotations:  kubectl.kubernetes.io/last-applied-configuration:
    >                   {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"magruela-pod","namespace":"default"},"spec":{"containers":[{"args":["...
    >   Status:       Pending
    >   IP:
    >   Containers:
    >     magruela-container:
    >       Image:      fedora:latest
    >       Port:       <none>
    >       Host Port:  <none>
    >       Command:
    >         /bin/sh
    >       Args:
    >         -c
    >         while true; do date >> /local-data/date-log.txt; sleep 1; done
    >       Environment:  <none>
    >       Mounts:
    >         /local-data from local-data (rw)
    >         /magruela-data from magruela-data (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from default-token-vjk7x (ro)
    >   Conditions:
    >     Type           Status
    >     PodScheduled   False
    >   Volumes:
    >     magruela-data:
    >       Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    >       ClaimName:  magruela-claim
    >       ReadOnly:   false
    >     local-data:
    >       Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    >       Medium:
    >       SizeLimit:  <unset>
    >     default-token-vjk7x:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  default-token-vjk7x
    >       Optional:    false
    >   QoS Class:       BestEffort
    >   Node-Selectors:  <none>
    >   Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
    >                    node.kubernetes.io/unreachable:NoExecute for 300s
    >   Events:
    >     Type     Reason             Age               From                Message
    >     ----     ------             ----              ----                -------
    >     Warning  FailedScheduling   <unknown>         default-scheduler   error while running "VolumeBinding" filter plugin for pod "magruela-pod": pod has unbound immediate PersistentVolumeClaims
    >     Warning  FailedScheduling   <unknown>         default-scheduler   error while running "VolumeBinding" filter plugin for pod "magruela-pod": pod has unbound immediate PersistentVolumeClaims
    >     Normal   NotTriggerScaleUp  1s (x3 over 21s)  cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added): 1 max limit reached


    kubectl \
        describe persistentvolumeclaim \
            magruela-claim

    >   Name:          magruela-claim
    >   Namespace:     default
    >   StorageClass:
    >   Status:        Pending
    >   Volume:
    >   Labels:        <none>
    >   Annotations:   kubectl.kubernetes.io/last-applied-configuration:
    >                    {"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"name":"magruela-claim","namespace":"default"},"spec":{"acc...
    >   Finalizers:    [kubernetes.io/pvc-protection]
    >   Capacity:
    >   Access Modes:
    >   VolumeMode:    Filesystem
    >   Mounted By:    magruela-pod
    >   Events:
    >     Type    Reason         Age                  From                         Message
    >     ----    ------         ----                 ----                         -------
    >     Normal  FailedBinding  1s (x24 over 5m21s)  persistentvolume-controller  no persistent volumes available for this claim and no storage class is set


    kubectl \
        describe persistentvolume \
            magruela-volume

    >   Name:            magruela-volume
    >   Labels:          aglais-dataset=test
    >                    aglais-datatype=text
    >   Annotations:     kubectl.kubernetes.io/last-applied-configuration:
    >                      {"apiVersion":"v1","kind":"PersistentVolume","metadata":{"annotations":{},"labels":{"aglais-dataset":"test","aglais-datatype":"text"},"nam...
    >   Finalizers:      [kubernetes.io/pv-protection]
    >   StorageClass:    manila-cephfs-storage-class
    >   Status:          Available
    >   Claim:
    >   Reclaim Policy:  Retain
    >   Access Modes:    RWX
    >   VolumeMode:      Filesystem
    >   Capacity:        4Gi
    >   Node Affinity:   <none>
    >   Message:
    >   Source:
    >       Type:        CephFS (a CephFS mount on the host that shares a pod's lifetime)
    >       Monitors:    [10.206.1.5:6789 10.206.1.6:6789 10.206.1.7:6789]
    >       Path:        volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121
    >       User:
    >       SecretFile:
    >       SecretRef:   nil
    >       ReadOnly:    false
    >   Events:          <none>


    #
    # This time the system didn't link the claim and volume together.
    # The volume is available, but it wasn't matched with the claim.
    #

    #
    # No user associated with the volume.
    # No secret associated with the volume.
    # Do we need these for the CephFS mount ?
    #

    #
    # This suggests we need to provide a secret based on our identity.
    # https://github.com/gman0/cloud-provider-openstack/blob/ef56215b90cac0cf92d1f750f2ab2e88e2ec01d5/docs/using-manila-provisioner.md#authentication-with-manila-v2-client
    #

# -----------------------------------------------------
# Create our secret.
# https://github.com/gman0/cloud-provider-openstack/blob/ef56215b90cac0cf92d1f750f2ab2e88e2ec01d5/docs/using-manila-provisioner.md#authentication-with-manila-v2-client
# https://github.com/gman0/cloud-provider-openstack/blob/ef56215b90cac0cf92d1f750f2ab2e88e2ec01d5/examples/manila-provisioner/generate-secrets.sh#L64
#[user@kubernator]

    #
    # The example script assumes we have the standard OS_whatever environment variables set.
    # It also assumes we have a user name and password.
    # We don't have either.
    #

    cat > /tmp/magruela-secret.yaml << EOF
apiVersion: v1
kind: Secret
metadata:
  name: magruela-secret
  namespace: default
data:
  os-authURL: "$(encode OS_AUTH_URL)"
  os-userID: "$(encode OS_USER_ID)"
  os-userName: "$(encode OS_USERNAME)"
  os-password: "$(encode OS_PASSWORD)"
  os-projectID: "$(encode OS_PROJECT_ID)"
  os-projectName: "$(encode OS_PROJECT_NAME)"
  os-domainID: "$(encode OS_PROJECT_DOMAIN_ID)"
  os-domainName: "$(encode OS_PROJECT_DOMAIN_NAME)"
  os-region: "$(encode OS_REGION_NAME)"
EOF

    #
    # We can get our credentials from our cloud config.
    #

    cat /etc/openstack/clouds.yaml

    >     ....
    >     gaia-prod:
    >       auth:
    >         auth_url: https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
    >         application_credential_id:     '41....da'
    >         application_credential_secret: 'yo....Ee'
    >       region_name: "RegionOne"
    >       interface: "public"
    >       identity_api_version: 3
    >       auth_type: "v3applicationcredential"
    >     ....

    #
    # Can we map our application credentials onto username and password ?
    #

    #
    # Perhaps .. but I don't think this is the issue.
    # Missing or bad credentials would fail when we tried to mount the share.
    # We haven't got that far.
    # The problem is the system hasn't matched the volume with the volume claim yet.
    #

    #
    # Just noticed - all of the contents of the secret are static.
    # Relating to our accont and nothing relating to the share.
    # So can we just re-use the existing secret ?
    #

    kubectl \
        describe secret \
            manila-e80564ee-e99f-11ea-8c83-0a580a640409

    >   Name:         manila-e80564ee-e99f-11ea-8c83-0a580a640409
    >   Namespace:    default
    >   Labels:       <none>
    >   Annotations:  <none>
    >
    >   Type:  Opaque
    >
    >   Data
    >   ====
    >   key:  40 bytes

    #
    # Second part ..
    # The dynamic mounted volume had lots of annotations.
    #

    >   ....
    >       "metadata": {
    >           "annotations": {
    >               "manila.cloud-provider-openstack.kubernetes.io/ID": "d71dae9a-b508-4a54-a5e9-7a91e8548b1e",
    >               "manila.cloud-provider-openstack.kubernetes.io/OSSecretName": "os-trustee",
    >               "manila.cloud-provider-openstack.kubernetes.io/OSSecretNamespace": "kube-system",
    >               "manila.cloud-provider-openstack.kubernetes.io/ProvisionType": "dynamic",
    >               "manila.cloud-provider-openstack.kubernetes.io/ShareSecretName": "manila-e80564ee-e99f-11ea-8c83-0a580a640409",
    >               "manila.cloud-provider-openstack.kubernetes.io/ShareSecretNamespace": "default",
    >               "pv.kubernetes.io/provisioned-by": "manila-provisioner"
    >           },
    >   ....

    #
    # The ID is the ID of the Openstack share.
    # Would it help if we replicated these ?
    #



    #
    # From the Manila provisioner documentation.
    # (*) now replaced by CSI
    # https://github.com/gman0/cloud-provider-openstack/blob/ef56215b90cac0cf92d1f750f2ab2e88e2ec01d5/docs/using-manila-provisioner.md

        "In order to use an existing Manila share, parameters [osShareID or osShareName] and osShareAccessID must be supplied."


# -----------------------------------------------------
# Delete the pod, claim and volume.
#[user@kubernator]

    kubectl delete \
        pod \
            magruela-pod

    kubectl delete \
        persistentvolumeclaim \
            magruela-claim

    kubectl delete \
        persistentvolume \
            magruela-volume

# -----------------------------------------------------
# Create the volume.
# Leaving out the StorageClass reference.
#[user@kubernator]

    cat > /tmp/magruela-volume.yaml << EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: magruela-volume
  labels:
    aglais-dataset:  test
    aglais-datatype: text
spec:
  capacity:
    storage: $((sharesize - 1))Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  volumeMode: Filesystem
  cephfs:
    path: ${sharepath}
    monitors:
$(
    for monitor in ${monitors}
    do
        echo "      - ${monitor}"
    done
)
EOF

    kubectl \
        apply \
            --filename /tmp/magruela-volume.yaml

    >   persistentvolume/magruela-volume created


# -----------------------------------------------------
# Create the volume claim.
#[user@kubernator]

    kubectl \
        apply \
            --filename /tmp/magruela-claim.yaml

    >   persistentvolumeclaim/magruela-claim created


# -----------------------------------------------------
# Get details for the volume and volume claim.
#[user@kubernator]

    kubectl get \
        --output json \
        persistentvolume magruela-volume

    >   {
    >       ....
    >       "spec": {
    >           ....
    >           "claimRef": {
    >               "apiVersion": "v1",
    >               "kind": "PersistentVolumeClaim",
    >               "name": "magruela-claim",
    >               "namespace": "default",
    >               "resourceVersion": "858425",
    >               "uid": "db20a911-c90d-4815-92bd-9521f701d5f1"
    >           },
    >           ....
    >       },
    >       ....
    >   }


    kubectl get \
        --output json \
        persistentvolumeclaim magruela-claim

    >   {
    >       ....
    >       "spec": {
    >           ....
    >           "selector": {
    >               "matchLabels": {
    >                   "aglais-dataset": "test",
    >                   "aglais-datatype": "text"
    >               }
    >           },
    >           "volumeMode": "Filesystem",
    >           "volumeName": "magruela-volume"
    >       },
    >       "status": {
    >           "accessModes": [
    >               "ReadWriteMany"
    >           ],
    >           "capacity": {
    >               "storage": "4Gi"
    >           },
    >           "phase": "Bound"
    >       }
    >   }

    #
    # Removed storageClassName from the volume spec.
    # Volume and claim are matched.
    #



# -----------------------------------------------------
# Create a Pod that mounts the volume ....
#[user@kubernator]

    kubectl \
        apply \
            --filename /tmp/magruela-pod.yaml

    >   pod/magruela-pod created


# -----------------------------------------------------
# Check the Pod status.
#[user@kubernator]

    kubectl \
        describe pod \
            magruela-pod

    >   ....
    >   ....
    >     Type     Reason       Age               From                                            Message
    >     ----     ------       ----              ----                                            -------
    >     Normal   Scheduled    <unknown>         default-scheduler                               Successfully assigned default/magruela-pod to tiberius-20200827-4tjt3wg4fxmz-node-0
    >     Warning  FailedMount  3s (x5 over 11s)  kubelet, tiberius-20200827-4tjt3wg4fxmz-node-0  MountVolume.SetUp failed for volume "magruela-volume" : CephFS: mount failed: mount failed: exit status 32
    >   Mounting command: mount
    >   Mounting arguments: -t ceph -o name=admin,secretfile=/etc/ceph/admin.secret 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121 /var/lib/kubelet/pods/2882e5f2-8c38-448e-9ae7-8cf2021b19dd/volumes/kubernetes.io~cephfs/magruela-volume
    >   Output: mount: wrong fs type, bad option, bad superblock on 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121,
    >          missing codepage or helper program, or other error


# -----------------------------------------------------
# Delete the pod, claim and volume.
#[user@kubernator]

    kubectl delete \
        pod \
            magruela-pod

    kubectl delete \
        persistentvolumeclaim \
            magruela-claim

    kubectl delete \
        persistentvolume \
            magruela-volume

# -----------------------------------------------------
# Create the volume.
# Adding a reference to our manila secret.
#[user@kubernator]

    cat > /tmp/magruela-volume.yaml << EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: magruela-volume
  labels:
    aglais-dataset:  test
    aglais-datatype: text
spec:
  capacity:
    storage: $((sharesize - 1))Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  volumeMode: Filesystem
  cephfs:
    path: ${sharepath}
    monitors:
$(
    for monitor in ${monitors}
    do
        echo "      - ${monitor}"
    done
)
    secretRef:
      name: manila-e80564ee-e99f-11ea-8c83-0a580a640409
      namespace: default
EOF

    kubectl \
        apply \
            --filename /tmp/magruela-volume.yaml

    >   persistentvolume/magruela-volume created


# -----------------------------------------------------
# Create the volume claim.
#[user@kubernator]

    kubectl \
        apply \
            --filename /tmp/magruela-claim.yaml

    >   persistentvolumeclaim/magruela-claim created


# -----------------------------------------------------
# Create a Pod that mounts the volume ....
#[user@kubernator]

    kubectl \
        apply \
            --filename /tmp/magruela-pod.yaml

    >   pod/magruela-pod created


# -----------------------------------------------------
# Check the Pod status.
#[user@kubernator]

    kubectl \
        describe pod \
            magruela-pod

    >   ....
    >   ....
    >   Events:
    >     Type     Reason       Age               From                                            Message
    >     ----     ------       ----              ----                                            -------
    >     Normal   Scheduled    <unknown>         default-scheduler                               Successfully assigned default/magruela-pod to tiberius-20200827-4tjt3wg4fxmz-node-0
    >     Warning  FailedMount  2s (x6 over 18s)  kubelet, tiberius-20200827-4tjt3wg4fxmz-node-0  MountVolume.SetUp failed for volume "magruela-volume" : CephFS: mount failed: mount failed: exit status 32
    >   Mounting command: mount
    >   Mounting arguments: -t ceph -o name=admin,secret=AQBM........== 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121 /var/lib/kubelet/pods/045fdba1-1632-4a25-ad6a-b3fc857a39ff/volumes/kubernetes.io~cephfs/magruela-volume
    >   Output: mount: permission denied

    #
    # Mount arguments owner name is 'admin'

    #
    # From
    # https://github.com/gman0/cloud-provider-openstack/blob/8eda8924db426f3eef3c8756899fa4f7a37a906e/docs/using-manila-csi-plugin.md#controller-service-volume-parameters

    One of :

    >   shareID 	    The UUID of the share
    >   shareName 	    The name of the share

    Required :

    >   shareAccessID 	The UUID of the access rule for the share
    >   type 	        Manila share type


    Share types :
    https://wiki.openstack.org/wiki/Manila/Concepts#share_type

    From our Openstack share :

    >   "share_type": "5d0f58c5-ed21-4e1f-91bb-fe1a49deb5d8",
    >   "share_type_name": "cephfsnativetype",

    Share type is part of the metadata that is in the manila-cephfs StorageClass.
    One option would be to include StorageClass in _both_ the volume and the volume claim.

    #
    # From the Manila provisioner documentation.
    # (*) now replaced by CSI
    # https://github.com/gman0/cloud-provider-openstack/blob/ef56215b90cac0cf92d1f750f2ab2e88e2ec01d5/docs/using-manila-provisioner.md

        "In order to use an existing Manila share, parameters [osShareID or osShareName] and osShareAccessID must be supplied."

    >   ....
    >       "metadata": {
    >           "annotations": {
    >               "manila.cloud-provider-openstack.kubernetes.io/ID": "d71dae9a-b508-4a54-a5e9-7a91e8548b1e",
    >               "manila.cloud-provider-openstack.kubernetes.io/OSSecretName": "os-trustee",
    >               "manila.cloud-provider-openstack.kubernetes.io/OSSecretNamespace": "kube-system",
    >               "manila.cloud-provider-openstack.kubernetes.io/ProvisionType": "dynamic",
    >               "manila.cloud-provider-openstack.kubernetes.io/ShareSecretName": "manila-e80564ee-e99f-11ea-8c83-0a580a640409",
    >               "manila.cloud-provider-openstack.kubernetes.io/ShareSecretNamespace": "default",
    >               "pv.kubernetes.io/provisioned-by": "manila-provisioner"
    >           },
    >   ....

# -----------------------------------------------------
# Check the share access created by the dynamic volume.
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
        share list

    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+
    >   | ID                                   | Name                                     | Size | Share Proto | Status    | Is Public | Share Type Name  | Host | Availability Zone |
    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+
    >   | ad1d9ca2-5b1c-4064-8c74-695286de6098 | gaia-dr2-share                           | 4399 | CEPHFS      | available | True      | cephfsnativetype |      | nova              |
    >   | a32ea61f-a922-4b9d-959b-a9d2d2e57c4b | magruela-share                           |    5 | CEPHFS      | available | False     | cephfsnativetype |      | nova              |
    >   | d71dae9a-b508-4a54-a5e9-7a91e8548b1e | pvc-41814ac5-00cc-4ba3-ae8b-6eddad01ef0c |    1 | CEPHFS      | available | False     | cephfsnativetype |      | nova              |
    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+

    openstack \
        --os-cloud "${cloudname:?}" \
        share access list \
            pvc-41814ac5-00cc-4ba3-ae8b-6eddad01ef0c

    >   +--------------------------------------+-------------+------------------------------------------+--------------+--------+----------------+----------------------------+----------------------------+
    >   | id                                   | access_type | access_to                                | access_level | state  | access_key     | created_at                 | updated_at                 |
    >   +--------------------------------------+-------------+------------------------------------------+--------------+--------+----------------+----------------------------+----------------------------+
    >   | 474be0df-773e-4025-a19c-f0953b17124a | cephx       | pvc-41814ac5-00cc-4ba3-ae8b-6eddad01ef0c | rw           | active | AQBM........== | 2020-08-29T02:32:43.000000 | 2020-08-29T02:32:44.000000 |
    >   +--------------------------------------+-------------+------------------------------------------+--------------+--------+----------------+----------------------------+----------------------------+

    The osShareAccessID is '474be0df-773e-4025-a19c-f0953b17124a', but can't find that in any of the Kubernetes objects.


# -----------------------------------------------------
# Delete the pod, claim and volume.
#[user@kubernator]

    kubectl delete \
        pod \
            magruela-pod

    kubectl delete \
        persistentvolumeclaim \
            magruela-claim

    kubectl delete \
        persistentvolume \
            magruela-volume

# -----------------------------------------------------
# Create the volume.
# Using the volume name as the user reference.
#[user@kubernator]

    cat > /tmp/magruela-volume.yaml << EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: magruela-volume
  labels:
    aglais-dataset:  test
    aglais-datatype: text
spec:
  capacity:
    storage: $((sharesize - 1))Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  volumeMode: Filesystem
  cephfs:
    path: ${sharepath}
    monitors:
$(
    for monitor in ${monitors}
    do
        echo "      - ${monitor}"
    done
)
    secretRef:
      name: manila-e80564ee-e99f-11ea-8c83-0a580a640409
      namespace: default
    user: magruela-volume
EOF

    kubectl \
        apply \
            --filename /tmp/magruela-volume.yaml

    >   persistentvolume/magruela-volume created


# -----------------------------------------------------
# Create the volume claim.
#[user@kubernator]

    kubectl \
        apply \
            --filename /tmp/magruela-claim.yaml

    >   persistentvolumeclaim/magruela-claim created


# -----------------------------------------------------
# Create a Pod that mounts the volume ....
#[user@kubernator]

    kubectl \
        apply \
            --filename /tmp/magruela-pod.yaml

    >   pod/magruela-pod created


# -----------------------------------------------------
# Check the Pod status.
#[user@kubernator]

    kubectl \
        describe pod \
            magruela-pod

    >   ....
    >   ....
    >   Events:
    >     Type     Reason       Age              From                                            Message
    >     ----     ------       ----             ----                                            -------
    >     Normal   Scheduled    <unknown>        default-scheduler                               Successfully assigned default/magruela-pod to tiberius-20200827-4tjt3wg4fxmz-node-3
    >     Warning  FailedMount  2s (x4 over 6s)  kubelet, tiberius-20200827-4tjt3wg4fxmz-node-3  MountVolume.SetUp failed for volume "magruela-volume" : CephFS: mount failed: mount failed: exit status 32
    >   Mounting command: mount
    >   Mounting arguments: -t ceph -o name=magruela-volume,secret=AQBM........== 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121 /var/lib/kubelet/pods/26e5974b-99d3-490c-b785-3a7b54a3f1a9/volumes/kubernetes.io~cephfs/magruela-volume
    >   Output: mount: permission denied


# -----------------------------------------------------
# Login to see if we can more info.
#[user@kubernator]

    kubectl exec \
        --tty \
        --stdin \
        magruela-pod \
        -- \
            /bin/bash

    >   error: unable to upgrade connection: container not found ("magruela-container")

    # So container fails completely.


# -----------------------------------------------------
# Try using the 'root' as the user reference.
#[user@kubernator]

    kubectl \
        describe pod \
            magruela-pod

    >   ....
    >   ....
    >   Events:
    >     Type     Reason       Age              From                                            Message
    >     ----     ------       ----             ----                                            -------
    >     Normal   Scheduled    <unknown>        default-scheduler                               Successfully assigned default/magruela-pod to tiberius-20200827-4tjt3wg4fxmz-node-3
    >     Warning  FailedMount  3s (x4 over 7s)  kubelet, tiberius-20200827-4tjt3wg4fxmz-node-3  MountVolume.SetUp failed for volume "magruela-volume" : CephFS: mount failed: mount failed: exit status 32
    >   Mounting command: mount
    >   Mounting arguments: -t ceph -o name=root,secret=AQBM........== 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121 /var/lib/kubelet/pods/7c05fb16-c001-4103-9d9e-b4d82b9e4843/volumes/kubernetes.io~cephfs/magruela-volume
    >   Output: mount: permission denied


# -----------------------------------------------------
# Try adding storage class to *both* volume and claim.
#[user@kubernator]

    vi /tmp/magruela-volume.yaml

        persistentVolumeReclaimPolicy: Retain
    +   storageClassName: manila-cephfs-storage-class
        volumeMode: Filesystem


    vi /tmp/magruela-claim.yaml

        resources:
         requests:
           storage: 3Gi
    +   storageClassName: manila-cephfs-storage-class
        selector:

    kubectl \
        apply \
            --filename /tmp/magruela-volume.yaml

    kubectl \
        apply \
            --filename /tmp/magruela-claim.yaml

    kubectl \
        apply \
            --filename /tmp/magruela-pod.yaml

    kubectl \
        describe pod \
            magruela-pod

    >   ....
    >   ....
    >   Events:
    >     Type     Reason       Age        From                                            Message
    >     ----     ------       ----       ----                                            -------
    >     Normal   Scheduled    <unknown>  default-scheduler                               Successfully assigned default/magruela-pod to tiberius-20200827-4tjt3wg4fxmz-node-1
    >     Warning  FailedMount  0s         kubelet, tiberius-20200827-4tjt3wg4fxmz-node-1  MountVolume.SetUp failed for volume "magruela-volume" : CephFS: mount failed: mount failed: exit status 32
    >   Mounting command: mount
    >   Mounting arguments: -t ceph -o name=admin,secret=AQBM........== 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121 /var/lib/kubelet/pods/b34a3404-db2f-4130-aba1-9637646f27c3/volumes/kubernetes.io~cephfs/magruela-volume
    >   Output: mount: permission denied


# -----------------------------------------------------
# Try making the share public.
#[user@kubernator]

    # Delete the pod, claim and volume.
    # Create the volume, cliam and pod.

    kubectl \
        describe pod \
            magruela-pod

    >   Events:
    >     Type     Reason       Age        From                                            Message
    >     ----     ------       ----       ----                                            -------
    >     Normal   Scheduled    <unknown>  default-scheduler                               Successfully assigned default/magruela-pod to tiberius-20200827-4tjt3wg4fxmz-node-1
    >     Warning  FailedMount  0s         kubelet, tiberius-20200827-4tjt3wg4fxmz-node-1  MountVolume.SetUp failed for volume "magruela-volume" : CephFS: mount failed: mount failed: exit status 32
    >   Mounting command: mount
    >   Mounting arguments: -t ceph -o name=admin,secret=AQBM........== 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121 /var/lib/kubelet/pods/05d386f1-f2e0-42ec-940b-f5f685d25feb/volumes/kubernetes.io~cephfs/magruela-volume
    >   Output: mount: permission denied


# -----------------------------------------------------
# Try adding annotations to the volume.
#[user@kubernator]

    vi /tmp/magruela-volume.yaml

        metadata:
          name: magruela-volume
          labels:
            aglais-dataset:  test
            aglais-datatype: text
    +     annotations:
    +       manila.cloud-provider-openstack.kubernetes.io/ID: a32ea61f-a922-4b9d-959b-a9d2d2e57c4b
    +       manila.cloud-provider-openstack.kubernetes.io/OSSecretName: os-trustee
    +       manila.cloud-provider-openstack.kubernetes.io/OSSecretNamespace: kube-system
    +       manila.cloud-provider-openstack.kubernetes.io/ProvisionType: dynamic
    +       manila.cloud-provider-openstack.kubernetes.io/ShareSecretName: manila-e80564ee-e99f-11ea-8c83-0a580a640409
    +       manila.cloud-provider-openstack.kubernetes.io/ShareSecretNamespace: default
        spec:

    >   ....
    >   ....
    >   Events:
    >     Type     Reason       Age                    From                                            Message
    >     ----     ------       ----                   ----                                            -------
    >     Normal   Scheduled    <unknown>              default-scheduler                               Successfully assigned default/magruela-pod to tiberius-20200827-4tjt3wg4fxmz-node-1
    >     Warning  FailedMount  5m31s                  kubelet, tiberius-20200827-4tjt3wg4fxmz-node-1  Unable to attach or mount volumes: unmounted volumes=[magruela-data], unattached volumes=[local-data default-token-vjk7x magruela-data]: timed out waiting for the condition
    >     Warning  FailedMount  3m16s (x2 over 7m47s)  kubelet, tiberius-20200827-4tjt3wg4fxmz-node-1  Unable to attach or mount volumes: unmounted volumes=[magruela-data], unattached volumes=[default-token-vjk7x magruela-data local-data]: timed out waiting for the condition
    >     Warning  FailedMount  95s (x12 over 9m50s)   kubelet, tiberius-20200827-4tjt3wg4fxmz-node-1  MountVolume.SetUp failed for volume "magruela-volume" : CephFS: mount failed: mount failed: exit status 32
    >   Mounting command: mount
    >   Mounting arguments: -t ceph -o name=admin,secret=AQBM........== 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121 /var/lib/kubelet/pods/05d386f1-f2e0-42ec-940b-f5f685d25feb/volumes/kubernetes.io~cephfs/magruela-volume
    >   Output: mount: permission denied
    >     Warning  FailedMount  61s  kubelet, tiberius-20200827-4tjt3wg4fxmz-node-1  Unable to attach or mount volumes: unmounted volumes=[magruela-data], unattached volumes=[magruela-data local-data default-token-vjk7x]: timed out waiting for the condition


# -----------------------------------------------------
# List our current shares and share rules.
#[user@kubernator]

    #
    # Discovered that the share didn't have any access rules !!
    # Created a RO rule via the GUI.
    #

    openstack \
        --os-cloud "${cloudname:?}" \
            share list

    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+
    >   | ID                                   | Name                                     | Size | Share Proto | Status    | Is Public | Share Type Name  | Host | Availability Zone |
    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+
    >   | ad1d9ca2-5b1c-4064-8c74-695286de6098 | gaia-dr2-share                           | 4399 | CEPHFS      | available | True      | cephfsnativetype |      | nova              |
    >   | a32ea61f-a922-4b9d-959b-a9d2d2e57c4b | magruela-share                           |    5 | CEPHFS      | available | True      | cephfsnativetype |      | nova              |
    >   | d71dae9a-b508-4a54-a5e9-7a91e8548b1e | pvc-41814ac5-00cc-4ba3-ae8b-6eddad01ef0c |    1 | CEPHFS      | available | False     | cephfsnativetype |      | nova              |
    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+

    shareid=a32ea61f-a922-4b9d-959b-a9d2d2e57c4b

    openstack \
        --os-cloud "${cloudname:?}" \
            share access list \
                "${shareid:?}"

    >   +--------------------------------------+-------------+----------------+--------------+--------+----------------+----------------------------+----------------------------+
    >   | id                                   | access_type | access_to      | access_level | state  | access_key     | created_at                 | updated_at                 |
    >   +--------------------------------------+-------------+----------------+--------------+--------+----------------+----------------------------+----------------------------+
    >   | fd598fb6-2963-4e93-990e-f463b796d704 | cephx       | magruela-share | ro           | active | AQDh........== | 2020-08-30T03:24:17.000000 | 2020-08-30T03:24:17.000000 |
    >   +--------------------------------------+-------------+----------------+--------------+--------+----------------+----------------------------+----------------------------+


# -----------------------------------------------------
# Delete the pod, claim and volume.
#[user@kubernator]

    kubectl delete \
        pod \
            magruela-pod

    kubectl delete \
        persistentvolumeclaim \
            magruela-claim

    kubectl delete \
        persistentvolume \
            magruela-volume

# -----------------------------------------------------
# Revert the changes.
#[user@kubernator]

    vi /tmp/magruela-volume.yaml

        accessModes:
          - ReadOnlyMany


    vi /tmp/magruela-claim.yaml

        accessModes:
          - ReadOnlyMany


# -----------------------------------------------------
# Create the volume, claim and pod.
#[user@kubernator]

    kubectl \
        apply \
            --filename /tmp/magruela-volume.yaml

    kubectl \
        apply \
            --filename /tmp/magruela-claim.yaml

    kubectl \
        apply \
            --filename /tmp/magruela-pod.yaml

    kubectl \
        describe pod \
            magruela-pod


    >   ....
    >   ....
    >   Events:
    >     Type     Reason       Age              From                                            Message
    >     ----     ------       ----             ----                                            -------
    >     Normal   Scheduled    <unknown>        default-scheduler                               Successfully assigned default/magruela-pod to tiberius-20200827-4tjt3wg4fxmz-node-1
    >     Warning  FailedMount  2s (x3 over 3s)  kubelet, tiberius-20200827-4tjt3wg4fxmz-node-1  MountVolume.SetUp failed for volume "magruela-volume" : CephFS: mount failed: mount failed: exit status 32
    >   Mounting command: mount
    >   Mounting arguments: -t ceph -o name=admin,secret=AQBM........== 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121 /var/lib/kubelet/pods/d2c5b7e0-63f8-4a59-a5a7-5354dac68f6c/volumes/kubernetes.io~cephfs/magruela-volume
    >   Output: mount: permission denied

# -----------------------------------------------------
# Check the share config.
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
            share show \
                "${shareid:?}"

    >   +---------------------------------------+---------------------------------------------------------------------------------------------------------------+
    >   | Field                                 | Value                                                                                                         |
    >   +---------------------------------------+---------------------------------------------------------------------------------------------------------------+
    >   | access_rules_status                   | active                                                                                                        |
    >   | availability_zone                     | nova                                                                                                          |
    >   | create_share_from_snapshot_support    | False                                                                                                         |
    >   | created_at                            | 2020-08-29T13:17:01.000000                                                                                    |
    >   | description                           |                                                                                                               |
    >   | export_locations                      |                                                                                                               |
    >   |                                       | path = 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121 |
    >   |                                       | id = c005fd86-70ba-4ea6-acb4-db8d0dcb9b20                                                                     |
    >   |                                       | preferred = False                                                                                             |
    >   | has_replicas                          | False                                                                                                         |
    >   | id                                    | a32ea61f-a922-4b9d-959b-a9d2d2e57c4b                                                                          |
    >   | is_public                             | True                                                                                                          |
    >   | mount_snapshot_support                | False                                                                                                         |
    >   | name                                  | magruela-share                                                                                                |
    >   | project_id                            | 21b4ae3a2ea44bc5a9c14005ed2963af                                                                              |
    >   | properties                            |                                                                                                               |
    >   | replication_type                      | None                                                                                                          |
    >   | revert_to_snapshot_support            | False                                                                                                         |
    >   | share_group_id                        | None                                                                                                          |
    >   | share_network_id                      | None                                                                                                          |
    >   | share_proto                           | CEPHFS                                                                                                        |
    >   | share_type                            | 5d0f58c5-ed21-4e1f-91bb-fe1a49deb5d8                                                                          |
    >   | share_type_name                       | cephfsnativetype                                                                                              |
    >   | size                                  | 5                                                                                                             |
    >   | snapshot_id                           | None                                                                                                          |
    >   | snapshot_support                      | False                                                                                                         |
    >   | source_share_group_snapshot_member_id | None                                                                                                          |
    >   | status                                | available                                                                                                     |
    >   | task_state                            | None                                                                                                          |
    >   | user_id                               | 98169f87de174ad4ac98c32e59646488                                                                              |
    >   | volume_type                           | cephfsnativetype                                                                                              |
    >   +---------------------------------------+---------------------------------------------------------------------------------------------------------------+


    openstack \
        --os-cloud "${cloudname:?}" \
            share access list \
                "${shareid:?}"

    >   +--------------------------------------+-------------+----------------+--------------+--------+----------------+----------------------------+----------------------------+
    >   | id                                   | access_type | access_to      | access_level | state  | access_key     | created_at                 | updated_at                 |
    >   +--------------------------------------+-------------+----------------+--------------+--------+----------------+----------------------------+----------------------------+
    >   | fd598fb6-2963-4e93-990e-f463b796d704 | cephx       | magruela-share | ro           | active | AQDh........== | 2020-08-30T03:24:17.000000 | 2020-08-30T03:24:17.000000 |
    >   +--------------------------------------+-------------+----------------+--------------+--------+----------------+----------------------------+----------------------------+


    openstack \
        --os-cloud "${cloudname:?}" \
            share access show \
                --format json \
                "fd598fb6-2963-4e93-990e-f463b796d704"




    https://docs.openstack.org/manila/ocata/devref/cephfs_native_driver.html

        Access is controlled via Cephs cephx authentication system. When a user requests share access
        for an ID, Ceph creates a corresponding Ceph auth ID and a secret key, if they do not already
        exist, and authorizes the ID to access the share. The client can then mount the share using the
        ID and the secret key.

    vi /tmp/magruela-volume.yaml

    +   user: c005fd86-70ba-4ea6-acb4-db8d0dcb9b20

    +   user: 8169f87de174ad4ac98c32e59646488

    +   user: fd598fb6-2963-4e93-990e-f463b796d704


    vi /tmp/magruela-volume.yaml


# -----------------------------------------------------
# List the shares.
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
            share list
    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+
    >   | ID                                   | Name                                     | Size | Share Proto | Status    | Is Public | Share Type Name  | Host | Availability Zone |
    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+
    >   | ad1d9ca2-5b1c-4064-8c74-695286de6098 | gaia-dr2-share                           | 4399 | CEPHFS      | available | True      | cephfsnativetype |      | nova              |
    >   | a32ea61f-a922-4b9d-959b-a9d2d2e57c4b | magruela-share                           |    5 | CEPHFS      | available | True      | cephfsnativetype |      | nova              |
    >   | d71dae9a-b508-4a54-a5e9-7a91e8548b1e | pvc-41814ac5-00cc-4ba3-ae8b-6eddad01ef0c |    1 | CEPHFS      | available | False     | cephfsnativetype |      | nova              |
    >   +--------------------------------------+------------------------------------------+------+-------------+-----------+-----------+------------------+------+-------------------+


# -----------------------------------------------------
# List the share rules for the dynamic share.
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
            share access list \
                'd71dae9a-b508-4a54-a5e9-7a91e8548b1e'

    >   +--------------------------------------+-------------+------------------------------------------+--------------+--------+----------------+----------------------------+----------------------------+
    >   | id                                   | access_type | access_to                                | access_level | state  | access_key     | created_at                 | updated_at                 |
    >   +--------------------------------------+-------------+------------------------------------------+--------------+--------+----------------+----------------------------+----------------------------+
    >   | 474be0df-773e-4025-a19c-f0953b17124a | cephx       | pvc-41814ac5-00cc-4ba3-ae8b-6eddad01ef0c | rw           | active | AQBM........== | 2020-08-29T02:32:43.000000 | 2020-08-29T02:32:44.000000 |
    >   +--------------------------------------+-------------+------------------------------------------+--------------+--------+----------------+----------------------------+----------------------------+


# -----------------------------------------------------
# List the share rules for our static share.
#[user@kubernator]

    openstack \
        --os-cloud "${cloudname:?}" \
            share access list \
                'a32ea61f-a922-4b9d-959b-a9d2d2e57c4b'

    >   +--------------------------------------+-------------+----------------+--------------+--------+----------------+----------------------------+----------------------------+
    >   | id                                   | access_type | access_to      | access_level | state  | access_key     | created_at                 | updated_at                 |
    >   +--------------------------------------+-------------+----------------+--------------+--------+----------------+----------------------------+----------------------------+
    >   | fd598fb6-2963-4e93-990e-f463b796d704 | cephx       | magruela-share | ro           | active | AQDh........== | 2020-08-30T03:24:17.000000 | 2020-08-30T03:24:17.000000 |
    >   +--------------------------------------+-------------+----------------+--------------+--------+----------------+----------------------------+----------------------------+

    #
    # The error message contains the key for the dynamic share, AQBM........, not the key for the static share AQDh........==.

    >   Mounting arguments: -t ceph -o name=admin,secret=AQBM........== 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121 /var/lib/kubelet/pods/05d386f1-f2e0-42ec-940b-f5f685d25feb/volumes/kubernetes.io~cephfs/magruela-volume
    >   Output: mount: permission denied

    #
    # So we have been using the secret key from the wrong share rule.
    # TODO Need to create a K8s secret containing the key.
    #
    # TODO Make sure the secret is base64 encoded.
    # https://bugzilla.redhat.com/show_bug.cgi?id=1260388
    #

    rawkey=$(
        openstack \
            --os-cloud "${cloudname:?}" \
                share access show \
                    --format json \
                    'fd598fb6-2963-4e93-990e-f463b796d704' \
        | jq -r '.access_key'
        )

    kubectl \
        create secret \
            generic magruela-secret \
            --from-literal "key=${rawkey:?}"

    >   secret/magruela-secret created


    kubectl \
        get secret \
            --output json \
            magruela-secret

    >   {
    >       "apiVersion": "v1",
    >       "data": {
    >           "key": "QVFE........=="
    >       },
    >       "kind": "Secret",
    >       "metadata": {
    >           "creationTimestamp": "2020-08-30T05:17:25Z",
    >           "name": "magruela-secret",
    >           "namespace": "default",
    >           "resourceVersion": "946274",
    >           "selfLink": "/api/v1/namespaces/default/secrets/magruela-secret",
    >           "uid": "7ffa7c96-cfbd-4ca0-b67b-913fddf9c56f"
    >       },
    >       "type": "Opaque"
    >   }


# -----------------------------------------------------
# Delete the pod, claim and volume.
#[user@kubernator]

    kubectl delete \
        pod \
            magruela-pod

    kubectl delete \
        persistentvolumeclaim \
            magruela-claim

    kubectl delete \
        persistentvolume \
            magruela-volume

# -----------------------------------------------------
# Revert the changes.
#[user@kubernator]

    vi /tmp/magruela-volume.yaml

        secretRef:
    ~     name: magruela-secret
          namespace: default


# -----------------------------------------------------
# Create the volume, claim and pod.
#[user@kubernator]

    kubectl \
        apply \
            --filename /tmp/magruela-volume.yaml

    kubectl \
        apply \
            --filename /tmp/magruela-claim.yaml

    kubectl \
        apply \
            --filename /tmp/magruela-pod.yaml

    kubectl \
        describe pod \
            magruela-pod


    >   ....
    >   ....
    >   Events:
    >     Type     Reason       Age        From                                            Message
    >     ----     ------       ----       ----                                            -------
    >     Normal   Scheduled    <unknown>  default-scheduler                               Successfully assigned default/magruela-pod to tiberius-20200827-4tjt3wg4fxmz-node-0
    >     Warning  FailedMount  1s         kubelet, tiberius-20200827-4tjt3wg4fxmz-node-0  MountVolume.SetUp failed for volume "magruela-volume" : CephFS: mount failed: mount failed: exit status 32
    >   Mounting command: mount
    >   Mounting arguments: -t ceph -o name=admin,secret=AQDh........== 10.206.1.5:6789,10.206.1.6:6789,10.206.1.7:6789:/volumes/_nogroup/64964470-5ed9-4fe8-a655-e89e0f6cf121 /var/lib/kubelet/pods/33034226-f792-4772-a531-40133cdbd57d/volumes/kubernetes.io~cephfs/magruela-volume
    >   Output: mount: permission denied

    #
    # OK, we still get permission denied, but at least we are using the right secret.
    #

# -----------------------------------------------------

    Edit the dynamic share so that it will fail.
    Add the dynamic share to the test pod and see what we get in the error message.

# -----------------------------------------------------

    kubectl \
        get persistentvolume \
            --output yaml \
            pvc-41814ac5-00cc-4ba3-ae8b-6eddad01ef0c \
    | tee /tmp/dynamicpv.yaml

    >   ....
    >   ....

    vi /tmp/dynamicpv.yaml

        secretRef:
    -     name: manila-e80564ee-e99f-11ea-8c83-0a580a640409
    +     name: magruela-secret

    TODO Change the references to self ...




