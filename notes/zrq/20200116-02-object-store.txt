#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    #
    # Check we can access the Swift ObjectStore from VMs in the 'dev' deployment.
    # These notes try to use the s3:// protocol.
    #

    #
    # Resources
    # https://stackoverflow.com/questions/30385981/how-to-access-s3a-files-from-apache-spark
    # http://deploymentzone.com/2015/12/20/s3a-on-spark-on-aws-ec2/
    #

    #
    # Later tests can look at the swift:// protocol.
    # https://hadoop.apache.org/docs/current/hadoop-openstack/index.html
    # Looks like the swift:// protocol needs username/password auth.
    #

    #
    # A recent re-write, equivalent to s3:// -> s3a://
    # https://github.com/walmartlabs/hadoop-openstack-swifta
    # Allows api key auth ?
    #

    #
    # Resources
    # https://github.com/steveloughran/cloudstore
    # https://aajisaka.github.io/hadoop-document/hadoop-project/hadoop-aws/tools/hadoop-aws/troubleshooting_s3a.html
    # https://github.com/zioproto/hadoop-swift-tutorial
    #

# -----------------------------------------------------
# Login to the dev gateway machine.
#[user@desktop]

    ssh -A fedora@128.232.227.134

# -----------------------------------------------------
# Login to the dev storage machine.
#[fedora@stv-dev-gateway]

    ssh -A fedora@stv-dev-storage

    >   ssh: Could not resolve hostname stv-dev-storage: Name or service not known


    cat /etc/hosts

    >   127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
    >   ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6


    ssh -A fedora@10.0.0.17


# -----------------------------------------------------
# Check the original cvs files.
# http://unixetc.co.uk/2012/05/20/large-directory-causes-ls-to-hang/
#[fedora@stv-dev-storage]


    ls -1 -f cdn.gea.esac.esa.int/Gaia/gdr2/gaia_source/csv | head

    >   .
    >   ..
    >   GaiaSource_1252016708047892864_1252380852554448768.csv.gz
    >   GaiaSource_2406399566686663680_2406916856842782080.csv.gz
    >   GaiaSource_1824041377614968576_1824072855495382528.csv.gz
    >   GaiaSource_2187945435385334912_2188034809363207680.csv.gz
    >   GaiaSource_4075544907274553216_4075568929049219328.csv.gz
    >   GaiaSource_304619799555540096_304889493436904448.csv.gz
    >   GaiaSource_4474587192006350208_4474644920666013184.csv.gz
    >   GaiaSource_4204687701920764160_4204721138255170688.csv.gz


# -----------------------------------------------------
# Check the original cvs files in HDFS.
# https://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfs
# https://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/FileSystemShell.html#ls
#[fedora@stv-dev-storage]

    # Commands seem really really slow.
    # Adding 'time' to check
    # By the time I added 'time', things had speeded up.
    # Feels like the system needed time to load stuff out of cache.

    time \
        hdfs dfs -ls /hadoop

    >   Found 1 items
    >   drwxr-xr-x   - fedora supergroup          0 2020-01-09 23:03 /hadoop/gaia

    >   real	0m1.517s
    >   user	0m1.881s
    >   sys	0m0.115s


    time \
        hadoop fs -ls /hadoop

    >   Found 1 items
    >   drwxr-xr-x   - fedora supergroup          0 2020-01-09 23:03 /hadoop/gaia

    >   real	0m1.677s
    >   user	0m1.883s
    >   sys	0m0.148s


    time \
        hadoop fs -ls /hadoop/gaia

    >   Found 2 items
    >   drwxr-xr-x   - fedora supergroup          0 2020-01-09 14:48 /hadoop/gaia/csv
    >   drwxr-xr-x   - fedora supergroup          0 2020-01-09 23:03 /hadoop/gaia/parquet

    >   real	0m1.605s
    >   user	0m1.897s
    >   sys	0m0.120s


    time \
        hadoop fs -ls /hadoop/gaia/csv

    >   Found 1 items
    >   drwxr-xr-x   - fedora supergroup          0 2020-01-09 22:52 /hadoop/gaia/csv/gdr2

    >   real	0m1.468s
    >   user	0m1.823s
    >   sys	0m0.093s


    time \
        hadoop fs -ls /hadoop/gaia/csv/gdr2

    >   Found 1 items
    >   drwxr-xr-x   - fedora supergroup          0 2020-01-09 22:51 /hadoop/gaia/csv/gdr2/gaia_source

    >   real	0m1.687s
    >   user	0m1.961s
    >   sys	0m0.128s


    time \
        hadoop fs -ls /hadoop/gaia/csv/gdr2/gaia_source


    >   Found 61234 items
    >   -rw-r--r--   2 fedora supergroup    5347523 2020-01-09 14:50 /hadoop/gaia/csv/gdr2/gaia_source/GaiaSource_1000172165251650944_1000424567594791808.csv.gz
    >   -rw-r--r--   2 fedora supergroup    5024698 2020-01-09 14:50 /hadoop/gaia/csv/gdr2/gaia_source/GaiaSource_1000424601954531200_1000677322125743488.csv.gz
    >   -rw-r--r--   2 fedora supergroup    5976430 2020-01-09 14:50 /hadoop/gaia/csv/gdr2/gaia_source/GaiaSource_1000677386549270528_1000959999693425920.csv.gz
    >   -rw-r--r--   2 fedora supergroup    6102333 2020-01-09 14:50 /hadoop/gaia/csv/gdr2/gaia_source/GaiaSource_1000960034052654336_1001215258190537216.csv.gz
    >   ....
    >   ....
    >   -rw-r--r--   2 fedora supergroup    6606030 2020-01-09 19:02 /hadoop/gaia/csv/gdr2/gaia_source/GaiaSource_998905179963456512_999107593183061504.csv.gz
    >   -rw-r--r--   2 fedora supergroup    6099972 2020-01-09 19:02 /hadoop/gaia/csv/gdr2/gaia_source/GaiaSource_999107627542802944_999299737134906496.csv.gz
    >   -rw-r--r--   2 fedora supergroup    7062102 2020-01-09 19:02 /hadoop/gaia/csv/gdr2/gaia_source/GaiaSource_999299767199192704_999535170063180672.csv.gz
    >   -rw-r--r--   2 fedora supergroup    5795991 2020-01-09 19:02 /hadoop/gaia/csv/gdr2/gaia_source/GaiaSource_999535200126184320_999716967439074432.csv.gz
    >   -rw-r--r--   2 fedora supergroup    5240860 2020-01-09 19:02 /hadoop/gaia/csv/gdr2/gaia_source/GaiaSource_999717001796824064_999922369954904960.csv.gz
    >   -rw-r--r--   2 fedora supergroup    5375567 2020-01-09 19:02 /hadoop/gaia/csv/gdr2/gaia_source/GaiaSource_999922404314639104_1000172126596665472.csv.gz

    >   real	1m54.766s
    >   user	0m6.871s
    >   sys	0m0.717s


# -----------------------------------------------------

    #
    # Can we transfer some of these files into S3 ?
    #

# -----------------------------------------------------
# Identify a small set of files.
#[fedora@stv-dev-storage]

    time \
        hadoop fs -ls /hadoop/gaia/csv/gdr2/gaia_source/GaiaSource_1000*

    >   -rw-r--r--   2 fedora supergroup    5347523 2020-01-09 14:50 /hadoop/gaia/csv/gdr2/gaia_source/GaiaSource_1000172165251650944_1000424567594791808.csv.gz
    >   -rw-r--r--   2 fedora supergroup    5024698 2020-01-09 14:50 /hadoop/gaia/csv/gdr2/gaia_source/GaiaSource_1000424601954531200_1000677322125743488.csv.gz
    >   -rw-r--r--   2 fedora supergroup    5976430 2020-01-09 14:50 /hadoop/gaia/csv/gdr2/gaia_source/GaiaSource_1000677386549270528_1000959999693425920.csv.gz
    >   -rw-r--r--   2 fedora supergroup    6102333 2020-01-09 14:50 /hadoop/gaia/csv/gdr2/gaia_source/GaiaSource_1000960034052654336_1001215258190537216.csv.gz

    >   real	0m2.835s
    >   user	0m4.051s
    >   sys	0m0.292s


    #
    # Probably easier to transfer from local file system rather than pull from HDFS and push to S3.
    #

    time \
        ls -1 -f cdn.gea.esac.esa.int/Gaia/gdr2/gaia_source/csv/GaiaSource_1000*

    >   cdn.gea.esac.esa.int/Gaia/gdr2/gaia_source/csv/GaiaSource_1000172165251650944_1000424567594791808.csv.gz
    >   cdn.gea.esac.esa.int/Gaia/gdr2/gaia_source/csv/GaiaSource_1000424601954531200_1000677322125743488.csv.gz
    >   cdn.gea.esac.esa.int/Gaia/gdr2/gaia_source/csv/GaiaSource_1000677386549270528_1000959999693425920.csv.gz
    >   cdn.gea.esac.esa.int/Gaia/gdr2/gaia_source/csv/GaiaSource_1000960034052654336_1001215258190537216.csv.gz

    >   real	0m2.188s
    >   user	0m0.019s
    >   sys	0m0.052s


# -----------------------------------------------------
# Install the S3 client.
#[fedora@stv-dev-storage]

    sudo \
        dnf install s3cmd

    >   Installed:
    >     s3cmd-2.0.2-3.fc30.noarch
    >     python3-magic-5.36-2.fc30.noarch


# -----------------------------------------------------
# Configure the S3 client.
#[fedora@stv-dev-storage]

    s3cmd \
        --configure

    >   ....
    >   ....
    >   New settings:
    >     Access Key: 93d0....f83c
    >     Secret Key: 0e28....25b1
    >     Default Region: US
    >     S3 Endpoint: cumulus.openstack.hpc.cam.ac.uk:6780
    >     DNS-style bucket+hostname:port template for accessing a bucket: cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1/%(bucket)
    >     Encryption password:
    >     Path to GPG program: /usr/bin/gpg
    >     Use HTTPS protocol: True
    >     HTTP Proxy server name:
    >     HTTP Proxy server port: 0
    >
    >   Test access with supplied credentials? [Y/n] y
    >   Please wait, attempting to list all buckets...
    >   Success. Your access key and secret key worked fine :-)
    >
    >   Now verifying that encryption works...
    >   Not configured. Never mind.
    >
    >   Save settings? [y/N] y
    >   Configuration saved to '/home/fedora/.s3cfg'


# -----------------------------------------------------
# Create an S3 bucket for our csv files.
#[fedora@stv-dev-storage]

    bucketname=gaia-csv-001
    bucketuri=s3://${bucketname:?}

    s3cmd \
        mb "${bucketuri:?}"

    >   Bucket 's3://gaia-csv-001/' created


# -----------------------------------------------------
# Transfer a small set of files into our bucket.
#[fedora@stv-dev-storage]

    pushd cdn.gea.esac.esa.int/Gaia/gdr2/gaia_source/csv

        date

        for filename in $(ls -1 -f GaiaSource_1000*)
        do

            s3cmd \
                put "${filename:?}" "s3://${bucketname:?}/${filename:?}"

        done

        date

    popd

    >   Thu 16 Jan 17:08:41 UTC 2020

    >   upload: 'GaiaSource_1000172165251650944_1000424567594791808.csv.gz' -> 's3://gaia-csv-001/GaiaSource_1000172165251650944_1000424567594791808.csv.gz'  [1 of 1]
    >    5347523 of 5347523   100% in    0s    19.86 MB/s  done
    >   upload: 'GaiaSource_1000424601954531200_1000677322125743488.csv.gz' -> 's3://gaia-csv-001/GaiaSource_1000424601954531200_1000677322125743488.csv.gz'  [1 of 1]
    >    5024698 of 5024698   100% in    0s    19.33 MB/s  done
    >   upload: 'GaiaSource_1000677386549270528_1000959999693425920.csv.gz' -> 's3://gaia-csv-001/GaiaSource_1000677386549270528_1000959999693425920.csv.gz'  [1 of 1]
    >    5976430 of 5976430   100% in    0s    22.38 MB/s  done
    >   upload: 'GaiaSource_1000960034052654336_1001215258190537216.csv.gz' -> 's3://gaia-csv-001/GaiaSource_1000960034052654336_1001215258190537216.csv.gz'  [1 of 1]
    >    6102333 of 6102333   100% in    0s    22.72 MB/s  done

    >   Thu 16 Jan 17:08:43 UTC 2020


# -----------------------------------------------------
# Create an S3 bucket for our parquet files.
#[fedora@stv-dev-storage]

    bucketname=gaia-pqt-001
    bucketuri=s3://${bucketname:?}

    s3cmd \
        mb "${bucketuri:?}"

    >   Bucket 's3://gaia-pqt-001/' created


# -----------------------------------------------------
# SSH to the master node.
#[fedora@stv-dev-storage]

    ssh -A stv-dev-master


# -----------------------------------------------------
# Create our python convert program.
# https://github.com/wfau/aglais/blob/master/notes/stv/20200109-csv-2-parquet-gdr2.txt
# https://stackoverflow.com/a/54570310
# https://stackoverflow.com/q/57477385
# https://gist.github.com/tobilg/e03dbc474ba976b9f235#gistcomment-2929143
# https://github.com/apache/hadoop/blob/branch-2.9.2/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java#L75
#[fedora@stv-dev-master]


    mkdir "${HOME}/scripts"
    mkdir "${HOME}/scripts/zrq"

    cat << 'EOF' > "${HOME}/scripts/zrq/convert-001.py"

import time
import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql import *
from pyspark.sql import SQLContext
from pyspark.sql.session import SparkSession
from pyspark.conf import SparkConf
from pyspark.context import SparkContext

## Define the csv file Schema
gaia_schema = StructType([
    StructField("solution_id", LongType(), True),
    StructField("designation", StringType(), True),
    StructField("source_id", LongType(), True),
    StructField("random_index", LongType(), True),
    StructField("ref_epoch", DoubleType(), True),
    StructField("ra", DoubleType(), True),
    StructField("ra_error", DoubleType(), True),
    StructField("dec", DoubleType(), True),
    StructField("dec_error", DoubleType(), True),
    StructField("parallax", DoubleType(), True),
    StructField("parallax_error", DoubleType(), True),
    StructField("parallax_over_error", FloatType(), True),
    StructField("pmra", DoubleType(), True),
    StructField("pmra_error", DoubleType(), True),
    StructField("pmdec", DoubleType(), True),
    StructField("pmdec_error", DoubleType(), True),
    StructField("ra_dec_corr", FloatType(), True),
    StructField("ra_parallax_corr", FloatType(), True),
    StructField("ra_pmra_corr", FloatType(), True),
    StructField("ra_pmdec_corr", FloatType(), True),
    StructField("dec_parallax_corr", FloatType(), True),
    StructField("dec_pmra_corr", FloatType(), True),
    StructField("dec_pmdec_corr", FloatType(), True),
    StructField("parallax_pmra_corr", FloatType(), True),
    StructField("parallax_pmdec_corr", FloatType(), True),
    StructField("pmra_pmdec_corr", FloatType(), True),
    StructField("astrometric_n_obs_al", IntegerType(), True),
    StructField("astrometric_n_obs_ac", IntegerType(), True),
    StructField("astrometric_n_good_obs_al", IntegerType(), True),
    StructField("astrometric_n_bad_obs_al", IntegerType(), True),
    StructField("astrometric_gof_al", FloatType(), True),
    StructField("astrometric_chi2_al", FloatType(), True),
    StructField("astrometric_excess_noise", DoubleType(), True),
    StructField("astrometric_excess_noise_sig", DoubleType(), True),
    StructField("astrometric_params_solved", ShortType(), True),
    StructField("astrometric_primary_flag", BooleanType(), True),
    StructField("astrometric_weight_al", FloatType(), True),
    StructField("astrometric_pseudo_colour", DoubleType(), True),
    StructField("astrometric_pseudo_colour_error", DoubleType(), True),
    StructField("mean_varpi_factor_al", FloatType(), True),
    StructField("astrometric_matched_observations", DoubleType(), True),
    StructField("visibility_periods_used", ShortType(), True),
    StructField("astrometric_sigma5d_max", FloatType(), True),
    StructField("frame_rotator_object_type", IntegerType(), True),
    StructField("matched_observations", ShortType(), True),
    StructField("duplicated_source", BooleanType(), True),
    StructField("phot_g_n_obs", IntegerType(), True),
    StructField("phot_g_mean_flux", DoubleType(), True),
    StructField("phot_g_mean_flux_error", DoubleType(), True),
    StructField("phot_g_mean_flux_over_error", FloatType(), True),
    StructField("phot_g_mean_mag", FloatType(), True),
    StructField("phot_bp_n_obs", IntegerType(), True),
    StructField("phot_bp_mean_flux", DoubleType(), True),
    StructField("phot_bp_mean_flux_error", DoubleType(), True),
    StructField("phot_bp_mean_flux_over_error", FloatType(), True),
    StructField("phot_bp_mean_mag", FloatType(), True),
    StructField("phot_rp_n_obs", IntegerType(), True),
    StructField("phot_rp_mean_flux", DoubleType(), True),
    StructField("phot_rp_mean_flux_error", DoubleType(), True),
    StructField("phot_rp_mean_flux_over_error", FloatType(), True),
    StructField("phot_rp_mean_mag", FloatType(), True),
    StructField("phot_bp_rp_excess_factor", FloatType(), True),
    StructField("phot_proc_mode", ShortType(), True),
    StructField("bp_rp", FloatType(), True),
    StructField("bp_g", FloatType(), True),
    StructField("g_rp", FloatType(), True),
    StructField("radial_velocity", DoubleType(), True),
    StructField("radial_velocity_error", DoubleType(), True),
    StructField("rv_nb_transits", IntegerType(), True),
    StructField("rv_template_teff", FloatType(), True),
    StructField("rv_template_logg", FloatType(), True),
    StructField("rv_template_fe_h", FloatType(), True),
    StructField("phot_variable_flag", StringType(), True),
    StructField("l", DoubleType(), True),
    StructField("b", DoubleType(), True),
    StructField("ecl_lon", DoubleType(), True),
    StructField("ecl_lat", DoubleType(), True),
    StructField("priam_flags", LongType(), True),
    StructField("teff_val", FloatType(), True),
    StructField("teff_percentile_lower", FloatType(), True),
    StructField("teff_percentile_upper", FloatType(), True),
    StructField("a_g_val", FloatType(), True),
    StructField("a_g_percentile_lower", FloatType(), True),
    StructField("a_g_percentile_upper", FloatType(), True),
    StructField("e_bp_min_rp_val", FloatType(), True),
    StructField("e_bp_min_rp_percentile_lower", FloatType(), True),
    StructField("e_bp_min_rp_percentile_upper", FloatType(), True),
    StructField("flame_flags", LongType(), True),
    StructField("radius_val", FloatType(), True),
    StructField("radius_percentile_lower", FloatType(), True),
    StructField("radius_percentile_upper", FloatType(), True),
    StructField("lum_val", FloatType(), True),
    StructField("lum_percentile_lower", FloatType(), True),
    StructField("lum_percentile_upper", FloatType(), True),

])

conf = pyspark.SparkConf()

context = SparkContext.getOrCreate(
    conf=conf
    )
context._jsc.hadoopConfiguration().set(
    'fs.s3a.endpoint', 'https://cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1/'
    )
context._jsc.hadoopConfiguration().set(
    'fs.s3a.path.style.access', 'true'
    )
context._jsc.hadoopConfiguration().set(
    'fs.s3a.access.key', '93d0....f83c'
    )
context._jsc.hadoopConfiguration().set(
    'fs.s3a.secret.key', '0e28....25b1'
    )

session = SparkSession(
    context
    )

start = time.time()
print("-- [Staring conversion] --")

dataframe = session.read.option(
    "header",
    "true"
    ).schema(
        gaia_schema
        ).csv(
            "s3://gaia-csv-001/*"
            )

dataframe.write.parquet(
    's3://gaia-pqt-001/'
    )

end = time.time()
print("-- [Ended conversion] --")
print(str(end - start) + " seconds taken" )

EOF


# -----------------------------------------------------
# Run the python conversion job.
# https://github.com/wfau/aglais/blob/master/notes/stv/20200109-csv-2-parquet-gdr2.txt
#[fedora@stv-dev-master]

    nohup \
        spark-submit \
            --master yarn-client \
            --num-executors 6 \
            --executor-cores 4 \
            --executor-memory 12GB \
            convert.py \
             > conversion-log.txt &


    >   [1] 3007
    >   [fedora@stv-dev-master ~]$ nohup: ignoring input and redirecting stderr to stdout


    cat conversion-log.txt

    >   Warning: Master yarn-client is deprecated since 2.0. Please use master "yarn" with specified deploy mode instead.
    >   2020-01-16 18:47:16,289 INFO spark.SparkContext: Running Spark version 2.4.4
    >   2020-01-16 18:47:16,307 INFO spark.SparkContext: Submitted application: convert.py
    >   ....
    >   ....
    >   2020-01-16 18:47:21,217 INFO yarn.Client: Submitting application application_1578764470640_0014 to ResourceManager
    >   2020-01-16 18:47:21,237 INFO impl.YarnClientImpl: Submitted application application_1578764470640_0014
    >   2020-01-16 18:47:21,239 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1578764470640_0014 and attemptId None
    >   2020-01-16 18:47:22,243 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 18:47:22,246 INFO yarn.Client:
    >   	 client token: N/A
    >   	 diagnostics: [Thu Jan 16 18:47:21 +0000 2020] Application is added to the scheduler and is not yet activated. Queue's AM resource limit exceeded.  Details : AM Partition = <DEFAULT_PARTITION>; AM Resource Request = <memory:15000, vCores:1>; Queue Resource Limit for AM = <memory:12000, vCores:1>; User AM Resource Limit of the queue = <memory:12000, vCores:1>; Queue AM Resource Usage = <memory:15000, vCores:1>;
    >   	 ApplicationMaster host: N/A
    >   	 ApplicationMaster RPC port: -1
    >   	 queue: default
    >   	 start time: 1579200441225
    >   	 final status: UNDEFINED
    >   	 tracking URL: http://stv-dev-master:8088/proxy/application_1578764470640_0014/
    >   	 user: fedora
    >   2020-01-16 18:47:23,247 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 18:47:24,248 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 18:47:25,250 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 18:47:26,251 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   ....
    >   ....
    >   2020-01-16 18:53:11,656 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 18:53:12,657 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 18:53:13,658 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 18:53:14,659 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   ....
    >   ....
    >   2020-01-16 18:58:44,974 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 18:58:45,975 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 18:58:46,977 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 18:58:47,977 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   ....
    >   ....
    >   2020-01-16 19:38:53,211 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 19:38:54,211 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 19:38:55,212 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 19:38:56,213 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   ....
    >   ....
    >   2020-01-16 19:48:15,717 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 19:48:16,718 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 19:48:17,719 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 19:48:18,720 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   ....
    >   ....
    >   2020-01-16 20:03:16,498 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 20:03:17,499 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 20:03:18,500 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 20:03:19,501 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   ....
    >   ....
    >   2020-01-16 20:12:09,988 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 20:12:10,989 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 20:12:11,990 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 20:12:12,990 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   ....
    >   ....

    #
    # Ok, my bad - copy/paste error
    # Ran the original 'convert.py' script, not our new one.
    # re-converting the whole data set on HDFS ....
    # Very boring ..
    #

    #
    # While we are waiting, login to the worker nodes and see what they are doing #
    #


# -----------------------------------------------------
# Login to one of the worker nodes.
#[fedora@stv-dev-master]

    ssh -A stv-dev-worker-1


# -----------------------------------------------------
# -----------------------------------------------------
# See what the machine is doing.
#[fedora@stv-dev-worker-1]

    free -h

    >                 total        used        free      shared  buff/cache   available
    >   Mem:           21Gi       4.5Gi       1.4Gi       0.0Ki        15Gi        16Gi
    >   Swap:            0B          0B          0B


    df -h

    >   Filesystem      Size  Used Avail Use% Mounted on
    >   devtmpfs         11G     0   11G   0% /dev
    >   tmpfs            11G     0   11G   0% /dev/shm
    >   tmpfs            11G  508K   11G   1% /run
    >   tmpfs            11G     0   11G   0% /sys/fs/cgroup
    >   /dev/vda1       493G  301G  172G  64% /
    >   tmpfs           2.2G     0  2.2G   0% /run/user/1000


    ps -ef | sed -n '/^fedora/ p'

    >   fedora    9982 30309  0 Jan12 ?        00:00:00 bash /tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1578764470640_0013/container_1578764470640_0013_01_000007/default_container_executor.sh
    >   fedora    9984  9982  0 Jan12 ?        00:00:00 /bin/bash -c /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java -server -Xmx13312m -Djava.io.tmpdir=/tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1578764470640_0013/container_1578764470640_0013_01_000007/tmp '-Dspark.driver.port=37375' -Dspark.yarn.app.container.log.dir=/home/fedora/hadoop/logs/userlogs/application_1578764470640_0013/container_1578764470640_0013_01_000007 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@stv-dev-zeppelin:37375 --executor-id 6 --hostname stv-dev-worker-1 --cores 4 --app-id application_1578764470640_0013 --user-class-path file:/tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1578764470640_0013/container_1578764470640_0013_01_000007/__app__.jar 1>/home/fedora/hadoop/logs/userlogs/application_1578764470640_0013/container_1578764470640_0013_01_000007/stdout 2>/home/fedora/hadoop/logs/userlogs/application_1578764470640_0013/container_1578764470640_0013_01_000007/stderr
    >   fedora    9996  9984  0 Jan12 ?        00:34:49 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java -server -Xmx13312m -Djava.io.tmpdir=/tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1578764470640_0013/container_1578764470640_0013_01_000007/tmp -Dspark.driver.port=37375 -Dspark.yarn.app.container.log.dir=/home/fedora/hadoop/logs/userlogs/application_1578764470640_0013/container_1578764470640_0013_01_000007 -XX:OnOutOfMemoryError=kill %p org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@stv-dev-zeppelin:37375 --executor-id 6 --hostname stv-dev-worker-1 --cores 4 --app-id application_1578764470640_0013 --user-class-path file:/tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1578764470640_0013/container_1578764470640_0013_01_000007/__app__.jar
    >   fedora   10197  9996  0 Jan12 ?        00:00:23 python -m pyspark.daemon
    >   fedora   30135     1  0 Jan11 ?        00:00:00 /usr/lib/systemd/systemd --user
    >   fedora   30137 30135  0 Jan11 ?        00:00:00 (sd-pam)
    >   fedora   30183     1  0 Jan11 ?        00:12:16 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java -Dproc_datanode -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=ERROR,RFAS -Dyarn.log.dir=/home/fedora/hadoop/logs -Dyarn.log.file=hadoop-fedora-datanode-stv-dev-worker-1.novalocal.log -Dyarn.home.dir=/home/fedora/hadoop -Dyarn.root.logger=INFO,console -Djava.library.path=/home/fedora/hadoop/lib/native -Dhadoop.log.dir=/home/fedora/hadoop/logs -Dhadoop.log.file=hadoop-fedora-datanode-stv-dev-worker-1.novalocal.log -Dhadoop.home.dir=/home/fedora/hadoop -Dhadoop.id.str=fedora -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.server.datanode.DataNode
    >   fedora   30309     1  0 Jan11 ?        00:22:47 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java -Dproc_nodemanager -Djava.net.preferIPv4Stack=true -Dyarn.log.dir=/home/fedora/hadoop/logs -Dyarn.log.file=hadoop-fedora-nodemanager-stv-dev-worker-1.novalocal.log -Dyarn.home.dir=/home/fedora/hadoop -Dyarn.root.logger=INFO,console -Djava.library.path=/home/fedora/hadoop/lib/native -Dhadoop.log.dir=/home/fedora/hadoop/logs -Dhadoop.log.file=hadoop-fedora-nodemanager-stv-dev-worker-1.novalocal.log -Dhadoop.home.dir=/home/fedora/hadoop -Dhadoop.id.str=fedora -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.yarn.server.nodemanager.NodeManager
    >   fedora   30991 30988  0 19:50 ?        00:00:00 sshd: fedora@pts/0
    >   fedora   30992 30991  0 19:50 pts/0    00:00:00 -bash
    >   fedora   31022 30992  0 19:53 pts/0    00:00:00 ps -ef
    >   fedora   31023 30992  0 19:53 pts/0    00:00:00 sed -n /^fedora/ p


    top

    >   top - 20:02:41 up 31 days, 20:51,  1 user,  load average: 0.07, 0.02, 0.00
    >   Tasks: 114 total,   1 running, 113 sleeping,   0 stopped,   0 zombie
    >   %Cpu(s):  0.1 us,  0.1 sy,  0.0 ni, 99.8 id,  0.0 wa,  0.1 hi,  0.0 si,  0.0 st
    >   MiB Mem :  22085.7 total,   1374.5 free,   4580.7 used,  16130.5 buff/cache
    >   MiB Swap:      0.0 total,      0.0 free,      0.0 used.  17128.3 avail Mem
    >
    >     PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
    >   30309 fedora    20   0 7633928 254096  10440 S   0.3   1.1  22:49.61 java
    >       1 root      20   0  170680   8480   4068 S   0.0   0.0   0:19.69 systemd
    >       2 root      20   0       0      0      0 S   0.0   0.0   0:00.57 kthreadd
    >       3 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 rcu_gp
    >       4 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 rcu_par_gp
    >       6 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker/0:0H-kblockd


    exit

# -----------------------------------------------------
# -----------------------------------------------------
# Login to each of the worker nodes and peek at 'top'.
#[fedora@stv-dev-master]

    ssh -A stv-dev-worker-2

        top

    >   top - 20:04:23 up 31 days, 20:53,  1 user,  load average: 0.00, 0.00, 0.00
    >   Tasks: 112 total,   1 running, 111 sleeping,   0 stopped,   0 zombie
    >   %Cpu(s):  0.1 us,  0.0 sy,  0.0 ni, 98.6 id,  1.3 wa,  0.1 hi,  0.0 si,  0.0 st
    >   MiB Mem :  22085.7 total,   1352.4 free,   5335.8 used,  15397.5 buff/cache
    >   MiB Swap:      0.0 total,      0.0 free,      0.0 used.  16373.2 avail Mem
    >
    >     PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
    >    8806 fedora    20   0   15.7g   4.5g  17252 S   0.3  20.7  34:00.56 java
    >   29142 fedora    20   0 7620560 251876   9584 S   0.3   1.1  19:44.17 java
    >       1 root      20   0  170680   8448   4112 S   0.0   0.0   0:17.14 systemd
    >       2 root      20   0       0      0      0 S   0.0   0.0   0:00.44 kthreadd

    exit


    ssh -A stv-dev-worker-3

        top

    >   top - 20:05:37 up 31 days, 20:54,  1 user,  load average: 0.01, 0.00, 0.00
    >   Tasks: 112 total,   1 running, 111 sleeping,   0 stopped,   0 zombie
    >   %Cpu(s):  0.1 us,  0.1 sy,  0.0 ni, 99.8 id,  0.0 wa,  0.1 hi,  0.0 si,  0.0 st
    >   MiB Mem :  22085.7 total,    964.5 free,   5626.4 used,  15494.8 buff/cache
    >   MiB Swap:      0.0 total,      0.0 free,      0.0 used.  16082.7 avail Mem
    >
    >     PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
    >     535 fedora    20   0 7645668 261096  10392 S   0.7   1.2  20:26.62 java
    >     399 fedora    20   0 7604708 408440  11344 S   0.3   1.8  11:29.38 java
    >       1 root      20   0  170544   8408   4080 S   0.0   0.0   0:14.79 systemd
    >       2 root      20   0       0      0      0 S   0.0   0.0   0:00.44 kthreadd

    exit


    ssh -A stv-dev-worker-4

        top

    >   top - 20:08:02 up 31 days, 20:56,  1 user,  load average: 0.00, 0.00, 0.00
    >   Tasks: 114 total,   1 running, 113 sleeping,   0 stopped,   0 zombie
    >   %Cpu(s):  0.1 us,  0.1 sy,  0.0 ni, 99.8 id,  0.0 wa,  0.1 hi,  0.0 si,  0.0 st
    >   MiB Mem :  22085.7 total,   1806.3 free,   5866.1 used,  14413.4 buff/cache
    >   MiB Swap:      0.0 total,      0.0 free,      0.0 used.  15843.0 avail Mem
    >
    >     PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
    >    6894 fedora    20   0 7592100 577188  11100 S   0.3   2.6  12:15.36 java
    >    7020 fedora    20   0 7635628 247540  10440 S   0.3   1.1  22:09.51 java
    >       1 root      20   0  170548   8344   4024 S   0.0   0.0   0:16.79 systemd

    exit


    ssh -A stv-dev-worker-5

        top

    >   top - 20:09:14 up 31 days, 20:58,  1 user,  load average: 0.00, 0.02, 0.00
    >   Tasks: 112 total,   1 running, 111 sleeping,   0 stopped,   0 zombie
    >   %Cpu(s):  0.1 us,  0.1 sy,  0.0 ni, 99.7 id,  0.1 wa,  0.1 hi,  0.0 si,  0.0 st
    >   MiB Mem :  22085.9 total,   8058.7 free,   1201.9 used,  12825.3 buff/cache
    >   MiB Swap:      0.0 total,      0.0 free,      0.0 used.  20507.3 avail Mem
    >
    >     PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
    >    4595 fedora    20   0   15.1g 397736  40628 S   0.3   1.8   9:41.94 java
    >   21801 fedora    20   0 7638132 262264   6976 S   0.3   1.2 105:20.65 java
    >   24922 fedora    20   0 7600236 410556  10968 S   0.3   1.8  13:36.46 java
    >       1 root      20   0  170548   8592   4276 S   0.0   0.0   0:25.88 systemd
    >       2 root      20   0       0      0      0 S   0.0   0.0   0:00.72 kthreadd
    >       3 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 rcu_gp

    exit


    ssh -A stv-dev-worker-6

        top

    >   top - 20:11:09 up 31 days, 21:00,  1 user,  load average: 0.04, 0.01, 0.00
    >   Tasks: 108 total,   1 running, 107 sleeping,   0 stopped,   0 zombie
    >   %Cpu(s):  0.0 us,  0.1 sy,  0.0 ni, 99.9 id,  0.0 wa,  0.0 hi,  0.1 si,  0.0 st
    >   MiB Mem :  22085.7 total,  21285.6 free,    190.0 used,    610.1 buff/cache
    >   MiB Swap:      0.0 total,      0.0 free,      0.0 used.  21575.7 avail Mem
    >
    >     PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
    >    2435 fedora    20   0   20664   3820   3260 R   0.3   0.0   0:00.06 top
    >       1 root      20   0  170548   8864   4532 S   0.0   0.0   0:22.79 systemd
    >       2 root      20   0       0      0      0 S   0.0   0.0   0:00.67 kthreadd
    >       3 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 rcu_gp
    >       4 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 rcu_par_gp
    >       6 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker/0:0H-kblockd

    exit

        #
        # All of the nodes are idle ...
        # No sign of anyone doing anything ....
        #

# -----------------------------------------------------
# Look at what YARN thinks is happening.
#[fedora@stv-dev-master]

    yarn application -list

    >   2020-01-16 20:25:48,995 INFO client.RMProxy: Connecting to ResourceManager at stv-dev-master/10.0.0.14:8032
    >   Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):2
    >                   Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL
    >   application_1578764470640_0014	          convert.py	               SPARK	    fedora	   default	          ACCEPTED	         UNDEFINED	             0%	                                N/A
    >   application_1578764470640_0013	            Zeppelin	               SPARK	    fedora	   default	           RUNNING	         UNDEFINED	            10%	       http://stv-dev-zeppelin:4040

# -----------------------------------------------------
# Stop our task before it runs ...
#[fedora@stv-dev-master]

    yarn application -stop 'application_1578764470640_0014'

    >   2020-01-16 20:28:47,795 INFO client.RMProxy: Connecting to ResourceManager at stv-dev-master/10.0.0.14:8032
    >   Exception in thread "main" java.lang.IllegalArgumentException: App admin client class name not specified for type SPARK
    >   	at org.apache.hadoop.yarn.client.api.AppAdminClient.createAppAdminClient(AppAdminClient.java:76)
    >   	at org.apache.hadoop.yarn.client.cli.ApplicationCLI.run(ApplicationCLI.java:579)
    >   	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
    >   	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
    >   	at org.apache.hadoop.yarn.client.cli.ApplicationCLI.main(ApplicationCLI.java:123)

    # OK, that don't look good.


# -----------------------------------------------------
# Look at what YARN thinks is happening.
#[fedora@stv-dev-master]

    yarn application -list

    >   2020-01-16 20:29:34,071 INFO client.RMProxy: Connecting to ResourceManager at stv-dev-master/10.0.0.14:8032
    >   Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):2
    >                   Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL
    >   application_1578764470640_0014	          convert.py	               SPARK	    fedora	   default	          ACCEPTED	         UNDEFINED	             0%	                                N/A
    >   application_1578764470640_0013	            Zeppelin	               SPARK	    fedora	   default	           RUNNING	         UNDEFINED	            10%	       http://stv-dev-zeppelin:4040


# -----------------------------------------------------
# Kill our task before it runs ...
#[fedora@stv-dev-master]

    yarn application -kill 'application_1578764470640_0014'

    >   2020-01-16 20:30:08,258 INFO client.RMProxy: Connecting to ResourceManager at stv-dev-master/10.0.0.14:8032
    >   Killing application application_1578764470640_0014
    >   2020-01-16 20:30:08,737 INFO impl.YarnClientImpl: Killed application application_1578764470640_0014


# -----------------------------------------------------
# Look at what YARN thinks is happening.
#[fedora@stv-dev-master]

    yarn application -list

    >   2020-01-16 20:30:32,425 INFO client.RMProxy: Connecting to ResourceManager at stv-dev-master/10.0.0.14:8032
    >   Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):1
    >                   Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL
    >   application_1578764470640_0013	            Zeppelin	               SPARK	    fedora	   default	           RUNNING	         UNDEFINED	            10%	       http://stv-dev-zeppelin:4040
    >   [fedora@stv-dev-master ~]$


# -----------------------------------------------------
# Check the ouput log.
#[fedora@stv-dev-master]

    cat conversion-log.txt

    >   2020-01-16 20:30:07,848 INFO yarn.Client: Application report for application_1578764470640_0014 (state: ACCEPTED)
    >   2020-01-16 20:30:08,849 INFO yarn.Client: Application report for application_1578764470640_0014 (state: KILLED)
    >   2020-01-16 20:30:08,849 INFO yarn.Client:
    >   	 client token: N/A
    >   	 diagnostics: Application application_1578764470640_0014 was killed by user fedora at 10.0.0.14
    >   	 ApplicationMaster host: N/A
    >   	 ApplicationMaster RPC port: -1
    >   	 queue: default
    >   	 start time: 1579200441225
    >   	 final status: KILLED
    >   	 tracking URL: http://stv-dev-master:8088/cluster/app/application_1578764470640_0014
    >   	 user: fedora
    >   2020-01-16 20:30:08,861 INFO yarn.Client: Deleted staging directory hdfs://stv-dev-master:9000/user/fedora/.sparkStaging/application_1578764470640_0014
    >   2020-01-16 20:30:08,862 ERROR cluster.YarnClientSchedulerBackend: The YARN application has already ended! It might have been killed or the Application Master may have failed to start. Check the YARN application logs for more details.
    >   2020-01-16 20:30:08,863 ERROR spark.SparkContext: Error initializing SparkContext.
    >   org.apache.spark.SparkException: Application application_1578764470640_0014 was killed by user fedora at 10.0.0.14
    >   	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.waitForApplication(YarnClientSchedulerBackend.scala:94)
    >   	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:63)
    >   	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:183)
    >   	at org.apache.spark.SparkContext.<init>(SparkContext.scala:501)
    >   	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
    >   	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    >   	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    >   	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    >   	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    >   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
    >   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    >   	at py4j.Gateway.invoke(Gateway.java:238)
    >   	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
    >   	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
    >   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   2020-01-16 20:30:08,870 INFO server.AbstractConnector: Stopped Spark@690a8c24{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
    >   2020-01-16 20:30:08,872 INFO ui.SparkUI: Stopped Spark web UI at http://stv-dev-master.novalocal:4040
    >   2020-01-16 20:30:08,876 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!
    >   2020-01-16 20:30:08,879 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors
    >   2020-01-16 20:30:08,881 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
    >   2020-01-16 20:30:08,883 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices
    >   (serviceOption=None,
    >    services=List(),
    >    started=false)
    >   2020-01-16 20:30:08,884 INFO cluster.YarnClientSchedulerBackend: Stopped
    >   2020-01-16 20:30:08,887 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
    >   2020-01-16 20:30:08,892 INFO memory.MemoryStore: MemoryStore cleared
    >   2020-01-16 20:30:08,892 INFO storage.BlockManager: BlockManager stopped
    >   2020-01-16 20:30:08,897 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
    >   2020-01-16 20:30:08,897 WARN metrics.MetricsSystem: Stopping a MetricsSystem that is not running
    >   2020-01-16 20:30:08,898 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
    >   2020-01-16 20:30:08,903 INFO spark.SparkContext: Successfully stopped SparkContext
    >   Traceback (most recent call last):
    >     File "/home/fedora/convert.py", line 12, in <module>
    >       sc = SparkContext.getOrCreate()
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/context.py", line 367, in getOrCreate
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/context.py", line 136, in __init__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/context.py", line 198, in _do_init
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/context.py", line 306, in _initialize_context
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1525, in __call__
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
    >   : org.apache.spark.SparkException: Application application_1578764470640_0014 was killed by user fedora at 10.0.0.14
    >   	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.waitForApplication(YarnClientSchedulerBackend.scala:94)
    >   	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:63)
    >   	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:183)
    >   	at org.apache.spark.SparkContext.<init>(SparkContext.scala:501)
    >   	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
    >   	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    >   	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    >   	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    >   	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    >   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
    >   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    >   	at py4j.Gateway.invoke(Gateway.java:238)
    >   	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
    >   	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
    >   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
    >   	at java.lang.Thread.run(Thread.java:748)
    >
    >   2020-01-16 20:30:08,926 INFO util.ShutdownHookManager: Shutdown hook called
    >   2020-01-16 20:30:08,927 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-8de04f15-f91b-4988-8cf2-f94495505100
    >   2020-01-16 20:30:08,929 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-1f372497-5bc1-459a-94af-4b62931c8f11


# -----------------------------------------------------
# Run our S3 based conversion job.
# https://github.com/wfau/aglais/blob/master/notes/stv/20200109-csv-2-parquet-gdr2.txt
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-001.py" \
         > convert-001.log &


    >   2020-01-16 20:44:51,667 INFO yarn.Client: Application report for application_1578764470640_0015 (state: ACCEPTED)
    >   2020-01-16 20:44:52,668 INFO yarn.Client: Application report for application_1578764470640_0015 (state: ACCEPTED)
    >   2020-01-16 20:44:53,669 INFO yarn.Client: Application report for application_1578764470640_0015 (state: ACCEPTED)
    >   2020-01-16 20:44:54,670 INFO yarn.Client: Application report for application_1578764470640_0015 (state: ACCEPTED)
    >   2020-01-16 20:44:55,671 INFO yarn.Client: Application report for application_1578764470640_0015 (state: ACCEPTED)
    >   2020-01-16 20:44:56,672 INFO yarn.Client: Application report for application_1578764470640_0015 (state: ACCEPTED)


    #
    # Got bored and killed the stuck job.
    # Pending job ran immediately.
    #

    >   2020-01-17 01:48:00,020 INFO yarn.Client: Application report for application_1578764470640_0015 (state: ACCEPTED)
    >   2020-01-17 01:48:01,010 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> stv-dev-master, PROXY_URI_BASES -> http://stv-dev-master:8088/proxy/application_1578764470640_0015), /proxy/application_1578764470640_0015
    >   2020-01-17 01:48:01,012 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.
    >   2020-01-17 01:48:01,023 INFO yarn.Client: Application report for application_1578764470640_0015 (state: RUNNING)
    >   2020-01-17 01:48:01,024 INFO yarn.Client:
    >   	 client token: N/A
    >   	 diagnostics: N/A
    >   	 ApplicationMaster host: 10.0.0.28
    >   	 ApplicationMaster RPC port: -1
    >   	 queue: default
    >   	 start time: 1579207321439
    >   	 final status: UNDEFINED
    >   	 tracking URL: http://stv-dev-master:8088/proxy/application_1578764470640_0015/
    >   	 user: fedora
    >   2020-01-17 01:48:01,025 INFO cluster.YarnClientSchedulerBackend: Application application_1578764470640_0015 has started running.


    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-001.py", line 142, in <module>
    >       "s3://gaia-csv-001/*"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o36.csv.
    >   : java.io.IOException: No FileSystem for scheme: s3
    >   	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)
    >   	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)
    >   	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)
    >   	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
    >   	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
    >   	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)
    >   	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)
    >   	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:547)
    >   	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)
    >   	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
    >   	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
    >   	at scala.collection.immutable.List.foreach(List.scala:392)
    >   	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
    >   	at scala.collection.immutable.List.flatMap(List.scala:355)
    >   	at org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)
    >   	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)
    >   	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)
    >   	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
    >   	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:618)
    >   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    >   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    >   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    >   	at java.lang.reflect.Method.invoke(Method.java:498)
    >   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    >   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    >   	at py4j.Gateway.invoke(Gateway.java:282)
    >   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    >   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
    >   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
    >   	at java.lang.Thread.run(Thread.java:748)


# -----------------------------------------------------
# Try using 's3a://' rather than 's3://'
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-001.py"

        dataframe = session.read.option(
            "header",
            "true"
            ).schema(
                gaia_schema
                ).csv(
    -               "s3://gaia-csv-001/*"
    +               "s3a://gaia-csv-001/*"
                    )

        dataframe.write.parquet(
    -       's3://gaia-pqt-001/'
    +       's3a://gaia-pqt-001/'
            )


    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-001.py" \
         2> convert-001.err \
         1> convert-001.log \
          &

    cat cat convert-001.log

    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-001.py", line 142, in <module>
    >       "s3a://gaia-csv-001/*"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o36.csv.
    >   : java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    >   	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)
    >   	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)
    >   	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)
    >   	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)
    >   	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
    >   	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
    >   	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)
    >   	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)
    >   	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:547)
    >   	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)
    >   	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
    >   	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
    >   	at scala.collection.immutable.List.foreach(List.scala:392)
    >   	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
    >   	at scala.collection.immutable.List.flatMap(List.scala:355)
    >   	at org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)
    >   	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)
    >   	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)
    >   	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
    >   	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:618)
    >   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    >   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    >   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    >   	at java.lang.reflect.Method.invoke(Method.java:498)
    >   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    >   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    >   	at py4j.Gateway.invoke(Gateway.java:282)
    >   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    >   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
    >   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
    >   	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101)
    >   	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)
    >   	... 30 more


# -----------------------------------------------------
# Login to a worker node to check what jars it already has.
#[fedora@stv-dev-master]

    ssh -A stv-dev-worker-1

# -----------------------------------------------------
# Find what jars it already has.
#[fedora@stv-dev-worker-1]

    find . -name '*.jar'

    >   ./hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.3.jar
    >   ./hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.3.jar
    >   ./hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.3.jar
    >   ./hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.3.jar
    >   ....
    >   ....
    >   ./hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar
    >   ./hadoop/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar
    >   ./hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar
    >   ./hadoop/share/hadoop/common/lib/jersey-json-1.19.jar


# -----------------------------------------------------
# Find what AWS jar it has.
#[fedora@stv-dev-worker-1]


    find . -name '*aws*.jar'

    >   ./hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.271.jar
    >   ./hadoop/share/hadoop/tools/lib/hadoop-aws-3.1.3.jar


# -----------------------------------------------------
# Check the dependencies for hadoop-aws-3.1.3.
# https://stackoverflow.com/a/50367472
# https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/3.1.3
#[fedora@stv-dev-worker-1]

    find . -name '*hadoop-common*.jar'

    >   ./hadoop/share/hadoop/common/hadoop-common-3.1.3.jar
    >   ./hadoop/share/hadoop/common/hadoop-common-3.1.3-tests.jar
    >   ./hadoop/share/hadoop/common/sources/hadoop-common-3.1.3-test-sources.jar
    >   ./hadoop/share/hadoop/common/sources/hadoop-common-3.1.3-sources.jar


    find . -name '*hadoop-mapreduce-client*.jar'

    >   ./hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.3.jar
    >   ./hadoop/share/hadoop/mapreduce/sources/hadoop-mapreduce-client-core-3.1.3-sources.jar
    >   ./hadoop/share/hadoop/mapreduce/sources/hadoop-mapreduce-client-core-3.1.3-test-sources.jar


# -----------------------------------------------------
# Check what the running Java task is using.
#[fedora@stv-dev-worker-1]

    ps -ef | sed -n '/^fedora/p'

    >   fedora   30183     1  0 Jan11 ?        00:12:35 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java -Dproc_datanode -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=ERROR,RFAS -Dyarn.log.dir=/home/fedora/hadoop/logs -Dyarn.log.file=hadoop-fedora-datanode-stv-dev-worker-1.novalocal.log -Dyarn.home.dir=/home/fedora/hadoop -Dyarn.root.logger=INFO,console -Djava.library.path=/home/fedora/hadoop/lib/native -Dhadoop.log.dir=/home/fedora/hadoop/logs -Dhadoop.log.file=hadoop-fedora-datanode-stv-dev-worker-1.novalocal.log -Dhadoop.home.dir=/home/fedora/hadoop -Dhadoop.id.str=fedora -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.server.datanode.DataNode
    >   fedora   30309     1  0 Jan11 ?        00:24:04 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java -Dproc_nodemanager -Djava.net.preferIPv4Stack=true -Dyarn.log.dir=/home/fedora/hadoop/logs -Dyarn.log.file=hadoop-fedora-nodemanager-stv-dev-worker-1.novalocal.log -Dyarn.home.dir=/home/fedora/hadoop -Dyarn.root.logger=INFO,console -Djava.library.path=/home/fedora/hadoop/lib/native -Dhadoop.log.dir=/home/fedora/hadoop/logs -Dhadoop.log.file=hadoop-fedora-nodemanager-stv-dev-worker-1.novalocal.log -Dhadoop.home.dir=/home/fedora/hadoop -Dhadoop.id.str=fedora -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.yarn.server.nodemanager.NodeManager


    /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java
        -Dproc_datanode
        -Djava.net.preferIPv4Stack=true
        -Dhadoop.security.logger=ERROR,RFAS
        -Dyarn.log.dir=/home/fedora/hadoop/logs
        -Dyarn.log.file=hadoop-fedora-datanode-stv-dev-worker-1.novalocal.log
        -Dyarn.home.dir=/home/fedora/hadoop
        -Dyarn.root.logger=INFO,console
        -Djava.library.path=/home/fedora/hadoop/lib/native
        -Dhadoop.log.dir=/home/fedora/hadoop/logs
        -Dhadoop.log.file=hadoop-fedora-datanode-stv-dev-worker-1.novalocal.log
        -Dhadoop.home.dir=/home/fedora/hadoop
        -Dhadoop.id.str=fedora
        -Dhadoop.root.logger=INFO,RFA
        -Dhadoop.policy.file=hadoop-policy.xml
        org.apache.hadoop.hdfs.server.datanode.DataNode

    /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java
        -Dproc_nodemanager
        -Djava.net.preferIPv4Stack=true
        -Dyarn.log.dir=/home/fedora/hadoop/logs
        -Dyarn.log.file=hadoop-fedora-nodemanager-stv-dev-worker-1.novalocal.log
        -Dyarn.home.dir=/home/fedora/hadoop
        -Dyarn.root.logger=INFO,console
        -Djava.library.path=/home/fedora/hadoop/lib/native
        -Dhadoop.log.dir=/home/fedora/hadoop/logs
        -Dhadoop.log.file=hadoop-fedora-nodemanager-stv-dev-worker-1.novalocal.log
        -Dhadoop.home.dir=/home/fedora/hadoop
        -Dhadoop.id.str=fedora
        -Dhadoop.root.logger=INFO,RFA
        -Dhadoop.policy.file=hadoop-policy.xml
        -Dhadoop.security.logger=INFO,NullAppender
        org.apache.hadoop.yarn.server.nodemanager.NodeManager


# -----------------------------------------------------
# Check if the class is on the worker.
#[fedora@stv-dev-worker-1]

    grep -r 'S3AFileSystem' *

    >   Binary file hadoop/share/hadoop/tools/lib/hadoop-aws-3.1.3.jar matches
    >   hadoop/share/doc/hadoop/api/index-all.html:<div class="block">All the constants used with the <code>S3AFileSystem</code>.</div>
    >   hadoop/share/doc/hadoop/hadoop-aws/tools/hadoop-aws/testing.html:<li>Create logs, log things. Know that the <tt>S3AFileSystem</tt> and its input and output streams <i>all</i> provide useful statistics in their {{toString()}} calls; logging them is useful on its own.</li>
    >   hadoop/share/doc/hadoop/hadoop-aws/tools/hadoop-aws/index.html:    HTTP requests to the S3 back-end by S3AFileSystem.  The User-Agent header
    >   hadoop/share/doc/hadoop/hadoop-aws/tools/hadoop-aws/index.html:  &lt;value&gt;org.apache.hadoop.fs.s3a.S3AFileSystem&lt;/value&gt;
    >   ....
    >   ....
    >   hadoop/share/doc/hadoop/hadoop-project-dist/hadoop-common/core-default.html:    HTTP requests to the S3 back-end by S3AFileSystem.  The User-Agent header
    >   hadoop/share/doc/hadoop/hadoop-project-dist/hadoop-common/core-default.html:<td><a name="fs.s3a.impl">fs.s3a.impl</a></td><td>org.apache.hadoop.fs.s3a.S3AFileSystem</td><td>The implementation class of the S3A Filesystem</td>
    >   hadoop/etc/hadoop/log4j.properties:#log4j.logger.org.apache.hadoop.fs.s3a.S3AFileSystem=WARN

# -----------------------------------------------------
# This is helpful ...
#[fedora@stv-dev-worker-1]

    https://aajisaka.github.io/hadoop-document/hadoop-project/hadoop-aws/tools/hadoop-aws/troubleshooting_s3a.html#ClassNotFoundException:_org.apache.hadoop.fs.s3a.S3AFileSystem

        ClassNotFoundException: org.apache.hadoop.fs.s3a.S3AFileSystem

        These are Hadoop filesystem client classes, found in the hadoop-aws JAR.
        An exception reporting this class as missing means that this JAR is not on the classpath.


# -----------------------------------------------------
# -----------------------------------------------------
# Back to the master and check if the class is there.
#[fedora@stv-dev-master]

    grep -r 'S3AFileSystem' *

    >   ....
    >   Binary file hadoop/share/hadoop/tools/lib/hadoop-aws-3.1.3.jar matches
    >   ....

    #
    # It is in the Hadoop directory, but not in the Spark directory.
    #


# -----------------------------------------------------
# List the Hadoop jars in the Spark deployment.
#[fedora@stv-dev-master]

    ls -1 spark/jars/hadoop*

    >   spark/jars/hadoop-annotations-2.7.3.jar
    >   spark/jars/hadoop-auth-2.7.3.jar
    >   spark/jars/hadoop-client-2.7.3.jar
    >   spark/jars/hadoop-common-2.7.3.jar
    >   spark/jars/hadoop-hdfs-2.7.3.jar
    >   spark/jars/hadoop-mapreduce-client-app-2.7.3.jar
    >   spark/jars/hadoop-mapreduce-client-common-2.7.3.jar
    >   spark/jars/hadoop-mapreduce-client-core-2.7.3.jar
    >   spark/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar
    >   spark/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar
    >   spark/jars/hadoop-yarn-api-2.7.3.jar
    >   spark/jars/hadoop-yarn-client-2.7.3.jar
    >   spark/jars/hadoop-yarn-common-2.7.3.jar
    >   spark/jars/hadoop-yarn-server-common-2.7.3.jar
    >   spark/jars/hadoop-yarn-server-web-proxy-2.7.3.jar

    #
    # This is an old version of Hadoop released Aug 18, 2016.
    # https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common/2.7.3
    #

# -----------------------------------------------------
# Add the matching version of the AWS jar.
# https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/2.7.3
#[fedora@stv-dev-master]

    pushd "${HOME}/spark/jars"

        wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar

    popd


    >   --2020-01-17 03:20:39--  https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar
    >   Resolving repo1.maven.org (repo1.maven.org)... 151.101.16.209
    >   Connecting to repo1.maven.org (repo1.maven.org)|151.101.16.209|:443... connected.
    >   HTTP request sent, awaiting response... 200 OK
    >   Length: 126287 (123K) [application/java-archive]
    >   Saving to: hadoop-aws-2.7.3.jar
    >
    >   hadoop-aws-2.7.3.jar                                        100%[====....====>] 123.33K  --.-KB/s    in 0.02s
    >
    >   2020-01-17 03:20:44 (6.43 MB/s) - hadoop-aws-2.7.3.jar saved [126287/126287]


# -----------------------------------------------------
# Run the Spark job with the extra jars.
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-001.py" \
         2> convert-001.err \
         1> convert-001.log \
         &


    cat convert-001.log

    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-001.py", line 142, in <module>
    >       "s3a://gaia-csv-001/*"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o36.csv.
    >   : java.lang.NoClassDefFoundError: com/amazonaws/AmazonServiceException
    >   	at java.lang.Class.forName0(Native Method)
    >   	at java.lang.Class.forName(Class.java:348)
    >   	at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2134)
    >   	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2099)
    >   	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)
    >   	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)
    >   	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)
    >   	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)
    >   	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
    >   	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
    >   	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)
    >   	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)
    >   	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:45)
    >   	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:332)
    >   	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)
    >   	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
    >   	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:618)
    >   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    >   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    >   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    >   	at java.lang.reflect.Method.invoke(Method.java:498)
    >   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    >   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    >   	at py4j.Gateway.invoke(Gateway.java:282)
    >   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    >   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
    >   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   Caused by: java.lang.ClassNotFoundException: com.amazonaws.AmazonServiceException
    >   	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    >   	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
    >   	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
    >   	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
    >   	... 28 more


# -----------------------------------------------------
# Missing the AWS dependencies.
# https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/2.7.3
#[fedora@stv-dev-master]

    #
    # This version of hadoop-aws3 depends on a really old version of aws-java-sdk from Mar 20, 2014
    # https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/2.7.3
    # https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk/1.7.4

    pushd "${HOME}/spark/jars"

        wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar

    popd

    >   --2020-01-17 03:36:54--  https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar
    >   Resolving repo1.maven.org (repo1.maven.org)... 151.101.16.209
    >   Connecting to repo1.maven.org (repo1.maven.org)|151.101.16.209|:443... connected.
    >   HTTP request sent, awaiting response... 200 OK
    >   Length: 11948376 (11M) [application/java-archive]
    >   Saving to: aws-java-sdk-1.7.4.jar
    >
    >   aws-java-sdk-1.7.4.jar                                      100%[====....====>]  11.39M  53.1MB/s    in 0.2s
    >
    >   2020-01-17 03:36:59 (53.1 MB/s) - aws-java-sdk-1.7.4.jar saved [11948376/11948376]


# -----------------------------------------------------
# Run the Spark job with the extra jars.
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-001.py" \
         2> convert-001.err \
         | tee convert-001.log

    >   -- [Staring conversion] --

    #
    # Hangs for ... > 5min
    #

    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-001.py", line 142, in <module>
    >       "s3a://gaia-csv-001/*"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o36.csv.
    >   : com.amazonaws.AmazonClientException: Unable to execute HTTP request: gaia-csv-001.cumulus.openstack.hpc.cam.ac.uk: Name or service not known
    >   	at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:454)
    >   	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232)
    >   	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528)
    >   	at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1031)
    >   	at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:994)
    >   	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:297)
    >   	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)
    >   	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)
    >   	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
    >   	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
    >   	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)
    >   	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)
    >   	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:547)
    >   	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)
    >   	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
    >   	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
    >   	at scala.collection.immutable.List.foreach(List.scala:392)
    >   	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
    >   	at scala.collection.immutable.List.flatMap(List.scala:355)
    >   	at org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)
    >   	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)
    >   	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)
    >   	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
    >   	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:618)
    >   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    >   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    >   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    >   	at java.lang.reflect.Method.invoke(Method.java:498)
    >   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    >   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    >   	at py4j.Gateway.invoke(Gateway.java:282)
    >   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    >   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
    >   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
    >   	at java.lang.Thread.run(Thread.java:748)
    >   Caused by: java.net.UnknownHostException: gaia-csv-001.cumulus.openstack.hpc.cam.ac.uk: Name or service not known
    >   	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
    >   	at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929)
    >   	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324)
    >   	at java.net.InetAddress.getAllByName0(InetAddress.java:1277)
    >   	at java.net.InetAddress.getAllByName(InetAddress.java:1193)
    >   	at java.net.InetAddress.getAllByName(InetAddress.java:1127)
    >   	at org.apache.http.impl.conn.SystemDefaultDnsResolver.resolve(SystemDefaultDnsResolver.java:45)
    >   	at org.apache.http.impl.conn.DefaultClientConnectionOperator.resolveHostname(DefaultClientConnectionOperator.java:263)
    >   	at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:162)
    >   	at org.apache.http.impl.conn.ManagedClientConnectionImpl.open(ManagedClientConnectionImpl.java:326)
    >   	at org.apache.http.impl.client.DefaultRequestDirector.tryConnect(DefaultRequestDirector.java:610)
    >   	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:445)
    >   	at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:835)
    >   	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)
    >   	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)
    >   	at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:384)
    >   	... 34 more

    #
    # This is trying to add the bucket name to the service hostname.
    # 'gaia-csv-001.cumulus.openstack.hpc.cam.ac.uk'
    #


# -----------------------------------------------------
# Check the settings
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-001.py"

        context._jsc.hadoopConfiguration().set(
    -       'fs.s3a.path.style.access', 'true'
    +       'fs.s3a.path.style.access', True
            )


# -----------------------------------------------------
# Run the Spark job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-001.py" \
         2> convert-001.err \
         | tee convert-001.log

    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-001.py", line 120, in <module>
    >       'fs.s3a.path.style.access', True
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 332, in get_return_value
    >   py4j.protocol.Py4JError: An error occurred while calling o26.set. Trace:
    >   py4j.Py4JException: Method set([class java.lang.String, class java.lang.Boolean]) does not exist
    >   	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
    >   	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)
    >   	at py4j.Gateway.invoke(Gateway.java:274)
    >   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    >   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
    >   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
    >   	at java.lang.Thread.run(Thread.java:748)


# -----------------------------------------------------
# Check the settings
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-001.py"

        context._jsc.hadoopConfiguration().set(
    -       'fs.s3a.endpoint', 'https://cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1/'
    +       'fs.s3a.endpoint', 'cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1/'
            )
        context._jsc.hadoopConfiguration().set(
    -       'fs.s3a.path.style.access', True
    +       'fs.s3a.path.style.access', 'true'
            )


# -----------------------------------------------------
# Run the Spark job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-001.py" \
         2> convert-001.err \
         | tee convert-001.log


    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-001.py", line 142, in <module>
    >       "s3a://gaia-csv-001/*"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o36.csv.
    >   : com.amazonaws.AmazonClientException: Unable to execute HTTP request: gaia-csv-001.cumulus.openstack.hpc.cam.ac.uk: Name or service not known
    >   	at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:454)
    >   	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232)
    >   	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528)
    >   	at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1031)
    >   	at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:994)


# -----------------------------------------------------
# Check the settings
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-001.py"

        context._jsc.hadoopConfiguration().set(
    -       'fs.s3a.path.style.access', 'true'
    +       "fs.s3a.path.style.access", "true"
            )


    #
    # AARRGH
    # fs.s3a.path.style.access does not seem to work
    # https://issues.apache.org/jira/browse/DRILL-7289

        "It works on the S3A connector since hadoop-2.8 ..."

    #
    # Allow using path style addressing for accessing the s3 endpoint
    # https://issues.apache.org/jira/browse/HADOOP-12963

        Target version 2.8.0

    #
    # We have hadoop-hdfs-2.7.3 ...
    # In theory ... we could swap the 2.7.3 jars for 2.8.0 ...
    #

# -----------------------------------------------------
# The Hadoop deployment includes much more recent versions.
#[fedora@stv-dev-master]

    find hadoop -name '*hdfs*.jar'

    >   hadoop/share/hadoop/hdfs/hadoop-hdfs-3.1.3.jar
    >   hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.3.jar
    >   hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.3.jar
    >   hadoop/share/hadoop/hdfs/hadoop-hdfs-3.1.3-tests.jar
    >   hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.3-tests.jar
    >   hadoop/share/hadoop/hdfs/sources/hadoop-hdfs-client-3.1.3-test-sources.jar
    >   hadoop/share/hadoop/hdfs/sources/hadoop-hdfs-rbf-3.1.3-test-sources.jar
    >   hadoop/share/hadoop/hdfs/sources/hadoop-hdfs-native-client-3.1.3-test-sources.jar
    >   hadoop/share/hadoop/hdfs/sources/hadoop-hdfs-3.1.3-sources.jar
    >   hadoop/share/hadoop/hdfs/sources/hadoop-hdfs-client-3.1.3-sources.jar
    >   hadoop/share/hadoop/hdfs/sources/hadoop-hdfs-native-client-3.1.3-sources.jar
    >   hadoop/share/hadoop/hdfs/sources/hadoop-hdfs-rbf-3.1.3-sources.jar
    >   hadoop/share/hadoop/hdfs/sources/hadoop-hdfs-3.1.3-test-sources.jar
    >   hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.1.3-tests.jar
    >   hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.3.jar
    >   hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.1.3.jar
    >   hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.3-tests.jar
    >   hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.3.jar


    find hadoop -name '*aws*.jar'

    >   hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.271.jar
    >   hadoop/share/hadoop/tools/lib/hadoop-aws-3.1.3.jar

    #
    # The 'right' way of doing this is to deploy Spark without Hadoop, and then add our own Hadoop.
    # https://spark.apache.org/docs/latest/hadoop-provided.html
    #

    #
    # Another option would be to use the Swift protocol.
    # https://hadoop.apache.org/docs/current/hadoop-openstack/index.html
    #

    #
    # spark-submit add multiple jars.
    # https://stackoverflow.com/questions/29099115/spark-submit-add-multiple-jars-in-classpath
    # https://stackoverflow.com/a/29100219
    #

# -----------------------------------------------------
# Download the OpenStack Swift jar.
# https://hadoop.apache.org/docs/current/hadoop-openstack/index.html#Installing
#[fedora@stv-dev-master]

    #
    # We could use the latest version, but that would cause conflicts.
    # Better to use the same version.
    # https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-openstack/2.7.3


    pushd "${HOME}/spark/jars"

        wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-openstack/2.7.3/hadoop-openstack-2.7.3.jar

    popd

    >   --2020-01-17 05:34:49--  https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-openstack/2.7.3/hadoop-openstack-2.7.3.jar
    >   Resolving repo1.maven.org (repo1.maven.org)... 151.101.16.209
    >   Connecting to repo1.maven.org (repo1.maven.org)|151.101.16.209|:443... connected.
    >   HTTP request sent, awaiting response... 200 OK
    >   Length: 138528 (135K) [application/java-archive]
    >   Saving to: hadoop-openstack-2.7.3.jar
    >
    >   hadoop-openstack-2.7.3.jar                                  100%[====.....====>] 135.28K  --.-KB/s    in 0.02s
    >
    >   2020-01-17 05:34:49 (7.12 MB/s) - hadoop-openstack-2.7.3.jar saved [138528/138528]

# -----------------------------------------------------
# Swift storage settings.
# https://github.com/apache/spark/blob/master/docs/storage-openstack-swift.md#configuration-parameters

    fs.swift.service.PROVIDER.auth.url
    fs.swift.service.PROVIDER.auth.endpoint.prefix
    fs.swift.service.PROVIDER.tenant
    fs.swift.service.PROVIDER.username
    fs.swift.service.PROVIDER.password
    fs.swift.service.PROVIDER.http.port
    fs.swift.service.PROVIDER.region
    fs.swift.service.PROVIDER.public


# -----------------------------------------------------
# Create a new Python task.
# Settings from the Horizon GUI.
# https://cumulus.openstack.hpc.cam.ac.uk/project/api_access/view_credentials/
#[fedora@stv-dev-master]

    cp "${HOME}/scripts/zrq/convert-001.py" \
       "${HOME}/scripts/zrq/convert-002.py"

    vi "${HOME}/scripts/zrq/convert-002.py"

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.auth.url',
            'https://cumulus.openstack.hpc.cam.ac.uk:5000'
            )

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.tenant',
            'iris-gaia-dev'
            )

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.public',
            'false'
            )

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.username',
            '93d0########################f83c'
            )

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.password',
            '0e28########################25b1'
            )

        ....
        ....

        dataframe = session.read.option(
            "header",
            "true"
            ).schema(
                gaia_schema
                ).csv(
                    "swift://gaia-csv-001.cumulus/*"
                    )

        dataframe.write.parquet(
            'swift://gaia-pqt-001.cumulus/'
            )


# -----------------------------------------------------
# Run the Spark job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log

    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-002.py", line 155, in <module>
    >       "swift://gaia-csv-001.cumulus/*"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o37.csv.
    >   : Invalid Response: Method POST on https://cumulus.openstack.hpc.cam.ac.uk:5000 failed, status code: 405, status line: HTTP/1.1 405 METHOD NOT ALLOWED  POST https://cumulus.openstack.hpc.cam.ac.uk:5000 => 405 : <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
    >   <title>405 Method Not Allowed</title>
    >   <h1>Method Not Allowed</h1>
    >   <p>The method is not allowed for the requested URL.</p>
    >
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient.buildException(SwiftRestClient.java:1503)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient.perform(SwiftRestClient.java:1402)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient.authenticate(SwiftRestClient.java:1079)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient.authIfNeeded(SwiftRestClient.java:1298)


# -----------------------------------------------------
# Try just using the Swift URL.
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-002.py"

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.auth.url',
    -       'https://cumulus.openstack.hpc.cam.ac.uk:5000'
    +       'https://cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1'
            )


# -----------------------------------------------------
# Run the Spark job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log

    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-002.py", line 155, in <module>
    >       "swift://gaia-csv-001.cumulus/*"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o37.csv.
    >   : Invalid Response: Method POST on https://cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1 failed, status code: 405, status line: HTTP/1.1 405 Method Not Allowed  POST https://cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1 => 405 : MethodNotAllowed
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient.buildException(SwiftRestClient.java:1503)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient.perform(SwiftRestClient.java:1402)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient.authenticate(SwiftRestClient.java:1079)


# -----------------------------------------------------
# If we ask for the clouds yaml file we get a slightly different URL.
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-002.py"

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.auth.url',
    -       'https://cumulus.openstack.hpc.cam.ac.uk:5000'
    +       'https://cumulus.openstack.hpc.cam.ac.uk:5000/v3'
            )

# -----------------------------------------------------
# Run the Spark job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log

    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-002.py", line 155, in <module>
    >       "swift://gaia-csv-001.cumulus/*"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o37.csv.
    >   : Invalid Response: Method POST on https://cumulus.openstack.hpc.cam.ac.uk:5000/v3 failed, status code: 405, status line: HTTP/1.1 405 METHOD NOT ALLOWED  POST https://cumulus.openstack.hpc.cam.ac.uk:5000/v3 => 405 : <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
    >   <title>405 Method Not Allowed</title>
    >   <h1>Method Not Allowed</h1>
    >   <p>The method is not allowed for the requested URL.</p>
    >
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient.buildException(SwiftRestClient.java:1503)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient.perform(SwiftRestClient.java:1402)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient.authenticate(SwiftRestClient.java:1079)


# -----------------------------------------------------
# A clue in the Identity API v3 docs.
# https://docs.openstack.org/api-ref/identity/v3/index.html#password-authentication-with-unscoped-authorization
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-002.py"

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.auth.url',
    -       'https://cumulus.openstack.hpc.cam.ac.uk:5000/v3'
    +       'https://cumulus.openstack.hpc.cam.ac.uk:5000/v3/auth/tokens'
            )


# -----------------------------------------------------
# Run the Spark job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log


    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-002.py", line 155, in <module>
    >       "swift://gaia-csv-001.cumulus/*"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o37.csv.
    >   : java.lang.NullPointerException
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1123)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1084)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient.perform(SwiftRestClient.java:1406)

    Now we are ghetting somewhere.
    NullPointerException - that really helps !!


# -----------------------------------------------------
# Guess ...
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-002.py"

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.auth.url',
    -       'https://cumulus.openstack.hpc.cam.ac.uk:5000/v3/auth/tokens'
    +       'https://cumulus.openstack.hpc.cam.ac.uk:5000/v3/auth'
            )


# -----------------------------------------------------
# Run the Spark job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log


    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o37.csv.
    >   : org.apache.spark.sql.AnalysisException: Path does not exist: swift://gaia-csv-001.cumulus/*;
    >   	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:552)
    >   	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)

    #
    # Ok - that mght be progress of a sort ...
    #

# -----------------------------------------------------
# Which part of the path is wrong ?
# Try replace the wildcard with the full path to a csv.gz file.
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-002.py"


        dataframe = session.read.option(
            "header",
            "true"
            ).schema(
                gaia_schema
                ).csv(
    -               "swift://gaia-csv-001.cumulus/*"
    +               "swift://gaia-csv-001.cumulus/GaiaSource_1000172165251650944_1000424567594791808.csv.gz"
                    )


# -----------------------------------------------------
# Run the Spark job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log

    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o37.csv.
    >   : org.apache.spark.sql.AnalysisException: Path does not exist: swift://gaia-csv-001.cumulus/GaiaSource_1000172165251650944_1000424567594791808.csv.gz;
    >   	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)
    >   	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)


# -----------------------------------------------------
# -----------------------------------------------------
# Jump back to our S3 client and check we have the S3 bucket name right.
#[user@openstacker]

    s3cmd ls

    >   2020-01-16 17:08  s3://gaia-csv-001
    >   2020-01-16 17:10  s3://gaia-pqt-001


    s3cmd ls 's3://gaia-csv-001'

    >   2020-01-17 11:01   5347523   s3://gaia-csv-001/GaiaSource_1000172165251650944_1000424567594791808.csv.gz
    >   2020-01-17 11:01   5024698   s3://gaia-csv-001/GaiaSource_1000424601954531200_1000677322125743488.csv.gz
    >   2020-01-17 11:02   5976430   s3://gaia-csv-001/GaiaSource_1000677386549270528_1000959999693425920.csv.gz
    >   2020-01-17 11:02   6102333   s3://gaia-csv-001/GaiaSource_1000960034052654336_1001215258190537216.csv.gz

    s3cmd ls 's3://gaia-pqt-001'

    >   -


# -----------------------------------------------------
# -----------------------------------------------------
# Check if we are getting past the authentication step.
# Deliberatley mis-spell the username/password.
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-002.py"

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.username',
    -       '93d0....f83c'
    +       '####....f83c'
            )

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.password',
    -       '0e28....25b1'
    +       '####....25b1'
            )


# -----------------------------------------------------
# Run the Spark job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log


    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o37.csv.
    >   : org.apache.spark.sql.AnalysisException: Path does not exist: swift://gaia-csv-001.cumulus/GaiaSource_1000172165251650944_1000424567594791808.csv.gz;
    >   	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)
    >   	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)

    #
    # So no.
    # Are we getting a 404 error during the authentication but reporting that as 'Path does not exist'.
    #

# -----------------------------------------------------
# Test the raw auth URL
#[fedora@stv-dev-master]

    curl 'https://cumulus.openstack.hpc.cam.ac.uk:5000/v3/auth'

    >   {
    >   "error": {
    >       "message": "(https://cumulus.openstack.hpc.cam.ac.uk:5000/v3/auth): The resource could not be found.",
    >       "code": 404,
    >       "title": "Not Found"
    >       }
    >   }

    # OK, that might do it.
    # Try adding '/' to the end ..

    curl 'https://cumulus.openstack.hpc.cam.ac.uk:5000/v3/auth/'

    >   {
    >   "error": {
    >       "message": "(https://cumulus.openstack.hpc.cam.ac.uk:5000/v3/auth): The resource could not be found.",
    >       "code": 404,
    >       "title": "Not Found"
    >       }
    >   }

    # Try removing '/auth'

    curl 'https://cumulus.openstack.hpc.cam.ac.uk:5000/v3/auth/'

    >   {
    >   "version": {
    >       "status": "stable",
    >       "updated": "2018-10-15T00:00:00Z",
    >       "media-types": [
    >               {
    >               "base": "application/json",
    >               "type": "application/vnd.openstack.identity-v3+json"
    >               }
    >           ],
    >       "id": "v3.11",
    >       "links": [
    >               {
    >               "href": "https://cumulus.openstack.hpc.cam.ac.uk:5000/v3/",
    >               "rel": "self"
    >               }
    >           ]
    >       }
    >   }

    # Seem to remember beig here before, but try it anyway.


# -----------------------------------------------------
# Seem to remember beig here before, but try it anyway.
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-002.py"

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.auth.url',
    -       'https://cumulus.openstack.hpc.cam.ac.uk:5000/v3/auth'
    +       'https://cumulus.openstack.hpc.cam.ac.uk:5000/v3'
            )


# -----------------------------------------------------
# Run the Spark job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log


    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-002.py", line 155, in <module>
    >       "swift://gaia-csv-001.cumulus/GaiaSource_1000172165251650944_1000424567594791808.csv.gz"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o37.csv.
    >   : Invalid Response: Method POST on https://cumulus.openstack.hpc.cam.ac.uk:5000/v3 failed, status code: 405, status line: HTTP/1.1 405 METHOD NOT ALLOWED  POST https://cumulus.openstack.hpc.cam.ac.uk:5000/v3 => 405 : <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
    >   <title>405 Method Not Allowed</title>
    >   <h1>Method Not Allowed</h1>
    >   <p>The method is not allowed for the requested URL.</p>

    #
    # Yep, been here before.
    #


# -----------------------------------------------------
# Another guess.
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-002.py"

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.auth.url',
    -       'https://cumulus.openstack.hpc.cam.ac.uk:5000/v3'
    +       'https://cumulus.openstack.hpc.cam.ac.uk:5000/v3/tokens'
            )


# -----------------------------------------------------
# Run the Spark job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log


    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-002.py", line 155, in <module>
    >       "swift://gaia-csv-001.cumulus/GaiaSource_1000172165251650944_1000424567594791808.csv.gz"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o37.csv.
    >   : java.lang.NullPointerException
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1123)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1084)


# -----------------------------------------------------
# Repair the username/password.
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-002.py"

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.username',
    -       '####....f83c'
    +       '93d0....f83c'
            )

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.password',
    -       '####....25b1'
    +       '0e28....25b1'
            )


# -----------------------------------------------------
# Run the Spark job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log


    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-002.py", line 155, in <module>
    >       "swift://gaia-csv-001.cumulus/GaiaSource_1000172165251650944_1000424567594791808.csv.gz"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o37.csv.
    >   : java.lang.NullPointerException
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1123)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1084)

# -----------------------------------------------------
# Go back the our api_access settings file ..
# https://cumulus.openstack.hpc.cam.ac.uk/project/api_access/openrc/
#[fedora@stv-dev-master]

    >   export OS_AUTH_URL=https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
    >   export OS_PROJECT_ID=08e2....d927
    >   export OS_PROJECT_NAME="iris-gaia-dev"
    >
    >   export OS_USER_DOMAIN_NAME="Federated"
    >   export OS_PROJECT_DOMAIN_ID="default"
    >
    >   unset OS_TENANT_ID
    >   unset OS_TENANT_NAME
    >
    >   export OS_USERNAME="xxxx"
    >   export OS_PASSWORD="xxxx"
    >
    >   export OS_REGION_NAME="RegionOne"
    >   export OS_INTERFACE=public
    >   export OS_IDENTITY_API_VERSION=3

# -----------------------------------------------------

    #
    # Looking at the source code for SwiftRestClient, it should throw a
    # SwiftConfigurationException if we don't give it a password.
    # https://github.com/openstack/sahara-extra/blob/master/hadoop-swiftfs/src/main/java/org/apache/hadoop/fs/swift/http/SwiftRestClient.java#L517

# -----------------------------------------------------
# Rename the password property.
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-002.py"

        context._jsc.hadoopConfiguration().set(
    -       'fs.swift.service.cumulus.password',
    +       'fs.swift.service.cumulus.mumble',
            '0e28....25b1'
            )


# -----------------------------------------------------
# Run the Spark job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log

    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-002.py", line 150, in <module>
    >       "swift://gaia-csv-001.cumulus/GaiaSource_1000172165251650944_1000424567594791808.csv.gz"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o36.csv.
    >   : org.apache.hadoop.fs.swift.exceptions.SwiftConfigurationException: Configuration for swift://gaia-csv-001.cumulus/GaiaSource_1000172165251650944_1000424567594791808.csv.gz must contain either fs.swift.password or fs.swift.apikey
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient.<init>(SwiftRestClient.java:469)

    #
    # YES !! we got a sensinble response (from the client).
    # At least we know it is reading these config params.
    #

# -----------------------------------------------------
# Restore the password property.
# Add the domain name (OS_USER_DOMAIN_NAME) and domain id (OS_PROJECT_DOMAIN_ID) properties.
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-002.py"

        context._jsc.hadoopConfiguration().set(
    -       'fs.swift.service.cumulus.mumble',
    +       'fs.swift.service.cumulus.password',
            '0e28....25b1'
            )

    +   context._jsc.hadoopConfiguration().set(
    +       'fs.swift.service.cumulus.domain.name',
    +       'Federated'
    +       )

    +   context._jsc.hadoopConfiguration().set(
    +       'fs.swift.service.cumulus.domain.id',
    +       'default'
    +       )


# -----------------------------------------------------
# Run the Spark job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log

    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-002.py", line 160, in <module>
    >       "swift://gaia-csv-001.cumulus/GaiaSource_1000172165251650944_1000424567594791808.csv.gz"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o38.csv.
    >   : java.lang.NullPointerException
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1123)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1084)


# -----------------------------------------------------
# Use different values for the domain name (OS_PROJECT_NAME) and domain id (OS_PROJECT_ID) properties.
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-002.py"

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.domain.name',
            'iris-gaia-dev'
            )

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.domain.id',
            '08e2....d927'
            )

    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-002.py", line 160, in <module>
    >       "swift://gaia-csv-001.cumulus/GaiaSource_1000172165251650944_1000424567594791808.csv.gz"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o38.csv.
    >   : java.lang.NullPointerException
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1123)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1084)



    #
    # A clue abiut the URL here ..
    # https://github.com/openstack/sahara-extra/blob/master/hadoop-swiftfs/src/main/java/org/apache/hadoop/fs/swift/http/SwiftRestClient.java#L515

        boolean isV3 = stringAuthUri.contains("/v3/auth/tokens");

    #
    # That is part of a commit from 5yrs ago (4 Mar 2015).
    # https://github.com/openstack/sahara-extra/commit/35fef155504644372a0d013e4768c42508e63779

    #
    # We are using a jar from 4yrs ago (Aug 18, 2016)
    # https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-openstack/2.7.3

    The latest code doesn't match the stack trace.
    The stack trace has :

	    at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1123)
	    at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1084)

    The source code indicates that would be

	    at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1222)
	    at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResultV3(SwiftRestClient.java:1350)

    We could try just dropping the new jar into place.
    The swift jars don't have many complicated dependencies.

# -----------------------------------------------------
# Download the latest OpenStack Swift jar.
# https://hadoop.apache.org/docs/current/hadoop-openstack/index.html#Installing
#[fedora@stv-dev-master]

    pushd "${HOME}/spark/jars"

        rm hadoop-openstack-2.7.3.jar
        wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-openstack/3.2.1/hadoop-openstack-3.2.1.jar

    popd

    >   --2020-01-17 13:41:15--  https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-openstack/3.2.1/hadoop-openstack-3.2.1.jar
    >   Resolving repo1.maven.org (repo1.maven.org)... 151.101.16.209
    >   Connecting to repo1.maven.org (repo1.maven.org)|151.101.16.209|:443... connected.
    >   HTTP request sent, awaiting response... 200 OK
    >   Length: 160034 (156K) [application/java-archive]
    >   Saving to: hadoop-openstack-3.2.1.jar
    >
    >   hadoop-openstack-3.2.1.jar                                  100%[====....====>] 156.28K  --.-KB/s    in 0.02s
    >
    >   2020-01-17 13:41:20 (7.22 MB/s) - hadoop-openstack-3.2.1.jar saved [160034/160034]

# -----------------------------------------------------
# Check our config properties.
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-002.py"


        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.auth.url',
            'https://cumulus.openstack.hpc.cam.ac.uk:5000/v3/auth/tokens'
            )

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.auth.endpoint.prefix',
            'endpoints'
            )

        context._jsc.hadoopConfiguration().setBoolean(
            'fs.swift.service.cumulus.public',
            True
            )

        context._jsc.hadoopConfiguration().set(
           'fs.swift.service.cumulus.tenant',
           'iris-gaia-dev'
           )

        context._jsc.hadoopConfiguration().set(
           'fs.swift.service.cumulus.region',
           'RegionOne'
           )

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.username',
            '93d0....f83c'
            )

        context._jsc.hadoopConfiguration().set(
            'fs.swift.service.cumulus.password',
            '0e28....25b1'
            )


# -----------------------------------------------------
# Run the Spark job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log

    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-002.py", line 166, in <module>
    >       "swift://gaia-csv-001.cumulus/GaiaSource_1000172165251650944_1000424567594791808.csv.gz"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o39.csv.
    >   : java.lang.NullPointerException
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1145)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1105)

    #
    # That suggests it didn't pick up the new jar version.
    #

# -----------------------------------------------------
# Check if we have other jars around ...
#[fedora@stv-dev-master]

    grep -r 'SwiftRestClient' *

    >   ....
    >   Binary file hadoop/share/hadoop/tools/lib/hadoop-openstack-3.1.3.jar matches
    >   ....
    >   Binary file spark/jars/hadoop-openstack-3.2.1.jar matches

    #
    # We have to copies of the jar, wich are recent versions.
    # The stack trace is showing method signatures from much older versions.
    # Suggests we need to restart the process to get the latest jar.
    #

    ps -ef | grep '^fedora'
    ps -ef | grep 'java'

    >   ....
    >   fedora   14707  1  0  Jan11 00:14:02 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.hadoop.hdfs.server.namenode.NameNode
    >   fedora   14957  1  0  Jan11 00:04:19 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode
    >   fedora   15188  1  0  Jan11 00:50:01 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.hadoop.yarn.server.resourcemanager.ResourceManager
    >   fedora   17975  1  0   2019 00:11:44 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.spark.deploy.SparkSubmit ....
    >   fedora   18056  1  0   2019 00:11:41 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.spark.deploy.SparkSubmit ....
    >   ....


# -----------------------------------------------------
# List active yarn jobs.
#[fedora@stv-dev-master]

    yarn application -list

    >   2020-01-17 14:00:57,466 INFO client.RMProxy: Connecting to ResourceManager at stv-dev-master/10.0.0.14:8032
    >   Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):0
    >                   Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL


# -----------------------------------------------------
# What happens if we run an example.
# https://github.com/wfau/aglais/blob/master/notes/stv/20191209-openstack-deployment.txt#L677
# (original has wrong path for examples)
#[fedora@stv-dev-master]

    spark-submit \
        --class org.apache.spark.examples.SparkPi \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 512m \
        spark/examples/jars/spark-examples*.jar \
        10 \
        2> example.err \
        1> example.log \
        &


    ps -ef | grep 'java'

    >   fedora   14707     1  0 Jan11 ?        00:14:03 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.hadoop.hdfs.server.namenode.NameNode
    >   fedora   14957     1  0 Jan11 ?        00:04:19 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode
    >   fedora   15188     1  0 Jan11 ?        00:50:07 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.hadoop.yarn.server.resourcemanager.ResourceManager
    >   fedora   17975     1  0  2019 ?        00:11:44 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.spark.deploy.SparkSubmit .... pyspark-shell
    >   fedora   18056     1  0  2019 ?        00:11:42 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.spark.deploy.SparkSubmit .... pyspark-shell


    yarn application -list

    >   2020-01-17 14:10:51,080 INFO client.RMProxy: Connecting to ResourceManager at stv-dev-master/10.0.0.14:8032
    >   Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):0
    >                   Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL


    ps -ef | grep 'java'

    >   fedora   13909  2747 99 14:10 pts/0    00:00:01 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.spark.deploy.SparkSubmit .... spark/examples/jars/spark-examples_2.11-2.4.4.jar
    >   fedora   14707     1  0 Jan11 ?        00:14:03 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.hadoop.hdfs.server.namenode.NameNode
    >   fedora   14957     1  0 Jan11 ?        00:04:19 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode
    >   fedora   15188     1  0 Jan11 ?        00:50:07 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.hadoop.yarn.server.resourcemanager.ResourceManager
    >   fedora   17975     1  0  2019 ?        00:11:44 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.spark.deploy.SparkSubmit .... pyspark-shell
    >   fedora   18056     1  0  2019 ?        00:11:42 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.spark.deploy.SparkSubmit .... pyspark-shell


    ps -ef | grep 'java'

    >   fedora   14707     1  0 Jan11 ?        00:14:03 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.hadoop.hdfs.server.namenode.NameNode
    >   fedora   14957     1  0 Jan11 ?        00:04:20 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode
    >   fedora   15188     1  0 Jan11 ?        00:50:08 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.hadoop.yarn.server.resourcemanager.ResourceManager
    >   fedora   17975     1  0  2019 ?        00:11:44 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.spark.deploy.SparkSubmit .... pyspark-shell
    >   fedora   18056     1  0  2019 ?        00:11:42 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.spark.deploy.SparkSubmit .... pyspark-shell

    #
    # I think we can kill off the instances of SparkSubmit from 2019 ..

    kill -9 18056
    kill -9 17975

    ps -ef | grep 'java'

    >   fedora   14707     1  0 Jan11 ?        00:14:04 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.hadoop.hdfs.server.namenode.NameNode
    >   fedora   14957     1  0 Jan11 ?        00:04:20 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode
    >   fedora   15188     1  0 Jan11 ?        00:50:08 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.fc30.x86_64/jre//bin/java .... org.apache.hadoop.yarn.server.resourcemanager.ResourceManager


# -----------------------------------------------------
# Check the available copies of SwiftRestClient ...
#[fedora@stv-dev-master]

    grep -r 'SwiftRestClient' *

    >   ....
    >   Binary file hadoop/share/hadoop/tools/lib/hadoop-openstack-3.1.3.jar matches
    >   ....
    >   Binary file spark/jars/hadoop-openstack-3.2.1.jar matches
    >   ....


# -----------------------------------------------------
# Run our Spark job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log


    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-002.py", line 166, in <module>
    >       "swift://gaia-csv-001.cumulus/GaiaSource_1000172165251650944_1000424567594791808.csv.gz"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o39.csv.
    >   : java.lang.NullPointerException
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1145)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1105)

    #
    # No idea where it is getting the older version from.
    #


# -----------------------------------------------------
# Can we specify additional jars to send to the workers.
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        --jars "${HOME}/spark/jars/hadoop-openstack-3.2.1.jar" \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log


    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-002.py", line 166, in <module>
    >       "swift://gaia-csv-001.cumulus/GaiaSource_1000172165251650944_1000424567594791808.csv.gz"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o41.csv.
    >   : java.lang.NullPointerException
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1145)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1105)

    #
    # Stack trace  still looks like an old version.
    #

# -----------------------------------------------------
# Delete the SwiftRestClient jar from spark library.
#[fedora@stv-dev-master]

    rm "${HOME}/spark/jars/hadoop-openstack-3.2.1.jar"


# -----------------------------------------------------
# Run the job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log

    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-002.py", line 166, in <module>
    >       "swift://gaia-csv-001.cumulus/GaiaSource_1000172165251650944_1000424567594791808.csv.gz"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o39.csv.
    >   : java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem not found
    >   	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)
    >   	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)

    # Riiight.
    # So that is the jar file it is using ..
    #

# -----------------------------------------------------
# Download the latest OpenStack Swift jar.
# https://hadoop.apache.org/docs/current/hadoop-openstack/index.html#Installing
#[fedora@stv-dev-master]

    pushd "${HOME}/spark/jars"

        wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-openstack/3.2.1/hadoop-openstack-3.2.1.jar

    popd

    >   --2020-01-17 14:38:18--  https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-openstack/3.2.1/hadoop-openstack-3.2.1.jar
    >   Resolving repo1.maven.org (repo1.maven.org)... 151.101.16.209
    >   Connecting to repo1.maven.org (repo1.maven.org)|151.101.16.209|:443... connected.
    >   HTTP request sent, awaiting response... 200 OK
    >   Length: 160034 (156K) [application/java-archive]
    >   Saving to: hadoop-openstack-3.2.1.jar
    >
    >   hadoop-openstack-3.2.1.jar                                  100%[====....====>] 156.28K  --.-KB/s    in 0.02s
    >
    >   2020-01-17 14:38:23 (8.15 MB/s) - hadoop-openstack-3.2.1.jar saved [160034/160034]


# -----------------------------------------------------
# Run the job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log


    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-002.py", line 166, in <module>
    >       "swift://gaia-csv-001.cumulus/GaiaSource_1000172165251650944_1000424567594791808.csv.gz"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o39.csv.
    >   : java.lang.NullPointerException
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1145)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1105)


    #
    # Found source code in the Apache svn repo.
    # http://svn.apache.org/viewvc/hadoop/common/trunk/hadoop-tools/hadoop-openstack/src/main/java/org/apache/hadoop/fs/swift/http/SwiftRestClient.java
    # Very similar to what we see in the stack trace.
    # Not changed for over 6yrs.
    #

# -----------------------------------------------------
# Check for a key phrase in the new version.
#[fedora@stv-dev-master]

    grep -r 'v3/auth/tokens' *

    >   scripts/zrq/convert-002.py:    'https://cumulus.openstack.hpc.cam.ac.uk:5000/v3/auth/tokens'

    #
    # I would have expected to see that in the jar file.
    # Perhaps not ... Java strings might not be plain text in the jar files.
    #

    grep -r 'SwiftRestClient' *

    >   ....
    >   Binary file hadoop/share/hadoop/tools/lib/hadoop-openstack-3.1.3.jar matches
    >   ....
    >   Binary file spark/jars/hadoop-openstack-3.2.1.jar matches
    >   ....

    #
    # Downloaded the source jar file for hadoop-openstack-3.2.1 from here:
    # https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-openstack/3.2.1/
    # https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-openstack/3.2.1/hadoop-openstack-3.2.1-sources.jar
    #

    SwiftRestClient is V2 only.
    None of the changes to handle v3 protocol.
    Marvellous.

    #
    # There is a version from CERN
    # https://repo1.maven.org/maven2/ch/cern/hadoop/hadoop-openstack/2.7.5.1/
    # https://repo1.maven.org/maven2/ch/cern/hadoop/hadoop-openstack/2.7.5.1/hadoop-openstack-2.7.5.1-sources.jar

    Also old version.
    SwiftRestClient is V2 only.
    None of the changes to handle v3 protocol.

    #
    # There is a version from OpenStack Sahara.
    # http://tarballs.openstack.org/sahara/dist/hadoop-openstack/stable/train/hadoop-openstack-3.0.1.jar


# -----------------------------------------------------
# Download the latest OpenStack Swift jar from OpenStack Sahara.
# http://tarballs.openstack.org/sahara/dist/hadoop-openstack/stable/train/
#[fedora@stv-dev-master]

    pushd "${HOME}/spark/jars"

        rm hadoop-openstack-*.jar
        wget http://tarballs.openstack.org/sahara/dist/hadoop-openstack/stable/train/hadoop-openstack-3.0.1.jar

    popd

    >   --2020-01-17 15:43:25--  http://tarballs.openstack.org/sahara/dist/hadoop-openstack/stable/train/hadoop-openstack-3.0.1.jar
    >   Resolving tarballs.openstack.org (tarballs.openstack.org)... 23.253.108.137, 2001:4800:7817:104:be76:4eff:fe05:dbee
    >   Connecting to tarballs.openstack.org (tarballs.openstack.org)|23.253.108.137|:80... connected.
    >   HTTP request sent, awaiting response... 200 OK
    >   Length: 134216 (131K) [application/java-archive]
    >   Saving to: hadoop-openstack-3.0.1.jar
    >
    >   hadoop-openstack-3.0.1.jar                                  100%[====....====>] 131.07K   387KB/s    in 0.3s
    >
    >   2020-01-17 15:43:26 (387 KB/s) - hadoop-openstack-3.0.1.jar saved [134216/134216]


# -----------------------------------------------------
# Run the job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-002.py" \
         2> convert-002.err \
         | tee convert-002.log

    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-002.py", line 166, in <module>
    >       "swift://gaia-csv-001.cumulus/GaiaSource_1000172165251650944_1000424567594791808.csv.gz"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o39.csv.
    >   : java.lang.NullPointerException
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1227)
    >   	at org.apache.hadoop.fs.swift.http.SwiftRestClient$AuthenticationPost.extractResult(SwiftRestClient.java:1187)

    #
    # Blurf :-(
    # Same errors, different line numbers.
    #


# -----------------------------------------------------
# OK - forget the swift interface and try the AWS S3 interface.
# -----------------------------------------------------
# Find what AWS jars we have.
#[fedora@stv-dev-master]

    find . -name '*aws*.jar'

    >   ./hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.271.jar
    >   ./hadoop/share/hadoop/tools/lib/hadoop-aws-3.1.3.jar
    >   ./spark/jars/hadoop-aws-2.7.3.jar
    >   ./spark/jars/aws-java-sdk-1.7.4.jar

    We need at least 2.8.0 to use bucket in path URLs rather than bucket in host.

    Latest is 3.2.1
    https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/3.2.1

    Latest AWS SDK is 1.11.708
    https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk/1.11.708


# -----------------------------------------------------
# Download the latest version of Hadoop AWS.
# https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/3.2.1
#[fedora@stv-dev-master]

    mkdir "${HOME}/attic"
    pushd "${HOME}/spark/jars"

        mv -v hadoop-aws-*.jar "${HOME}/attic"
        wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.1/hadoop-aws-3.2.1.jar

    popd


# -----------------------------------------------------
# Check our config properties.
#[fedora@stv-dev-master]

    vi "${HOME}/scripts/zrq/convert-001.py"

        context._jsc.hadoopConfiguration().set(
            "fs.s3a.endpoint", "cumulus.openstack.hpc.cam.ac.uk:6780/swift/v1/"
            )
        context._jsc.hadoopConfiguration().set(
            "fs.s3a.path.style.access", "true"
            )
        context._jsc.hadoopConfiguration().set(
            "fs.s3a.access.key", "93d0....f83c"
            )
        context._jsc.hadoopConfiguration().set(
            "fs.s3a.secret.key", "0e28....25b1"
            )


# -----------------------------------------------------
# Run the job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-001.py" \
         2> convert-001.err \
         | tee convert-001.log

    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-001.py", line 142, in <module>
    >       "s3a://gaia-csv-001/*"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o36.csv.
    >   : java.lang.NoClassDefFoundError: org/apache/hadoop/fs/StreamCapabilities
    >   	at java.lang.ClassLoader.defineClass1(Native Method)

    #
    # Ok, jumping to the latest was a bit of a stretch.
    # Too many opther things changed.
    #

# -----------------------------------------------------
# Download a new enough version of Hadoop AWS.
# https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/2.8.0
#[fedora@stv-dev-master]

    pushd "${HOME}/spark/jars"

        mv -v hadoop-aws-*.jar "${HOME}/attic"
        wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.8.0/hadoop-aws-2.8.0.jar

    popd


# -----------------------------------------------------
# Run the job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-001.py" \
         2> convert-001.err \
         | tee convert-001.log


    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-001.py", line 142, in <module>
    >       "s3a://gaia-csv-001/*"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o36.csv.
    >   : java.lang.NoClassDefFoundError: org/apache/hadoop/fs/StorageStatistics
    >   	at java.lang.Class.forName0(Native Method)
    >   	at java.lang.Class.forName(Class.java:348)


# -----------------------------------------------------
# Sanity check download the old version of Hadoop AWS.
# https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3
#[fedora@stv-dev-master]

    pushd "${HOME}/spark/jars"

        rm -v hadoop-aws-*.jar
        wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar

    popd


# -----------------------------------------------------
# Run the job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-001.py" \
         2> convert-001.err \
         | tee convert-001.log

    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-001.py", line 142, in <module>
    >       "s3a://gaia-csv-001/*"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o36.csv.
    >   : com.amazonaws.AmazonClientException: Unable to execute HTTP request: gaia-csv-001.cumulus.openstack.hpc.cam.ac.uk: Name or service not known
    >   	at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:454)
    >   	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:232)
    >   	at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3528)
    >   	at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1031)

    # Back to the bucket in URL hostname mess.


# -----------------------------------------------------
# Download the latest version of Hadoop AWS.
# https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/3.2.1
#[fedora@stv-dev-master]

    pushd "${HOME}/spark/jars"

        rm -v hadoop-aws-*.jar
        wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.1/hadoop-aws-3.2.1.jar

    popd


# -----------------------------------------------------
# Run the job ....
#[fedora@stv-dev-master]

    spark-submit \
        --master yarn-client \
        --num-executors 6 \
        --executor-cores 4 \
        --executor-memory 12GB \
        "${HOME}/scripts/zrq/convert-001.py" \
         2> convert-001.err \
         | tee convert-001.log

    >   -- [Staring conversion] --
    >   Traceback (most recent call last):
    >     File "/home/fedora/scripts/zrq/convert-001.py", line 142, in <module>
    >       "s3a://gaia-csv-001/*"
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 476, in csv
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
    >     File "/home/fedora/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    >     File "/home/fedora/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    >   py4j.protocol.Py4JJavaError: An error occurred while calling o36.csv.
    >   : java.lang.NoClassDefFoundError: org/apache/hadoop/fs/StreamCapabilities


# -----------------------------------------------------
# Find what Hadoop jars we have.
#[fedora@stv-dev-master]

    find spark -name '*hadoop*.jar'

    >   spark/jars/hadoop-yarn-api-2.7.3.jar
    >   spark/jars/hadoop-auth-2.7.3.jar
    >   spark/jars/hadoop-aws-3.2.1.jar
    >   spark/jars/hadoop-yarn-common-2.7.3.jar
    >   spark/jars/hadoop-annotations-2.7.3.jar
    >   spark/jars/parquet-hadoop-1.10.1.jar
    >   spark/jars/avro-mapred-1.8.2-hadoop2.jar
    >   spark/jars/hadoop-yarn-client-2.7.3.jar
    >   spark/jars/hadoop-client-2.7.3.jar
    >   spark/jars/hadoop-yarn-server-web-proxy-2.7.3.jar
    >   spark/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar
    >   spark/jars/hadoop-mapreduce-client-common-2.7.3.jar
    >   spark/jars/hadoop-mapreduce-client-core-2.7.3.jar
    >   spark/jars/hadoop-hdfs-2.7.3.jar
    >   spark/jars/hadoop-yarn-server-common-2.7.3.jar
    >   spark/jars/hadoop-openstack-3.0.1.jar
    >   spark/jars/hadoop-mapreduce-client-app-2.7.3.jar
    >   spark/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar
    >   spark/jars/hadoop-common-2.7.3.jar
    >   spark/jars/parquet-hadoop-bundle-1.6.0.jar




