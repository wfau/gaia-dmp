#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#

    #
    # Step through the example cluster deploy from StackHPC
    # https://github.com/RSE-Cambridge/iris-magnum/blob/master/magnum-tour/README.md
    #

# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    podman run \
        --rm \
        --tty \
        --interactive \
        --hostname terraformer \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml:z" \
        atolmis/terraform-client \
        bash


# -----------------------------------------------------
# Set the project and cluster names.
#[user@terraformer]

    cloudname=gaia-prod
    clustername=Tiberius


# -----------------------------------------------------
# Clone the StackHPC examples.
#[user@terraformer]

    pushd "${HOME}"

        git clone 'https://github.com/RSE-Cambridge/iris-magnum.git'

    popd


# -----------------------------------------------------
# Fetch our public key from OpenStack.
#[user@terraformer]

    keyname=$(
        openstack \
        --os-cloud "${cloudname:?}" \
            keypair list \
                --format json \
        | jq -r '.[0] | .Name'
        )

    keyfile=/root/.ssh/id_rsa.pub
    mkdir $(dirname ${keyfile})

    openstack \
        --os-cloud "${cloudname:?}" \
            keypair show \
            --public-key \
            "${keyname:?}" \
    | tee "${keyfile:?}"

--START--
ssh-rsa AAAAB3Nz........zV4ksPOL Cambridge HPC OpenStack
--END--

    # The StachHPC module expects this.
    # Difficult to customise the filename.


# -----------------------------------------------------
# Use the unrestricted cloud credentials.
#[user@terraformer]

    # The Terraform module expects OS_CLOUD.
    export OS_CLOUD="${cloudname:?}-super"


# -----------------------------------------------------
# Set the cluster name.
#[user@terraformer]

    # This is hard-coded into the templates
    export clustername=my-test


# -----------------------------------------------------
# Run Terraform to deploy the example cluster.
#[user@terraformer]

    pushd "${HOME}/iris-magnum"
        pushd 'terraform/examples/cluster'

            terraform init

--START--
Initializing modules...
....
Terraform has been successfully initialized!
....
--END--


            terraform plan

--START--
Terraform will perform the following actions:

  # module.cluster.null_resource.kubeconfig will be created
  + resource "null_resource" "kubeconfig" {
    ....
    }

  # module.cluster.openstack_compute_keypair_v2.keypair will be created
  + resource "openstack_compute_keypair_v2" "keypair" {
    ....
    }

  # module.cluster.openstack_containerinfra_cluster_v1.cluster will be created
  + resource "openstack_containerinfra_cluster_v1" "cluster" {
    ....
    }

Plan: 3 to add, 0 to change, 0 to destroy.
--END--


            terraform apply

--START--
....
....
Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only 'yes' will be accepted to approve.

  Enter a value: yes
....
....
Error: Unable to create openstack_compute_keypair_v2 my-test: Expected HTTP response code [200 201] when accessing [POST https://cumulus.openstack.hpc.cam.ac.uk:8774/v2.1/os-keypairs], but got 409 instead
{"conflictingRequest": {"message": "Key pair 'my-test' already exists.", "code": 409}}

  on ../../modules/cluster/main.tf line 9, in resource "openstack_compute_keypair_v2" "keypair":
   9: resource "openstack_compute_keypair_v2" "keypair" {
--END--

    #
    # Terraform state is empty but Openstack already contains a ssh key called 'my-test'.
    # Terraform is so fragile to local state.
    #


# -----------------------------------------------------
# Delete the duplicate ssh key.
#[user@terraformer]

    openstack \
        --os-cloud "${cloudname:?}" \
            keypair delete \
                "${clustername:?}"


    openstack \
        --os-cloud "${cloudname:?}" \
            keypair list

--START--
+------------------+-------------------------------------------------+
| Name             | Fingerprint                                     |
+------------------+-------------------------------------------------+
| zrq-gaia-keypair | a4:8b:f3:0a:31:eb:93:b2:98:62:c5:d2:02:31:0f:b4 |
+------------------+-------------------------------------------------+
--END--


# -----------------------------------------------------
# Run Terraform to deploy the example cluster.
#[user@terraformer]

    pushd "${HOME}/iris-magnum"
        pushd 'terraform/examples/cluster'

            terraform init

--START--
Initializing modules...
....
Terraform has been successfully initialized!
....
--END--


            terraform plan

--START--
....
Terraform will perform the following actions:

  # module.cluster.null_resource.kubeconfig will be created
  + resource "null_resource" "kubeconfig" {
    ....
    }

  # module.cluster.openstack_compute_keypair_v2.keypair will be created
  + resource "openstack_compute_keypair_v2" "keypair" {
    ....
    }

  # module.cluster.openstack_containerinfra_cluster_v1.cluster will be created
  + resource "openstack_containerinfra_cluster_v1" "cluster" {
    ....
    }

Plan: 3 to add, 0 to change, 0 to destroy.
--END--


            terraform apply

--START--
....
....
Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only 'yes' will be accepted to approve.

  Enter a value: yes

module.cluster.openstack_compute_keypair_v2.keypair: Creating...
module.cluster.openstack_compute_keypair_v2.keypair: Creation complete after 1s [id=my-test]
module.cluster.openstack_containerinfra_cluster_v1.cluster: Creating...
module.cluster.openstack_containerinfra_cluster_v1.cluster: Still creating... [10s elapsed]
....
....
module.cluster.openstack_containerinfra_cluster_v1.cluster: Still creating... [1m0s elapsed]

Error: Error waiting for openstack_containerinfra_cluster_v1 ab708936-f205-4953-9ce0-50f9097fbecd to become ready: json: cannot unmarshal object into Go struct field Cluster.health_status_reason of type string

  on ../../modules/cluster/main.tf line 14, in resource "openstack_containerinfra_cluster_v1" "cluster":
  14: resource "openstack_containerinfra_cluster_v1" "cluster" {
--END--

    # This always times out.
    # Ignore and watch the cluster list.

        popd
    popd


# -----------------------------------------------------
# List our clusters.
#[user@terraformer]

    watch \
        openstack \
            --os-cloud "${cloudname:?}" \
            coe cluster list


--START--
+--------------------------------------+---------+---------+------------+--------------+--------------------+---------------+
| uuid                                 | name    | keypair | node_count | master_count | status             | health_status |
+--------------------------------------+---------+---------+------------+--------------+--------------------+---------------+
| ab708936-f205-4953-9ce0-50f9097fbecd | my-test | my-test |          1 |            1 | CREATE_IN_PROGRESS | None          |
+--------------------------------------+---------+---------+------------+--------------+--------------------+---------------+
--END--


--START--
+--------------------------------------+---------+---------+------------+--------------+-----------------+---------------+
| uuid                                 | name    | keypair | node_count | master_count | status          | health_status |
+--------------------------------------+---------+---------+------------+--------------+-----------------+---------------+
| ab708936-f205-4953-9ce0-50f9097fbecd | my-test | my-test |          1 |            1 | CREATE_COMPLETE | HEALTHY       |
+--------------------------------------+---------+---------+------------+--------------+-----------------+---------------+
--END--


# -----------------------------------------------------
# Get the details for our new cluster.
#[user@terraformer]


    clusterid=$(
        openstack \
        --os-cloud "${cloudname:?}" \
            coe cluster list \
                --format json \
        | jq -r '.[] | select(.name == "'${clustername:?}'") | .uuid'
        )

    openstack \
        --os-cloud "${cloudname:?}" \
        coe cluster show \
            "${clusterid}" \
                --format json \
    | jq '.'


--START--
{
  "status": "CREATE_COMPLETE",
  "health_status": "HEALTHY",
  "cluster_template_id": "d54167d9-495f-437e-88fe-d182b2a230ea",
  "node_addresses": [
    "10.0.0.83"
  ],
  "uuid": "ab708936-f205-4953-9ce0-50f9097fbecd",
  "stack_id": "33553b5b-7221-4898-8a3f-92c0c61da418",
  "status_reason": null,
  "created_at": "2020-06-23T11:09:38+00:00",
  "updated_at": "2020-06-23T11:15:40+00:00",
  "coe_version": "v1.15.9",
  "labels": {
    "auto_healing_controller": "magnum-auto-healer",
    "max_node_count": "2",
    "cloud_provider_tag": "v1.15.0",
    "etcd_tag": "3.3.17",
    "monitoring_enabled": "true",
    "tiller_enabled": "true",
    "autoscaler_tag": "v1.15.2",
    "master_lb_floating_ip_enabled": "true",
    "min_node_count": "1",
    "tiller_tag": "v2.16.1",
    "use_podman": "true",
    "auto_healing_enabled": "true",
    "heat_container_agent_tag": "train-stable-1",
    "kube_tag": "v1.15.9",
    "auto_scaling_enabled": "true"
  },
  "faults": "",
  "keypair": "my-test",
  "api_address": "https://128.232.227.218:6443",
  "master_addresses": [
    "10.0.0.95"
  ],
  "create_timeout": null,
  "node_count": 1,
  "discovery_url": "https://discovery.etcd.io/b0a06da82b49d3a4cda9233a01cedc6f",
  "master_count": 1,
  "container_version": "1.12.6",
  "name": "my-test",
  "master_flavor_id": "general.v1.tiny",
  "flavor_id": "general.v1.tiny",
  "health_status_reason": {
    "my-test-lmck76anbakg-master-0.Ready": "True",
    "my-test-lmck76anbakg-node-0.Ready": "True",
    "api": "ok"
  },
  "project_id": "21b4ae3a2ea44bc5a9c14005ed2963af"
}
--END--


# -----------------------------------------------------
# Get the kubectl config for our cluster.
#[user@terraformer]

    pushd "${HOME}"

        openstack \
            --os-cloud "${cloudname:?}" \
            coe cluster config \
                "${clusterid}"

    popd

    cat "${HOME}/config"


--START--
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1C........RS0tLS0t
    server: https://128.232.227.218:6443
  name: my-test
contexts:
- context:
    cluster: my-test
    user: admin
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: admin
  user:
    client-certificate-data: LS0tLS1C........FLS0tLS0=
    client-key-data: LS0tLS1C........tLS0tLQo=
--END--


# -----------------------------------------------------
# Use kubectl to get details of our cluster.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        cluster-info

--START--
Kubernetes master is running at https://128.232.227.218:6443
Heapster is running at https://128.232.227.218:6443/api/v1/namespaces/kube-system/services/heapster/proxy
CoreDNS is running at https://128.232.227.218:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
--END--


# -----------------------------------------------------
# Create a 'hello world' deployment.
#[user@terraformer]

    helloname=hello-node
    helloname=AlberiusCant

    kubectl \
        --kubeconfig "${HOME}/config" \
        create deployment "${helloname:?}" \
            --image=gcr.io/hello-minikube-zero-install/hello-node

--START--
The Deployment "AlberiusCant" is invalid: metadata.name:
    Invalid value: "AlberiusCant": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.',
    and must start and end with an alphanumeric character (e.g. 'example.com',
    regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')
--END--

    #
    # Wow - OK, back to the 1980's :-(
    #

    helloname=alberius-cant

    kubectl \
        --kubeconfig "${HOME}/config" \
        create deployment \
            "${helloname:?}" \
            --image=gcr.io/hello-minikube-zero-install/hello-node

--START--
deployment.apps/alberius-cant created
--END--


# -----------------------------------------------------
# Expose our a 'hello world' deployment with a load balancer.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        expose deployment \
            "${helloname:?}" \
            --type=LoadBalancer \
            --port=8080


--START--
service/alberius-cant exposed
--END--


# -----------------------------------------------------
# Check how far it has reached.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        get all

--START--
NAME                                 READY   STATUS             RESTARTS   AGE
pod/alberius-cant-6d68d7fbbf-sf2b2   0/1     ImagePullBackOff   0          2m33s


NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
service/alberius-cant   LoadBalancer   10.254.177.129   <pending>     8080:32497/TCP   53s
service/kubernetes      ClusterIP      10.254.0.1       <none>        443/TCP          35m


NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/alberius-cant   0/1     1            0           2m33s

NAME                                       DESIRED   CURRENT   READY   AGE
replicaset.apps/alberius-cant-6d68d7fbbf   1         1         0       2m33s
--END--


--START--
NAME                                 READY   STATUS             RESTARTS   AGE
pod/alberius-cant-6d68d7fbbf-sf2b2   0/1     ImagePullBackOff   0          3m48s


NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)          AGE
service/alberius-cant   LoadBalancer   10.254.177.129   128.232.227.214   8080:32497/TCP   2m8s
service/kubernetes      ClusterIP      10.254.0.1       <none>            443/TCP          36m
....
....
--END--



--START--
NAME                                 READY   STATUS             RESTARTS   AGE
pod/alberius-cant-6d68d7fbbf-sf2b2   0/1     ImagePullBackOff   0          5m12s
....
....
--END--

    #
    # Stuck at ImagePullBackOff for 5m12s sounds like a problem.
    # https://managedkube.com/kubernetes/k8sbot/troubleshooting/imagepullbackoff/2019/02/23/imagepullbackoff.html

# -----------------------------------------------------
# Get the details about the stalled pod.
#[user@terraformer]

    podname=alberius-cant-6d68d7fbbf-sf2b2

    kubectl \
        --kubeconfig "${HOME}/config" \
        describe pod \
            "${podname:?}"

--START--
Name:           alberius-cant-6d68d7fbbf-sf2b2
Namespace:      default
Node:           my-test-lmck76anbakg-node-0/10.0.0.83
Start Time:     Tue, 23 Jun 2020 11:45:55 +0000
Labels:         app=alberius-cant
                pod-template-hash=6d68d7fbbf
Annotations:    <none>
Status:         Pending
IP:             10.100.1.48
Controlled By:  ReplicaSet/alberius-cant-6d68d7fbbf
Containers:
  hello-node:
    Container ID:
    Image:          gcr.io/hello-minikube-zero-install/hello-node
    Image ID:
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       ImagePullBackOff
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-d759j (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  default-token-d759j:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-d759j
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason          Age                  From                                  Message
  ----     ------          ----                 ----                                  -------
  Normal   Scheduled       30m                  default-scheduler                     Successfully assigned default/alberius-cant-6d68d7fbbf-sf2b2 to my-test-lmck76anbakg-node-0
  Normal   Pulling         30m (x2 over 30m)    kubelet, my-test-lmck76anbakg-node-0  Pulling image "gcr.io/hello-minikube-zero-install/hello-node"
  Warning  Failed          30m (x2 over 30m)    kubelet, my-test-lmck76anbakg-node-0  Failed to pull image "gcr.io/hello-minikube-zero-install/hello-node": rpc error: code = Unknown desc = unable to retrieve auth token: 401 unauthorized
  Warning  Failed          30m (x2 over 30m)    kubelet, my-test-lmck76anbakg-node-0  Error: ErrImagePull
  Normal   SandboxChanged  30m (x6 over 30m)    kubelet, my-test-lmck76anbakg-node-0  Pod sandbox changed, it will be killed and re-created.
  Normal   BackOff         10m (x92 over 30m)   kubelet, my-test-lmck76anbakg-node-0  Back-off pulling image "gcr.io/hello-minikube-zero-install/hello-node"
  Warning  Failed          14s (x136 over 30m)  kubelet, my-test-lmck76anbakg-node-0  Error: ImagePullBackOff
--END--

    #
    # This looks like the cause.
    #


--START--
....
  Normal   Pulling         30m (x2 over 30m)    kubelet, my-test-lmck76anbakg-node-0  Pulling image "gcr.io/hello-minikube-zero-install/hello-node"
  Warning  Failed          30m (x2 over 30m)    kubelet, my-test-lmck76anbakg-node-0  Failed to pull image "gcr.io/hello-minikube-zero-install/hello-node": rpc error: code = Unknown desc = unable to retrieve auth token: 401 unauthorized
....
--END--

    #
    # Failed to pull image "gcr.io/hello-minikube-zero-install/hello-node":
    # rpc error:
    #   code = Unknown
    #   desc = unable to retrieve auth token: 401 unauthorized
    #


    #
    # We have an out of date image name.
    # Looks like the original 'hello-minikube-zero-install' has been removed.
    # https://github.com/kubernetes/website/pull/20598/files
    #

    -   kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node
    +   kubectl create deployment hello-node --image=k8s.gcr.io/echoserver:1.4


# -----------------------------------------------------
# Try deleting the deployment.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        delete deployment \
            "${helloname:?}"

--START--
deployment.extensions "alberius-cant" deleted
--END--


# -----------------------------------------------------
# Check what we have left.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        get all

--START--
NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)          AGE
service/alberius-cant   LoadBalancer   10.254.177.129   128.232.227.214   8080:32497/TCP   38m
service/kubernetes      ClusterIP      10.254.0.1       <none>            443/TCP          72m
--END--


# -----------------------------------------------------
# Delete our load balancer.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        delete service \
            "${helloname:?}"

--START--
service "alberius-cant" deleted
--END--


# -----------------------------------------------------
# Check what we have left.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        get all

--START--
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.254.0.1   <none>        443/TCP   75m
--END--


# -----------------------------------------------------
# Try create the deployment again.
#[user@terraformer]

    helloname=alberius-cant
    imagename=k8s.gcr.io/echoserver

    kubectl \
        --kubeconfig "${HOME}/config" \
        create deployment \
            "${helloname:?}" \
            --image=${imagename:?}

--START--
deployment.apps/alberius-cant created
--END--


# -----------------------------------------------------
# Check what happened.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        get all


--START--
NAME                                 READY   STATUS             RESTARTS   AGE
pod/alberius-cant-655df5874b-n482v   0/1     ImagePullBackOff   0          17s


NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.254.0.1   <none>        443/TCP   77m


NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/alberius-cant   0/1     1            0           17s

NAME                                       DESIRED   CURRENT   READY   AGE
replicaset.apps/alberius-cant-655df5874b   1         1         0       18s
--END--


    #
    # OK, a different error ..
    #


--START--
pod/alberius-cant-655df5874b-n482v   0/1     ErrImagePull   0          91s
--END--


# -----------------------------------------------------
# Get the details about the failed pod.
#[user@terraformer]

    podname=alberius-cant-655df5874b-n482v

    kubectl \
        --kubeconfig "${HOME}/config" \
        describe pod \
            "${podname:?}"


--START--
Name:           alberius-cant-655df5874b-n482v
Namespace:      default
Node:           my-test-lmck76anbakg-node-0/10.0.0.83
Start Time:     Tue, 23 Jun 2020 12:30:32 +0000
Labels:         app=alberius-cant
                pod-template-hash=655df5874b
Annotations:    <none>
Status:         Pending
IP:             10.100.1.65
Controlled By:  ReplicaSet/alberius-cant-655df5874b
Containers:
  echoserver:
    Container ID:
    Image:          k8s.gcr.io/echoserver
    Image ID:
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       ImagePullBackOff
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-d759j (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  default-token-d759j:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-d759j
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason          Age                    From                                  Message
  ----     ------          ----                   ----                                  -------
  Normal   Scheduled       2m43s                  default-scheduler                     Successfully assigned default/alberius-cant-655df5874b-n482v to my-test-lmck76anbakg-node-0
  Normal   Pulling         2m29s (x2 over 2m41s)  kubelet, my-test-lmck76anbakg-node-0  Pulling image "k8s.gcr.io/echoserver"
  Warning  Failed          2m28s (x2 over 2m40s)  kubelet, my-test-lmck76anbakg-node-0  Failed to pull image "k8s.gcr.io/echoserver": rpc error: code = Unknown desc = manifest for k8s.gcr.io/echoserver:latest not found
  Warning  Failed          2m28s (x2 over 2m40s)  kubelet, my-test-lmck76anbakg-node-0  Error: ErrImagePull
  Normal   SandboxChanged  2m26s (x7 over 2m40s)  kubelet, my-test-lmck76anbakg-node-0  Pod sandbox changed, it will be killed and re-created.
  Normal   BackOff         2m24s (x6 over 2m39s)  kubelet, my-test-lmck76anbakg-node-0  Back-off pulling image "k8s.gcr.io/echoserver"
  Warning  Failed          2m24s (x6 over 2m39s)  kubelet, my-test-lmck76anbakg-node-0  Error: ImagePullBackOff
--END--

    #
    # This looks like the cause.
    #

--START--
....
  Normal   Pulling         2m29s (x2 over 2m41s)  kubelet, my-test-lmck76anbakg-node-0  Pulling image "k8s.gcr.io/echoserver"
  Warning  Failed          2m28s (x2 over 2m40s)  kubelet, my-test-lmck76anbakg-node-0  Failed to pull image "k8s.gcr.io/echoserver": rpc error: code = Unknown desc = manifest for k8s.gcr.io/echoserver:latest not found
....
--END--

    #
    # Need to specify the image version.
    #

# -----------------------------------------------------
# Delete the failed deployment.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        delete deployment \
            "${helloname:?}"

--START--
deployment.extensions "alberius-cant" deleted
--END--


# -----------------------------------------------------
# Try create the deployment again.
#[user@terraformer]

    helloname=alberius-cant
    imagename=k8s.gcr.io/echoserver:1.4

    kubectl \
        --kubeconfig "${HOME}/config" \
        create deployment \
            "${helloname:?}" \
            --image=${imagename:?}

--START--
deployment.apps/alberius-cant created
--END--


# -----------------------------------------------------
# Check what happened.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        get all

--START--
NAME                                 READY   STATUS    RESTARTS   AGE
pod/alberius-cant-6f64989475-nkszd   1/1     Running   0          17s


NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.254.0.1   <none>        443/TCP   82m


NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/alberius-cant   1/1     1            1           17s

NAME                                       DESIRED   CURRENT   READY   AGE
replicaset.apps/alberius-cant-6f64989475   1         1         1       17s
--END--

    #
    # Better :-)
    #


# -----------------------------------------------------
# Expose our a 'hello world' deployment with a load balancer.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        expose deployment \
            "${helloname:?}" \
            --type=LoadBalancer \
            --port=8080


--START--
service/alberius-cant exposed
--END--


# -----------------------------------------------------
# Check what happened.
#[user@terraformer]

    kubectl \
        --kubeconfig "${HOME}/config" \
        get all

--START--
NAME                                 READY   STATUS    RESTARTS   AGE
pod/alberius-cant-6f64989475-nkszd   1/1     Running   0          94s

NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
service/alberius-cant   LoadBalancer   10.254.237.125   <pending>     8080:32701/TCP   25s
service/kubernetes      ClusterIP      10.254.0.1       <none>        443/TCP          84m

NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/alberius-cant   1/1     1            1           94s

NAME                                       DESIRED   CURRENT   READY   AGE
replicaset.apps/alberius-cant-6f64989475   1         1         1       94s
--END--


--START--
NAME                                 READY   STATUS    RESTARTS   AGE
pod/alberius-cant-6f64989475-nkszd   1/1     Running   0          2m31s

NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)          AGE
service/alberius-cant   LoadBalancer   10.254.237.125   128.232.227.137   8080:32701/TCP   82s
service/kubernetes      ClusterIP      10.254.0.1       <none>            443/TCP          84m
....
--END--

    #
    # Takes ~2min to create the external IP address.
    #


# -----------------------------------------------------
# Machine readable format.
# https://kubernetes.io/docs/reference/kubectl/cheatsheet/#formatting-output
#[user@terraformer]

    kubectl \
        --output json \
        --kubeconfig "${HOME}/config" \
        get all \
    | jq '.'


--START--
{
  "apiVersion": "v1",
  "items": [
    {
      "apiVersion": "v1",
      "kind": "Pod",
      "metadata": {
        "creationTimestamp": "2020-06-23T12:35:43Z",
        "generateName": "alberius-cant-6f64989475-",
        "labels": {
          "app": "alberius-cant",
          "pod-template-hash": "6f64989475"
        },
        "name": "alberius-cant-6f64989475-nkszd",
        "namespace": "default",
        "ownerReferences": [
          {
            "apiVersion": "apps/v1",
            "blockOwnerDeletion": true,
            "controller": true,
            "kind": "ReplicaSet",
            "name": "alberius-cant-6f64989475",
            "uid": "6841f4be-7bcc-4dc6-ad28-68db3d705c80"
          }
        ],
        "resourceVersion": "14646",
        "selfLink": "/api/v1/namespaces/default/pods/alberius-cant-6f64989475-nkszd",
        "uid": "4593c425-e0ed-466d-92ae-7184bec43e52"
      },
      "spec": {
        "containers": [
          {
            "image": "k8s.gcr.io/echoserver:1.4",
            "imagePullPolicy": "IfNotPresent",
            "name": "echoserver",
            "resources": {},
            "terminationMessagePath": "/dev/termination-log",
            "terminationMessagePolicy": "File",
            "volumeMounts": [
              {
                "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                "name": "default-token-d759j",
                "readOnly": true
              }
            ]
          }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "nodeName": "my-test-lmck76anbakg-node-0",
        "restartPolicy": "Always",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
          {
            "effect": "NoExecute",
            "key": "node.kubernetes.io/not-ready",
            "operator": "Exists",
            "tolerationSeconds": 300
          },
          {
            "effect": "NoExecute",
            "key": "node.kubernetes.io/unreachable",
            "operator": "Exists",
            "tolerationSeconds": 300
          }
        ],
        "volumes": [
          {
            "name": "default-token-d759j",
            "secret": {
              "defaultMode": 420,
              "secretName": "default-token-d759j"
            }
          }
        ]
      },
      "status": {
        "conditions": [
          {
            "lastProbeTime": null,
            "lastTransitionTime": "2020-06-23T12:35:43Z",
            "status": "True",
            "type": "Initialized"
          },
          {
            "lastProbeTime": null,
            "lastTransitionTime": "2020-06-23T12:35:50Z",
            "status": "True",
            "type": "Ready"
          },
          {
            "lastProbeTime": null,
            "lastTransitionTime": "2020-06-23T12:35:50Z",
            "status": "True",
            "type": "ContainersReady"
          },
          {
            "lastProbeTime": null,
            "lastTransitionTime": "2020-06-23T12:35:43Z",
            "status": "True",
            "type": "PodScheduled"
          }
        ],
        "containerStatuses": [
          {
            "containerID": "docker://bc0f88a7880d0f6b3191475c56def26cb29fc9177ed61b13ce1d030d236b2165",
            "image": "k8s.gcr.io/echoserver:1.4",
            "imageID": "docker-pullable://k8s.gcr.io/echoserver@sha256:5d99aa1120524c801bc8c1a7077e8f5ec122ba16b6dda1a5d3826057f67b9bcb",
            "lastState": {},
            "name": "echoserver",
            "ready": true,
            "restartCount": 0,
            "state": {
              "running": {
                "startedAt": "2020-06-23T12:35:50Z"
              }
            }
          }
        ],
        "hostIP": "10.0.0.83",
        "phase": "Running",
        "podIP": "10.100.1.66",
        "qosClass": "BestEffort",
        "startTime": "2020-06-23T12:35:43Z"
      }
    },
    {
      "apiVersion": "v1",
      "kind": "Service",
      "metadata": {
        "creationTimestamp": "2020-06-23T12:36:52Z",
        "labels": {
          "app": "alberius-cant"
        },
        "name": "alberius-cant",
        "namespace": "default",
        "resourceVersion": "15007",
        "selfLink": "/api/v1/namespaces/default/services/alberius-cant",
        "uid": "7524a846-f01a-431b-8473-37b23c2ed514"
      },
      "spec": {
        "clusterIP": "10.254.237.125",
        "externalTrafficPolicy": "Cluster",
        "ports": [
          {
            "nodePort": 32701,
            "port": 8080,
            "protocol": "TCP",
            "targetPort": 8080
          }
        ],
        "selector": {
          "app": "alberius-cant"
        },
        "sessionAffinity": "None",
        "type": "LoadBalancer"
      },
      "status": {
        "loadBalancer": {
          "ingress": [
            {
              "ip": "128.232.227.137"
            }
          ]
        }
      }
    },
    {
      "apiVersion": "v1",
      "kind": "Service",
      "metadata": {
        "creationTimestamp": "2020-06-23T11:13:15Z",
        "labels": {
          "component": "apiserver",
          "provider": "kubernetes"
        },
        "name": "kubernetes",
        "namespace": "default",
        "resourceVersion": "160",
        "selfLink": "/api/v1/namespaces/default/services/kubernetes",
        "uid": "d7f038ec-6b6f-49e0-a497-4eab4bcc6afd"
      },
      "spec": {
        "clusterIP": "10.254.0.1",
        "ports": [
          {
            "name": "https",
            "port": 443,
            "protocol": "TCP",
            "targetPort": 6443
          }
        ],
        "sessionAffinity": "None",
        "type": "ClusterIP"
      },
      "status": {
        "loadBalancer": {}
      }
    },
    {
      "apiVersion": "apps/v1",
      "kind": "Deployment",
      "metadata": {
        "annotations": {
          "deployment.kubernetes.io/revision": "1"
        },
        "creationTimestamp": "2020-06-23T12:35:43Z",
        "generation": 1,
        "labels": {
          "app": "alberius-cant"
        },
        "name": "alberius-cant",
        "namespace": "default",
        "resourceVersion": "14648",
        "selfLink": "/apis/apps/v1/namespaces/default/deployments/alberius-cant",
        "uid": "4a101ac4-8a3c-4f4b-bfb1-ed4955ff95ac"
      },
      "spec": {
        "progressDeadlineSeconds": 600,
        "replicas": 1,
        "revisionHistoryLimit": 10,
        "selector": {
          "matchLabels": {
            "app": "alberius-cant"
          }
        },
        "strategy": {
          "rollingUpdate": {
            "maxSurge": "25%",
            "maxUnavailable": "25%"
          },
          "type": "RollingUpdate"
        },
        "template": {
          "metadata": {
            "creationTimestamp": null,
            "labels": {
              "app": "alberius-cant"
            }
          },
          "spec": {
            "containers": [
              {
                "image": "k8s.gcr.io/echoserver:1.4",
                "imagePullPolicy": "IfNotPresent",
                "name": "echoserver",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File"
              }
            ],
            "dnsPolicy": "ClusterFirst",
            "restartPolicy": "Always",
            "schedulerName": "default-scheduler",
            "securityContext": {},
            "terminationGracePeriodSeconds": 30
          }
        }
      },
      "status": {
        "availableReplicas": 1,
        "conditions": [
          {
            "lastTransitionTime": "2020-06-23T12:35:50Z",
            "lastUpdateTime": "2020-06-23T12:35:50Z",
            "message": "Deployment has minimum availability.",
            "reason": "MinimumReplicasAvailable",
            "status": "True",
            "type": "Available"
          },
          {
            "lastTransitionTime": "2020-06-23T12:35:43Z",
            "lastUpdateTime": "2020-06-23T12:35:50Z",
            "message": "ReplicaSet \"alberius-cant-6f64989475\" has successfully progressed.",
            "reason": "NewReplicaSetAvailable",
            "status": "True",
            "type": "Progressing"
          }
        ],
        "observedGeneration": 1,
        "readyReplicas": 1,
        "replicas": 1,
        "updatedReplicas": 1
      }
    },
    {
      "apiVersion": "apps/v1",
      "kind": "ReplicaSet",
      "metadata": {
        "annotations": {
          "deployment.kubernetes.io/desired-replicas": "1",
          "deployment.kubernetes.io/max-replicas": "2",
          "deployment.kubernetes.io/revision": "1"
        },
        "creationTimestamp": "2020-06-23T12:35:43Z",
        "generation": 1,
        "labels": {
          "app": "alberius-cant",
          "pod-template-hash": "6f64989475"
        },
        "name": "alberius-cant-6f64989475",
        "namespace": "default",
        "ownerReferences": [
          {
            "apiVersion": "apps/v1",
            "blockOwnerDeletion": true,
            "controller": true,
            "kind": "Deployment",
            "name": "alberius-cant",
            "uid": "4a101ac4-8a3c-4f4b-bfb1-ed4955ff95ac"
          }
        ],
        "resourceVersion": "14647",
        "selfLink": "/apis/apps/v1/namespaces/default/replicasets/alberius-cant-6f64989475",
        "uid": "6841f4be-7bcc-4dc6-ad28-68db3d705c80"
      },
      "spec": {
        "replicas": 1,
        "selector": {
          "matchLabels": {
            "app": "alberius-cant",
            "pod-template-hash": "6f64989475"
          }
        },
        "template": {
          "metadata": {
            "creationTimestamp": null,
            "labels": {
              "app": "alberius-cant",
              "pod-template-hash": "6f64989475"
            }
          },
          "spec": {
            "containers": [
              {
                "image": "k8s.gcr.io/echoserver:1.4",
                "imagePullPolicy": "IfNotPresent",
                "name": "echoserver",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File"
              }
            ],
            "dnsPolicy": "ClusterFirst",
            "restartPolicy": "Always",
            "schedulerName": "default-scheduler",
            "securityContext": {},
            "terminationGracePeriodSeconds": 30
          }
        }
      },
      "status": {
        "availableReplicas": 1,
        "fullyLabeledReplicas": 1,
        "observedGeneration": 1,
        "readyReplicas": 1,
        "replicas": 1
      }
    }
  ],
  "kind": "List",
  "metadata": {
    "resourceVersion": "",
    "selfLink": ""
  }
}
--END--


# -----------------------------------------------------
# Extract the public endpoint.
#[user@terraformer]

    kubectl \
        --output json \
        --kubeconfig "${HOME}/config" \
        get all \
    | jq '.'


# -----------------------------------------------------
# Try the public endpoint.
#[user@terraformer]

    kubectl \
        --output json \
        --kubeconfig "${HOME}/config" \
        get all \
    | jq -r '
        .items[]
        | select(.kind == "Service")
        | select(.metadata.name == "'${helloname:?}'")
        | .status.loadBalancer.ingress[0].ip
        '
--START--
128.232.227.137
--END--

    publicip=$(
        kubectl \
            --output json \
            --kubeconfig "${HOME}/config" \
            get all \
        | jq -r '
            .items[]
            | select(.kind == "Service")
            | select(.metadata.name == "'${helloname:?}'")
            | .status.loadBalancer.ingress[0].ip
            '
        )


    curl "http://${publicip}:8080/"

--START--
CLIENT VALUES:
client_address=10.100.1.1
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://128.232.227.137:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=128.232.227.137:8080
user-agent=curl/7.69.1
BODY:
-no body in request-
--END--


# -----------------------------------------------------
# List the Openstack load balancers.
#[user@terraformer]

    openstack \
    --os-cloud "${cloudname:?}" \
        loadbalancer list

--START--
+--------------------------------------+-------------------------------------------------------------------------+----------------------------------+-------------+---------------------+----------+
| id                                   | name                                                                    | project_id                       | vip_address | provisioning_status | provider |
+--------------------------------------+-------------------------------------------------------------------------+----------------------------------+-------------+---------------------+----------+
| 24c37a2c-85ae-46ae-9230-c65d8b323e3c | my-test-lmck76anbakg-api_lb-svzaiuvai6hu-loadbalancer-x7kmmbrh3zhd      | 21b4ae3a2ea44bc5a9c14005ed2963af | 10.0.0.37   | ACTIVE              | amphora  |
| b9369bff-c48d-42b9-bf7a-2e913aeaa02b | my-test-lmck76anbakg-etcd_lb-f5rgsjse3vy2-loadbalancer-pmhynebpnfll     | 21b4ae3a2ea44bc5a9c14005ed2963af | 10.0.0.9    | ACTIVE              | amphora  |
| 01275cba-6ab5-4dcf-a210-ed594ee900bc | kube_service_ab708936-f205-4953-9ce0-50f9097fbecd_default_alberius-cant | 21b4ae3a2ea44bc5a9c14005ed2963af | 10.0.0.58   | ACTIVE              | amphora  |
+--------------------------------------+-------------------------------------------------------------------------+----------------------------------+-------------+---------------------+----------+
--END--








    #
    # Move this to terraform-spark when we get there ..
    #

# -----------------------------------------------------
# Install Helm.
# TODO Add this to the Docker image ?
# https://helm.sh/docs/intro/install/
# https://github.com/helm/helm/releases/tag/v3.2.4
#[user@terraformer]

    tarfile=helm-v3.2.4-linux-amd64.tar.gz

    pushd $(mktemp -d)

        wget "https://get.helm.sh/${tarfile:?}"
        tar -zxvf "${tarfile:?}"

        mv linux-amd64/helm /usr/local/bin/helm

    popd






