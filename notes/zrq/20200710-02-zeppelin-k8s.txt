#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#

    #
    # Integrated Zeppelin and Spark deployment.
    #

# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --hostname kubernator \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml:z" \
        --volume "${AGLAIS_CODE}/experiments/zrq/kubernetes:/kubernetes:z" \
        atolmis/openstack-client \
        bash


# -----------------------------------------------------
# Set the cloud, credentials and cluster names.
#[user@kubernator]

    cloudname=gaia-prod
    clustername=Tiberius


# -----------------------------------------------------
# Get our cluster details.
#[user@kubernator]

    clusterid=$(
        openstack \
        --os-cloud "${cloudname:?}" \
            coe cluster list \
                --format json \
        | jq -r '.[] | select(.name == "'${clustername:?}'") | .uuid'
        )

    openstack \
        --os-cloud "${cloudname:?}" \
        coe cluster show \
            "${clusterid}"

    >   +----------------------+------------------------------------------------------------------------------------------------------+
    >   | Field                | Value                                                                                                |
    >   +----------------------+------------------------------------------------------------------------------------------------------+
    >   | status               | CREATE_COMPLETE                                                                                      |
    >   | health_status        | HEALTHY                                                                                              |
    >   | cluster_template_id  | d54167d9-495f-437e-88fe-d182b2a230ea                                                                 |
    >   | node_addresses       | ['10.0.0.240', '10.0.0.231', '10.0.0.36', '10.0.0.56']                                               |
    >   | uuid                 | 6a70 .... 9925                                                                                       |
    >   | stack_id             | 82131591-75a6-46be-9f28-a6aa9689f949                                                                 |
    >   | status_reason        | None                                                                                                 |
    >   | created_at           | 2020-07-09T17:33:03+00:00                                                                            |
    >   | updated_at           | 2020-07-09T17:40:07+00:00                                                                            |
    >   | coe_version          | v1.15.9                                                                                              |
    >   | labels               | {'auto_healing_controller': 'magnum-auto-healer' .... 'auto_scaling_enabled': 'true'}                |
    >   | labels_overridden    |                                                                                                      |
    >   | labels_skipped       |                                                                                                      |
    >   | labels_added         |                                                                                                      |
    >   | faults               |                                                                                                      |
    >   | keypair              | Tiberius-keypair                                                                                     |
    >   | api_address          | https://128.232.227.194:6443                                                                         |
    >   | master_addresses     | ['10.0.0.27']                                                                                        |
    >   | create_timeout       | None                                                                                                 |
    >   | node_count           | 4                                                                                                    |
    >   | discovery_url        | https://discovery.etcd.io/fe96 .... 9c08                                                             |
    >   | master_count         | 1                                                                                                    |
    >   | container_version    | 1.12.6                                                                                               |
    >   | name                 | Tiberius                                                                                             |
    >   | master_flavor_id     | general.v1.tiny                                                                                      |
    >   | flavor_id            | general.v1.tiny                                                                                      |
    >   | health_status_reason | {'tiberius-otskjpsfprmh-node-0.Ready': 'True', .... 'tiberius-otskjpsfprmh-node-2.Ready': 'True'}    |
    >   | project_id           | 21b4 .... 63af                                                                                       |
    >   +----------------------+------------------------------------------------------------------------------------------------------+


    openstack \
        --os-cloud "${cloudname:?}" \
        coe cluster show \
            "${clusterid}" \
                --format json \
    | jq '.'


    >   {
    >     "status": "CREATE_COMPLETE",
    >     "health_status": "HEALTHY",
    >     "cluster_template_id": "d541 .... 30ea",
    >     "node_addresses": [
    >       "10.0.0.240",
    >       "10.0.0.231",
    >       "10.0.0.36",
    >       "10.0.0.56"
    >     ],
    >     "uuid": "6a70 .... 9925",
    >     "stack_id": "8213 .... f949",
    >     "status_reason": null,
    >     "created_at": "2020-07-09T17:33:03+00:00",
    >     "updated_at": "2020-07-09T17:40:07+00:00",
    >     "coe_version": "v1.15.9",
    >     "labels": {
    >       "auto_healing_controller": "magnum-auto-healer",
    >       "max_node_count": "10",
    >       "cloud_provider_tag": "v1.15.0",
    >       "etcd_tag": "3.3.17",
    >       "monitoring_enabled": "true",
    >       "tiller_enabled": "true",
    >       "autoscaler_tag": "v1.15.2",
    >       "master_lb_floating_ip_enabled": "true",
    >       "min_node_count": "4",
    >       "tiller_tag": "v2.16.1",
    >       "use_podman": "true",
    >       "auto_healing_enabled": "true",
    >       "heat_container_agent_tag": "train-stable-1",
    >       "kube_tag": "v1.15.9",
    >       "auto_scaling_enabled": "true"
    >     },
    >     "labels_overridden": "",
    >     "labels_skipped": "",
    >     "labels_added": "",
    >     "faults": "",
    >     "keypair": "Tiberius-keypair",
    >     "api_address": "https://128.232.227.194:6443",
    >     "master_addresses": [
    >       "10.0.0.27"
    >     ],
    >     "create_timeout": null,
    >     "node_count": 4,
    >     "discovery_url": "https://discovery.etcd.io/fe96 .... 9c08",
    >     "master_count": 1,
    >     "container_version": "1.12.6",
    >     "name": "Tiberius",
    >     "master_flavor_id": "general.v1.tiny",
    >     "flavor_id": "general.v1.tiny",
    >     "health_status_reason": {
    >       "tiberius-otskjpsfprmh-node-0.Ready": "True",
    >       "tiberius-otskjpsfprmh-master-0.Ready": "True",
    >       "tiberius-otskjpsfprmh-node-3.Ready": "True",
    >       "api": "ok",
    >       "tiberius-otskjpsfprmh-node-1.Ready": "True",
    >       "tiberius-otskjpsfprmh-node-2.Ready": "True"
    >     },
    >     "project_id": "21b4 .... 63af"
    >   }


# -----------------------------------------------------
# Get the connection details for our cluster.
#[user@kubernator]

    mkdir -p "${HOME}/.kube/${clustername:?}"
    openstack \
        --os-cloud "${cloudname:?}-super" \
        coe cluster config \
            "${clustername:?}" \
                --force \
                --dir "${HOME}/.kube/${clustername:?}"

    >   'SHELL'


# -----------------------------------------------------
# Check kubectl can get the connection details for our cluster.
#[user@kubernator]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        cluster-info

    >   Kubernetes master is running at https://128.232.227.194:6443
    >   Heapster is running at https://128.232.227.194:6443/api/v1/namespaces/kube-system/services/heapster/proxy
    >   CoreDNS is running at https://128.232.227.194:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy


# -----------------------------------------------------
# Download the Zeppelin deployment template.
#[user@kubernator]

    mkdir "${HOME}/zeppelin"
    pushd "${HOME}/zeppelin"

        curl -s -O https://raw.githubusercontent.com/apache/zeppelin/master/k8s/zeppelin-server.yaml

    popd


# -----------------------------------------------------
# Deploy Zeppelin.
#[user@kubernator]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        apply \
            --filename "${HOME}/zeppelin/zeppelin-server.yaml"

    >   configmap/zeppelin-server-conf-map created
    >   configmap/zeppelin-server-conf created
    >   deployment.apps/zeppelin-server created
    >   service/zeppelin-server created
    >   serviceaccount/zeppelin-server created
    >   role.rbac.authorization.k8s.io/zeppelin-server-role created
    >   rolebinding.rbac.authorization.k8s.io/zeppelin-server-role-binding created

    # Error message in the K8s dashboard.

    >   Failed to pull image "apache/zeppelin:0.9.0-SNAPSHOT": rpc error: code = Unknown desc = manifest for docker.io/apache/zeppelin:0.9.0-SNAPSHOT not found


# -----------------------------------------------------
# Fix the Zeppelin image version.
#[user@kubernator]

    vi "${HOME}/zeppelin/zeppelin-server.yaml"

        ZEPPELIN_K8S_SPARK_CONTAINER_IMAGE: spark:2.4.5
    -   ZEPPELIN_K8S_CONTAINER_IMAGE: apache/zeppelin:0.9.0-SNAPSHOT
    +   ZEPPELIN_K8S_CONTAINER_IMAGE: apache/zeppelin:0.9.0

# -----------------------------------------------------
# Delete the Zeppelin Deployment.
#[user@kubernator]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        --namespace default \
        delete deployment \
            zeppelin-server


# -----------------------------------------------------
# Re-deploy Zeppelin.
#[user@kubernator]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        apply \
            --filename "${HOME}/zeppelin/zeppelin-server.yaml"

    >   configmap/zeppelin-server-conf-map unchanged
    >   configmap/zeppelin-server-conf unchanged
    >   deployment.apps/zeppelin-server created
    >   service/zeppelin-server unchanged
    >   serviceaccount/zeppelin-server unchanged
    >   role.rbac.authorization.k8s.io/zeppelin-server-role unchanged
    >   rolebinding.rbac.authorization.k8s.io/zeppelin-server-role-binding unchanged

    >   Failed to pull image "apache/zeppelin:0.9.0-SNAPSHOT": rpc error: code = Unknown desc = manifest for docker.io/apache/zeppelin:0.9.0-SNAPSHOT not found


# -----------------------------------------------------
# Delete the Zeppelin Pods.
#[user@kubernator]

    for podname in $(
        kubectl \
            --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
            --namespace default \
            get pods \
                --output json \
        | jq -r '.items[] | select(.metadata.name | test("zeppelin-server")) | .metadata.name'
        )
    do
        echo "Pod [${podname:?}]"
        kubectl \
            --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
            --namespace default \
            delete pod \
                "${podname:?}"
    done

    >   Pod [zeppelin-server-7868786bcc-7hb6z]
    >   pod "zeppelin-server-7868786bcc-7hb6z" deleted


# -----------------------------------------------------
# Delete the Zeppelin Deployment.
#[user@kubernator]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        --namespace default \
        delete deployment \
            zeppelin-server

    >   deployment.extensions "zeppelin-server" deleted


# -----------------------------------------------------
# Re-deploy Zeppelin.
#[user@kubernator]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        apply \
            --filename "${HOME}/zeppelin/zeppelin-server.yaml"

    >   configmap/zeppelin-server-conf-map unchanged
    >   configmap/zeppelin-server-conf unchanged
    >   deployment.apps/zeppelin-server created
    >   service/zeppelin-server unchanged
    >   serviceaccount/zeppelin-server unchanged
    >   role.rbac.authorization.k8s.io/zeppelin-server-role unchanged
    >   rolebinding.rbac.authorization.k8s.io/zeppelin-server-role-binding unchanged

    >   Failed to pull image "apache/zeppelin:0.9.0-SNAPSHOT": rpc error: code = Unknown desc = manifest for docker.io/apache/zeppelin:0.9.0-SNAPSHOT not found

    # Arrgh.
    # The image version is hard coded a second time.
    #


# -----------------------------------------------------
# Fix the Zeppelin image version.
#[user@kubernator]

    pushd "${HOME}/zeppelin"

        cp 'zeppelin-server.yaml' 'zeppelin-server.back'

        vi 'zeppelin-server.yaml'

            ....


        diff 'zeppelin-server.back' 'zeppelin-server.yaml'

    >   32c32
    >   <   ZEPPELIN_K8S_CONTAINER_IMAGE: apache/zeppelin:0.9.0-SNAPSHOT
    >   ---
    >   >   ZEPPELIN_K8S_CONTAINER_IMAGE: apache/zeppelin:0.9.0
    >   118c118
    >   <         image: apache/zeppelin:0.9.0-SNAPSHOT
    >   ---
    >   >         image: apache/zeppelin:0.9.0

    popd


# -----------------------------------------------------
# Delete the Zeppelin Deployment.
#[user@kubernator]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        --namespace default \
        delete deployment \
            zeppelin-server

    >   deployment.extensions "zeppelin-server" deleted


# -----------------------------------------------------
# Re-deploy Zeppelin.
#[user@kubernator]

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        apply \
            --filename "${HOME}/zeppelin/zeppelin-server.yaml"

    >   configmap/zeppelin-server-conf-map unchanged
    >   configmap/zeppelin-server-conf unchanged
    >   deployment.apps/zeppelin-server created
    >   service/zeppelin-server unchanged
    >   serviceaccount/zeppelin-server unchanged
    >   role.rbac.authorization.k8s.io/zeppelin-server-role unchanged
    >   rolebinding.rbac.authorization.k8s.io/zeppelin-server-role-binding unchanged


# -----------------------------------------------------
# Expose Zeppelin with a LoadBalancer.
#[user@kubernator]

    cat > /tmp/balancer.yaml << EOF
---
kind: Service
apiVersion: v1
metadata:
  name: zeppelin-external
spec:
  ports:
    - name: http
      port: 80
  selector:
    app.kubernetes.io/name: zeppelin-server
  type: LoadBalancer
EOF

    kubectl \
        --kubeconfig "${HOME}/.kube/${clustername:?}/config" \
        apply \
            --filename /tmp/balancer.yaml

    >   service/zeppelin-external created

    #
    # Worked :-) got the endpoint IP address from the dashboard GUI.
    #


