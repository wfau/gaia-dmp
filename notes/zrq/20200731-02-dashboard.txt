#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    Deploy Dashboard with Ingress using the Helm chart.
    Export the Ingress configuration to see how it works.
    See if this dashboard configuration is useable for development.

# -----------------------------------------------------

    # Deleted old cluster.
    # notes/zrq/20200718-03-openstack-delete.txt

    # Created new cluster.
    # notes/zrq/20200718-04-terraform-create.txt

# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --hostname kubernator \
        --publish 127.0.0.1:8443:8443 \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --env "clustername=${MAGNUM_CLUSTER:?}" \
        --volume "${HOME}/clouds.yaml:/etc/openstack/clouds.yaml:z" \
        --volume "${AGLAIS_CODE}/experiments/zrq/kubernetes:/kubernetes:z" \
        --volume "${ZEPPELIN_CODE}:/zeppelin:z" \
        atolmis/openstack-client \
        bash


# -----------------------------------------------------
# Get the connection details for our cluster.
#[user@kubernator]

    mkdir -p "${HOME}/.kube"
    openstack \
        --os-cloud "${cloudname:?}-super" \
        coe cluster config \
            "${clustername:?}" \
                --force \
                --dir "${HOME}/.kube"

    >   'SHELL'


# -----------------------------------------------------
# Install Dashboard using Helm.
# https://github.com/kubernetes/dashboard/tree/master/aio/deploy/helm-chart/kubernetes-dashboard
#[user@kubernator]

    helm repo add \
        kubernetes-dashboard \
        https://kubernetes.github.io/dashboard/

    >   "kubernetes-dashboard" has been added to your repositories

    options=(
        ingress.enabled=true,
        ingress.hosts=aglais-001.metagrid.xyz
        )

    echo "Options [${options[*]}]"

    helm install \
        valeria \
        kubernetes-dashboard/kubernetes-dashboard \
        --set "${options[*]}"

    >   NAME: valeria
    >   LAST DEPLOYED: Fri Jul 31 17:50:38 2020
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None
    >   ....
    >   ....


# -----------------------------------------------------
# Check what was installed.
#[user@kubernator]

    kubectl get Pods -A

    >   NAMESPACE   NAME                                            READY   STATUS      RESTARTS   AGE
    >   ....
    >   default     valeria-kubernetes-dashboard-68c8575456-mr8dq   1/1     Running     0          38s
    >   ....


    kubectl get Services -A

    >   NAMESPACE       NAME                            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)      AGE
    >   ....
    >   default         valeria-kubernetes-dashboard    ClusterIP   10.254.20.184    <none>        443/TCP      75s
    >   ....


    kubectl get ServiceAccount -A

    >   NAMESPACE         NAME                              SECRETS   AGE
    >   ....
    >   default           valeria-kubernetes-dashboard      1         104s
    >   ....


    kubectl get ClusterRole -A

    >   NAME                                        AGE
    >   ....
    >   valeria-kubernetes-dashboard-metrics        2m14s
    >   ....


    kubectl get ClusterRoleBinding -A

    >   NAME                                        AGE
    >   ....
    >   valeria-kubernetes-dashboard-metrics        3m11s
    >   ....


    kubectl get Ingress -A

    >   NAMESPACE   NAME                           HOSTS   ADDRESS   PORTS   AGE
    >   default     valeria-kubernetes-dashboard   *                 80      8m56s


# -----------------------------------------------------
# Check what the details of the Ingress.
#[user@kubernator]

    kubectl get Ingress \
        --output json \
        valeria-kubernetes-dashboard

    >   {
    >       "apiVersion": "extensions/v1beta1",
    >       "kind": "Ingress",
    >       "metadata": {
    >           "annotations": {
    >               "meta.helm.sh/release-name": "valeria",
    >               "meta.helm.sh/release-namespace": "default",
    >               "nginx.ingress.kubernetes.io/backend-protocol": "HTTPS",
    >               "service.alpha.kubernetes.io/app-protocols": "{\"https\":\"HTTPS\"}"
    >           },
    >           "creationTimestamp": "2020-07-31T17:50:40Z",
    >           "generation": 1,
    >           "labels": {
    >               "app.kubernetes.io/instance": "valeria",
    >               "app.kubernetes.io/managed-by": "Helm",
    >               "app.kubernetes.io/name": "kubernetes-dashboard",
    >               "app.kubernetes.io/version": "2.0.3",
    >               "helm.sh/chart": "kubernetes-dashboard-2.3.0"
    >           },
    >           "name": "valeria-kubernetes-dashboard",
    >           "namespace": "default",
    >           "resourceVersion": "45098",
    >           "selfLink": "/apis/extensions/v1beta1/namespaces/default/ingresses/valeria-kubernetes-dashboard",
    >           "uid": "2e0655be-338c-4248-a0e9-37db15138e01"
    >       },
    >       "spec": {
    >           "rules": [
    >               {
    >                   "http": {
    >                       "paths": [
    >                           {
    >                               "backend": {
    >                                   "serviceName": "valeria-kubernetes-dashboard",
    >                                   "servicePort": 443
    >                               },
    >                               "path": "/"
    >                           }
    >                       ]
    >                   }
    >               }
    >           ]
    >       },
    >       "status": {
    >           "loadBalancer": {}
    >       }
    >   }

    # No IP address, because we haven't added the Nginx Ingress controller yet.
    # No host name, possibly because the option list included a space ?

# -----------------------------------------------------
# Try fix the hostname.
#[user@kubernator]

    helm delete \
        valeria


    options=(
        ingress.enabled=true
        ingress.hosts=aglais-001.metagrid.xyz
        )

    lista=${options[*]}
    listb=${lista// /,}

    echo "Options [${listb}]"

    helm install \
        valeria \
        kubernetes-dashboard/kubernetes-dashboard \
        --set "${listb}"

    >   Error: template: kubernetes-dashboard/templates/ingress.yaml:42:28:
    >       executing "kubernetes-dashboard/templates/ingress.yaml" at <.Values.ingress.hosts>:
    >           range can't iterate over aglais-001.metagrid.xyz

    # It didn't like spaces in the list of options to set.
    # It expects the host names to be a list, which can't be expressed in a single string.

# -----------------------------------------------------
# Try passing the options in a YAML file.
# https://github.com/kubernetes/dashboard/blob/master/aio/deploy/helm-chart/kubernetes-dashboard/values.yaml
#[user@kubernator]

    helm delete \
        valeria

    cat > /tmp/dashboard-ingress.yaml << EOF
ingress:

  enabled: true

  paths:
    - /

  hosts:
    - aglais-001.metagrid.xyz
EOF

    helm install \
        valeria \
        kubernetes-dashboard/kubernetes-dashboard \
        --values /tmp/dashboard-ingress.yaml



    >   NAME: valeria
    >   LAST DEPLOYED: Fri Jul 31 18:30:58 2020
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None
    >   NOTES:
    >   ..
    >   *** PLEASE BE PATIENT: kubernetes-dashboard may take a few minutes to install ***
    >   ..
    >   From outside the cluster, the server URL(s) are:
    >        https://aglais-001.metagrid.xyz


# -----------------------------------------------------
# Check the details of the Ingress.
#[user@kubernator]

    kubectl get Ingress \
        --output json \
        valeria-kubernetes-dashboard

    >   {
    >       "apiVersion": "extensions/v1beta1",
    >       "kind": "Ingress",
    >       "metadata": {
    >           "annotations": {
    >               "meta.helm.sh/release-name": "valeria",
    >               "meta.helm.sh/release-namespace": "default",
    >               "nginx.ingress.kubernetes.io/backend-protocol": "HTTPS",
    >               "service.alpha.kubernetes.io/app-protocols": "{\"https\":\"HTTPS\"}"
    >           },
    >           "creationTimestamp": "2020-07-31T18:30:59Z",
    >           "generation": 1,
    >           "labels": {
    >               "app.kubernetes.io/instance": "valeria",
    >               "app.kubernetes.io/managed-by": "Helm",
    >               "app.kubernetes.io/name": "kubernetes-dashboard",
    >               "app.kubernetes.io/version": "2.0.3",
    >               "helm.sh/chart": "kubernetes-dashboard-2.3.0"
    >           },
    >           "name": "valeria-kubernetes-dashboard",
    >           "namespace": "default",
    >           "resourceVersion": "52536",
    >           "selfLink": "/apis/extensions/v1beta1/namespaces/default/ingresses/valeria-kubernetes-dashboard",
    >           "uid": "104cf4aa-f6c2-423e-9da4-5b1ff32929aa"
    >       },
    >       "spec": {
    >           "rules": [
    >               {
    >                   "host": "aglais-001.metagrid.xyz",
    >                   "http": {
    >                       "paths": [
    >                           {
    >                               "backend": {
    >                                   "serviceName": "valeria-kubernetes-dashboard",
    >                                   "servicePort": 443
    >                               },
    >                               "path": "/"
    >                           }
    >                       ]
    >                   }
    >               }
    >           ]
    >       },
    >       "status": {
    >           "loadBalancer": {}
    >       }
    >   }

    # Fixed the hostname, using YAML file for the values is easier.
    # Still no IP address, need to add the Nginx Ingress controller.

    # Interesting to see if adding the controller _after_ the Ingress works.


# -----------------------------------------------------
# Install the Kubernetes NGINX Ingress using Helm.
# https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md#using-helm
# (*) K8s-NGINX rather than NGINX-NGINX
#[user@kubernator]

    helm repo add \
        ingress-nginx \
        https://kubernetes.github.io/ingress-nginx

    >   "ingress-nginx" has been added to your repositories


    helm install \
        augusta \
        ingress-nginx/ingress-nginx

    >   NAME: augusta
    >   LAST DEPLOYED: Fri Jul 31 18:36:00 2020
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None
    >   ....
    >   ....


# -----------------------------------------------------
# Check the controller version.
# https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md#detect-installed-version
#[user@kubernator]

    POD_NAMESPACE=default
    POD_NAME=$(kubectl get pods -n $POD_NAMESPACE -l app.kubernetes.io/name=ingress-nginx --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')

    kubectl exec -it $POD_NAME -n $POD_NAMESPACE -- /nginx-ingress-controller --version

    >   -------------------------------------------------------------------------------
    >   NGINX Ingress controller
    >     Release:       v0.34.1
    >     Build:         v20200715-ingress-nginx-2.11.0-8-gda5fa45e2
    >     Repository:    https://github.com/kubernetes/ingress-nginx
    >     nginx version: nginx/1.19.1
    >
    >   -------------------------------------------------------------------------------


# -----------------------------------------------------
# Check the controller IP address.
#[user@kubernator]

    kubectl get Service \
        --output json \
        augusta-ingress-nginx-controller \
    | jq -r '.status.loadBalancer.ingress[0].ip'

    >   128.232.227.228


# -----------------------------------------------------
# Update our external DNS service.

    ....
    ....
    aglais-001.metagrid.xyz.    A    128.232.227.228



# -----------------------------------------------------
# See if the Ingress was updated with the IP address.
#[user@kubernator]

    kubectl get Ingress \
        --output json \
        valeria-kubernetes-dashboard

    >   {
    >       ....
    >       ....
    >       "status": {
    >           "loadBalancer": {
    >               "ingress": [
    >                   {
    >                       "ip": "128.232.227.228"
    >                   }
    >               ]
    >           }
    >       }
    >   }


    # Yes, you can add the Ingress controller after the Ingress.


# -----------------------------------------------------
# -----------------------------------------------------
# Try accessing from outside ...
#[user@desktop]

    firefox "https://aglais-001.metagrid.xyz/" &


        # Got the site .. OK
        # Login page asking for Token or Kubeconfig file.



# -----------------------------------------------------
# -----------------------------------------------------
# Use JQ to decode the secret token for our account.
# https://github.com/stedolan/jq/issues/47#issuecomment-374179653
#[user@kubernator]

    kubectl \
        get secrets \
    | grep valeria

    >   sh.helm.release.v1.valeria.v1              helm.sh/release.v1                    1      35m
    >   valeria-kubernetes-dashboard-certs         Opaque                                0      35m
    >   valeria-kubernetes-dashboard-token-295tj   kubernetes.io/service-account-token   3      35m


    dashtoken=$(
        kubectl \
            --output json \
            get secret \
                valeria-kubernetes-dashboard-token-295tj \
        | jq -r '.data.token | @base64d'
        )

    cat << EOF
---- ----
${dashtoken:?}
---- ----
EOF

    >   ---- ----
    >   eyJh....HW9A
    >   ---- ----


# -----------------------------------------------------
# -----------------------------------------------------
# Try using the token to login ...
#[user@desktop]

    firefox "https://aglais-001.metagrid.xyz/" &

    # We can login, but we can't access any resorces :-(


    >   namespaces is forbidden:
    >       User "system:serviceaccount:default:valeria-kubernetes-dashboard"
    >           cannot list resource "namespaces" in API group "" at the cluster scope


# -----------------------------------------------------
# -----------------------------------------------------
# Check the account details.
#[user@kubernator]

    kubectl \
        --output json \
        get ServiceAccount \
            valeria-kubernetes-dashboard

    >   {
    >       "apiVersion": "v1",
    >       "kind": "ServiceAccount",
    >       "metadata": {
    >           "annotations": {
    >               "meta.helm.sh/release-name": "valeria",
    >               "meta.helm.sh/release-namespace": "default"
    >           },
    >           "creationTimestamp": "2020-07-31T18:30:59Z",
    >           "labels": {
    >               "app.kubernetes.io/instance": "valeria",
    >               "app.kubernetes.io/managed-by": "Helm",
    >               "app.kubernetes.io/name": "kubernetes-dashboard",
    >               "app.kubernetes.io/version": "2.0.3",
    >               "helm.sh/chart": "kubernetes-dashboard-2.3.0"
    >           },
    >           "name": "valeria-kubernetes-dashboard",
    >           "namespace": "default",
    >           "resourceVersion": "52514",
    >           "selfLink": "/api/v1/namespaces/default/serviceaccounts/valeria-kubernetes-dashboard",
    >           "uid": "a6f6d4bf-46da-4710-8ea6-1b5aec82541d"
    >       },
    >       "secrets": [
    >           {
    >               "name": "valeria-kubernetes-dashboard-token-295tj"
    >           }
    >       ]
    >   }



    kubectl \
        --output json \
        get Role \
            valeria-kubernetes-dashboard

    >   {
    >       "apiVersion": "rbac.authorization.k8s.io/v1",
    >       "kind": "Role",
    >       "metadata": {
    >           "annotations": {
    >               "meta.helm.sh/release-name": "valeria",
    >               "meta.helm.sh/release-namespace": "default"
    >           },
    >           "creationTimestamp": "2020-07-31T18:30:59Z",
    >           "labels": {
    >               "app.kubernetes.io/instance": "valeria",
    >               "app.kubernetes.io/managed-by": "Helm",
    >               "app.kubernetes.io/name": "kubernetes-dashboard",
    >               "app.kubernetes.io/version": "2.0.3",
    >               "helm.sh/chart": "kubernetes-dashboard-2.3.0"
    >           },
    >           "name": "valeria-kubernetes-dashboard",
    >           "namespace": "default",
    >           "resourceVersion": "52521",
    >           "selfLink": "/apis/rbac.authorization.k8s.io/v1/namespaces/default/roles/valeria-kubernetes-dashboard",
    >           "uid": "7f07e2e7-1104-4929-b053-309b8ad54c7a"
    >       },
    >       "rules": [
    >           {
    >               "apiGroups": [
    >                   ""
    >               ],
    >               "resourceNames": [
    >                   "kubernetes-dashboard-key-holder",
    >                   "kubernetes-dashboard-certs",
    >                   "kubernetes-dashboard-csrf"
    >               ],
    >               "resources": [
    >                   "secrets"
    >               ],
    >               "verbs": [
    >                   "get",
    >                   "update",
    >                   "delete"
    >               ]
    >           },
    >           {
    >               "apiGroups": [
    >                   ""
    >               ],
    >               "resourceNames": [
    >                   "kubernetes-dashboard-settings"
    >               ],
    >               "resources": [
    >                   "configmaps"
    >               ],
    >               "verbs": [
    >                   "get",
    >                   "update"
    >               ]
    >           },
    >           {
    >               "apiGroups": [
    >                   ""
    >               ],
    >               "resourceNames": [
    >                   "heapster",
    >                   "dashboard-metrics-scraper"
    >               ],
    >               "resources": [
    >                   "services"
    >               ],
    >               "verbs": [
    >                   "proxy"
    >               ]
    >           },
    >           {
    >               "apiGroups": [
    >                   ""
    >               ],
    >               "resourceNames": [
    >                   "heapster",
    >                   "http:heapster:",
    >                   "https:heapster:",
    >                   "dashboard-metrics-scraper",
    >                   "http:dashboard-metrics-scraper"
    >               ],
    >               "resources": [
    >                   "services/proxy"
    >               ],
    >               "verbs": [
    >                   "get"
    >               ]
    >           }
    >       ]
    >   }


    kubectl \
        --output json \
        get RoleBinding \
            valeria-kubernetes-dashboard

    >   {
    >       "apiVersion": "rbac.authorization.k8s.io/v1",
    >       "kind": "RoleBinding",
    >       "metadata": {
    >           "annotations": {
    >               "meta.helm.sh/release-name": "valeria",
    >               "meta.helm.sh/release-namespace": "default"
    >           },
    >           "creationTimestamp": "2020-07-31T18:30:59Z",
    >           "labels": {
    >               "app.kubernetes.io/instance": "valeria",
    >               "app.kubernetes.io/managed-by": "Helm",
    >               "app.kubernetes.io/name": "kubernetes-dashboard",
    >               "app.kubernetes.io/version": "2.0.3",
    >               "helm.sh/chart": "kubernetes-dashboard-2.3.0"
    >           },
    >           "name": "valeria-kubernetes-dashboard",
    >           "namespace": "default",
    >           "resourceVersion": "52522",
    >           "selfLink": "/apis/rbac.authorization.k8s.io/v1/namespaces/default/rolebindings/valeria-kubernetes-dashboard",
    >           "uid": "67cb86ea-86f2-4c6c-aa90-03f630095fe4"
    >       },
    >       "roleRef": {
    >           "apiGroup": "rbac.authorization.k8s.io",
    >           "kind": "Role",
    >           "name": "valeria-kubernetes-dashboard"
    >       },
    >       "subjects": [
    >           {
    >               "kind": "ServiceAccount",
    >               "name": "valeria-kubernetes-dashboard",
    >               "namespace": "default"
    >           }
    >       ]
    >   }


    kubectl \
        --output json \
        get ClusterRole \
            valeria-kubernetes-dashboard-metrics

    >   {
    >       "apiVersion": "rbac.authorization.k8s.io/v1",
    >       "kind": "ClusterRole",
    >       "metadata": {
    >           "annotations": {
    >               "meta.helm.sh/release-name": "valeria",
    >               "meta.helm.sh/release-namespace": "default"
    >           },
    >           "creationTimestamp": "2020-07-31T18:30:59Z",
    >           "labels": {
    >               "app.kubernetes.io/instance": "valeria",
    >               "app.kubernetes.io/managed-by": "Helm",
    >               "app.kubernetes.io/name": "kubernetes-dashboard",
    >               "app.kubernetes.io/version": "2.0.3",
    >               "helm.sh/chart": "kubernetes-dashboard-2.3.0"
    >           },
    >           "name": "valeria-kubernetes-dashboard-metrics",
    >           "resourceVersion": "52519",
    >           "selfLink": "/apis/rbac.authorization.k8s.io/v1/clusterroles/valeria-kubernetes-dashboard-metrics",
    >           "uid": "19d4cef4-f1c1-4759-9017-4a6bd79c1d4d"
    >       },
    >       "rules": [
    >           {
    >               "apiGroups": [
    >                   "metrics.k8s.io"
    >               ],
    >               "resources": [
    >                   "pods",
    >                   "nodes"
    >               ],
    >               "verbs": [
    >                   "get",
    >                   "list",
    >                   "watch"
    >               ]
    >           }
    >       ]
    >   }


    kubectl \
        --output json \
        get ClusterRoleBinding \
            valeria-kubernetes-dashboard-metrics

    >   {
    >       "apiVersion": "rbac.authorization.k8s.io/v1",
    >       "kind": "ClusterRoleBinding",
    >       "metadata": {
    >           "annotations": {
    >               "meta.helm.sh/release-name": "valeria",
    >               "meta.helm.sh/release-namespace": "default"
    >           },
    >           "creationTimestamp": "2020-07-31T18:30:59Z",
    >           "labels": {
    >               "app.kubernetes.io/instance": "valeria",
    >               "app.kubernetes.io/managed-by": "Helm",
    >               "app.kubernetes.io/name": "kubernetes-dashboard",
    >               "app.kubernetes.io/version": "2.0.3",
    >               "helm.sh/chart": "kubernetes-dashboard-2.3.0"
    >           },
    >           "name": "valeria-kubernetes-dashboard-metrics",
    >           "resourceVersion": "52520",
    >           "selfLink": "/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/valeria-kubernetes-dashboard-metrics",
    >           "uid": "c952cb59-0d32-4a0f-88c9-777b996c91ee"
    >       },
    >       "roleRef": {
    >           "apiGroup": "rbac.authorization.k8s.io",
    >           "kind": "ClusterRole",
    >           "name": "valeria-kubernetes-dashboard-metrics"
    >       },
    >       "subjects": [
    >           {
    >               "kind": "ServiceAccount",
    >               "name": "valeria-kubernetes-dashboard",
    >               "namespace": "default"
    >           }
    >       ]
    >   }


    # The Role and RoleBinding cover specific named resources,
    # that cover config for the dashboard or heapster metrics.

    # The ClusterRole and ClusterRoleBinding are restricted to
    # the 'metrics.k8s.io' api group.

    # Basically, not a lot else.


# -----------------------------------------------------
# Check what the dashboard account can do.
#[user@kubernator]


    kubectl \
        auth can-i \
        --all-namespaces \
        --as valeria-kubernetes-dashboard \
            list pods

    >   no


    kubectl \
        auth can-i \
        --all-namespaces \
        --as valeria-kubernetes-dashboard \
            get pods

    >   no


    kubectl \
        auth can-i \
        --all-namespaces \
        --as valeria-kubernetes-dashboard \
            list nodes

    >   no


    kubectl \
        auth can-i \
        --all-namespaces \
        --as valeria-kubernetes-dashboard \
            get nodes

    >   no

    # Basically, not a lot.
    # Which is why the Dashboard documentation has this page:
    # https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md#creating-a-service-account


    IMPORTANT: Make sure that you know what you are doing before proceeding.
    Granting admin privileges to Dashboard's Service Account might be a security risk.

    - but without this, Dashboard can't actually _do_ anything.

    # Before we go there, there is a setting in the Heml chart
    # rbac.clusterReadOnlyRole

        If set, an additional cluster role / role binding will be created
        with read only permissions to all resources listed inside.

    # Can we update the deployment to add this ?
    # Or do we need to delete and deploy again ?


# -----------------------------------------------------
# Update the settings to add the clusterReadOnlyRole flag.
# https://github.com/kubernetes/dashboard/blob/master/aio/deploy/helm-chart/kubernetes-dashboard/values.yaml
#[user@kubernator]

    cat > /tmp/dashboard-roles.yaml << EOF
rbac:
  clusterReadOnlyRole: true
EOF

    helm upgrade \
        valeria \
        kubernetes-dashboard/kubernetes-dashboard \
        --values /tmp/dashboard-roles.yaml

    >   Release "valeria" has been upgraded. Happy Helming!
    >   NAME: valeria
    >   LAST DEPLOYED: Sat Aug  1 02:27:52 2020
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 2
    >   TEST SUITE: None
    >   ....
    >   ....


# -----------------------------------------------------
# Check for a new role.
#[user@kubernator]

    kubectl \
        get ClusterRoles

    >   NAME                                        AGE
    >   ....
    >   valeria-kubernetes-dashboard-readonly       6m40s
    >   ....


    kubectl \
        --output json \
        get ClusterRole \
            valeria-kubernetes-dashboard-readonly

    >   {
    >       "apiVersion": "rbac.authorization.k8s.io/v1",
    >       "kind": "ClusterRole",
    >       "metadata": {
    >           "annotations": {
    >               "meta.helm.sh/release-name": "valeria",
    >               "meta.helm.sh/release-namespace": "default"
    >           },
    >           "creationTimestamp": "2020-08-01T02:27:53Z",
    >           "labels": {
    >               "app.kubernetes.io/instance": "valeria",
    >               "app.kubernetes.io/managed-by": "Helm",
    >               "app.kubernetes.io/name": "kubernetes-dashboard",
    >               "app.kubernetes.io/version": "2.0.3",
    >               "helm.sh/chart": "kubernetes-dashboard-2.3.0"
    >           },
    >           "name": "valeria-kubernetes-dashboard-readonly",
    >           "resourceVersion": "143671",
    >           "selfLink": "/apis/rbac.authorization.k8s.io/v1/clusterroles/valeria-kubernetes-dashboard-readonly",
    >           "uid": "b7a270c7-951a-4f96-a230-a791dbb92e2a"
    >       },
    >       "rules": [
    >           {
    >               "apiGroups": [
    >                   ""
    >               ],
    >               "resources": [
    >                   "configmaps",
    >                   "endpoints",
    >                   "persistentvolumeclaims",
    >                   "pods",
    >                   "replicationcontrollers",
    >                   "replicationcontrollers/scale",
    >                   "serviceaccounts",
    >                   "services",
    >                   "nodes",
    >                   "persistentvolumeclaims",
    >                   "persistentvolumes"
    >               ],
    >               "verbs": [
    >                   "get",
    >                   "list",
    >                   "watch"
    >               ]
    >           },
    >           {
    >               "apiGroups": [
    >                   ""
    >               ],
    >               "resources": [
    >                   "bindings",
    >                   "events",
    >                   "limitranges",
    >                   "namespaces/status",
    >                   "pods/log",
    >                   "pods/status",
    >                   "replicationcontrollers/status",
    >                   "resourcequotas",
    >                   "resourcequotas/status"
    >               ],
    >               "verbs": [
    >                   "get",
    >                   "list",
    >                   "watch"
    >               ]
    >           },
    >           {
    >               "apiGroups": [
    >                   ""
    >               ],
    >               "resources": [
    >                   "namespaces"
    >               ],
    >               "verbs": [
    >                   "get",
    >                   "list",
    >                   "watch"
    >               ]
    >           },
    >           {
    >               "apiGroups": [
    >                   "apps"
    >               ],
    >               "resources": [
    >                   "daemonsets",
    >                   "deployments",
    >                   "deployments/scale",
    >                   "replicasets",
    >                   "replicasets/scale",
    >                   "statefulsets"
    >               ],
    >               "verbs": [
    >                   "get",
    >                   "list",
    >                   "watch"
    >               ]
    >           },
    >           {
    >               "apiGroups": [
    >                   "autoscaling"
    >               ],
    >               "resources": [
    >                   "horizontalpodautoscalers"
    >               ],
    >               "verbs": [
    >                   "get",
    >                   "list",
    >                   "watch"
    >               ]
    >           },
    >           {
    >               "apiGroups": [
    >                   "batch"
    >               ],
    >               "resources": [
    >                   "cronjobs",
    >                   "jobs"
    >               ],
    >               "verbs": [
    >                   "get",
    >                   "list",
    >                   "watch"
    >               ]
    >           },
    >           {
    >               "apiGroups": [
    >                   "extensions"
    >               ],
    >               "resources": [
    >                   "daemonsets",
    >                   "deployments",
    >                   "deployments/scale",
    >                   "ingresses",
    >                   "networkpolicies",
    >                   "replicasets",
    >                   "replicasets/scale",
    >                   "replicationcontrollers/scale"
    >               ],
    >               "verbs": [
    >                   "get",
    >                   "list",
    >                   "watch"
    >               ]
    >           },
    >           {
    >               "apiGroups": [
    >                   "policy"
    >               ],
    >               "resources": [
    >                   "poddisruptionbudgets"
    >               ],
    >               "verbs": [
    >                   "get",
    >                   "list",
    >                   "watch"
    >               ]
    >           },
    >           {
    >               "apiGroups": [
    >                   "networking.k8s.io"
    >               ],
    >               "resources": [
    >                   "networkpolicies"
    >               ],
    >               "verbs": [
    >                   "get",
    >                   "list",
    >                   "watch"
    >               ]
    >           },
    >           {
    >               "apiGroups": [
    >                   "storage.k8s.io"
    >               ],
    >               "resources": [
    >                   "storageclasses",
    >                   "volumeattachments"
    >               ],
    >               "verbs": [
    >                   "get",
    >                   "list",
    >                   "watch"
    >               ]
    >           },
    >           {
    >               "apiGroups": [
    >                   "rbac.authorization.k8s.io"
    >               ],
    >               "resources": [
    >                   "clusterrolebindings",
    >                   "clusterroles",
    >                   "roles",
    >                   "rolebindings"
    >               ],
    >               "verbs": [
    >                   "get",
    >                   "list",
    >                   "watch"
    >               ]
    >           }
    >       ]
    >   }


# -----------------------------------------------------
# Check if the Ingress is still there.
#[user@kubernator]

    kubectl get Ingress \
        --output json \
        valeria-kubernetes-dashboard

    >   Error from server (NotFound): ingresses.extensions "valeria-kubernetes-dashboard" not found


# -----------------------------------------------------
# Update the deployment with both sets of settings.
#[user@kubernator]

    helm upgrade \
        valeria \
        kubernetes-dashboard/kubernetes-dashboard \
        --values /tmp/dashboard-ingress.yaml,/tmp/dashboard-roles.yaml


# -----------------------------------------------------
# Check if the Ingress maps to the IP address.
#[user@kubernator]

    kubectl get Ingress \
        --output json \
        valeria-kubernetes-dashboard \
    | jq -r '.status.loadBalancer.ingress[0].ip'

    >   128.232.227.228


# -----------------------------------------------------
# Check the read only role is also there.
#[user@kubernator]

    kubectl \
        describe ClusterRole \
            valeria-kubernetes-dashboard-readonly

    >   Name:         valeria-kubernetes-dashboard-readonly
    >   Labels:       app.kubernetes.io/instance=valeria
    >                 app.kubernetes.io/managed-by=Helm
    >                 app.kubernetes.io/name=kubernetes-dashboard
    >                 app.kubernetes.io/version=2.0.3
    >                 helm.sh/chart=kubernetes-dashboard-2.3.0
    >   Annotations:  meta.helm.sh/release-name: valeria
    >                 meta.helm.sh/release-namespace: default
    >   PolicyRule:
    >     Resources                                      Non-Resource URLs  Resource Names  Verbs
    >     ---------                                      -----------------  --------------  -----
    >     persistentvolumeclaims                         []                 []              [get list watch get list watch]
    >     bindings                                       []                 []              [get list watch]
    >     configmaps                                     []                 []              [get list watch]
    >     endpoints                                      []                 []              [get list watch]
    >     events                                         []                 []              [get list watch]
    >     limitranges                                    []                 []              [get list watch]
    >     namespaces/status                              []                 []              [get list watch]
    >     namespaces                                     []                 []              [get list watch]
    >     nodes                                          []                 []              [get list watch]
    >     persistentvolumes                              []                 []              [get list watch]
    >     pods/log                                       []                 []              [get list watch]
    >     pods/status                                    []                 []              [get list watch]
    >     pods                                           []                 []              [get list watch]
    >     replicationcontrollers/scale                   []                 []              [get list watch]
    >     replicationcontrollers/status                  []                 []              [get list watch]
    >     replicationcontrollers                         []                 []              [get list watch]
    >     resourcequotas/status                          []                 []              [get list watch]
    >     resourcequotas                                 []                 []              [get list watch]
    >     serviceaccounts                                []                 []              [get list watch]
    >     services                                       []                 []              [get list watch]
    >     daemonsets.apps                                []                 []              [get list watch]
    >     deployments.apps/scale                         []                 []              [get list watch]
    >     deployments.apps                               []                 []              [get list watch]
    >     replicasets.apps/scale                         []                 []              [get list watch]
    >     replicasets.apps                               []                 []              [get list watch]
    >     statefulsets.apps                              []                 []              [get list watch]
    >     horizontalpodautoscalers.autoscaling           []                 []              [get list watch]
    >     cronjobs.batch                                 []                 []              [get list watch]
    >     jobs.batch                                     []                 []              [get list watch]
    >     daemonsets.extensions                          []                 []              [get list watch]
    >     deployments.extensions/scale                   []                 []              [get list watch]
    >     deployments.extensions                         []                 []              [get list watch]
    >     ingresses.extensions                           []                 []              [get list watch]
    >     networkpolicies.extensions                     []                 []              [get list watch]
    >     replicasets.extensions/scale                   []                 []              [get list watch]
    >     replicasets.extensions                         []                 []              [get list watch]
    >     replicationcontrollers.extensions/scale        []                 []              [get list watch]
    >     networkpolicies.networking.k8s.io              []                 []              [get list watch]
    >     poddisruptionbudgets.policy                    []                 []              [get list watch]
    >     clusterrolebindings.rbac.authorization.k8s.io  []                 []              [get list watch]
    >     clusterroles.rbac.authorization.k8s.io         []                 []              [get list watch]
    >     rolebindings.rbac.authorization.k8s.io         []                 []              [get list watch]
    >     roles.rbac.authorization.k8s.io                []                 []              [get list watch]
    >     storageclasses.storage.k8s.io                  []                 []              [get list watch]
    >     volumeattachments.storage.k8s.io               []                 []              [get list watch]


# -----------------------------------------------------
# -----------------------------------------------------
# Try using the account token to login ...
#[user@desktop]

    firefox "https://aglais-001.metagrid.xyz/" &

    Yay - we can see everything, but we can't change anything.

        pods "valeria-kubernetes-dashboard-68c8575456-cl68c" is forbidden:
        User "system:serviceaccount:default:valeria-kubernetes-dashboard"
        cannot delete resource "pods" in API group "" in the namespace "default"


# -----------------------------------------------------
# -----------------------------------------------------
# Create a new ServiceAccount.
# https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md#creating-a-service-account
#[user@kubernator]

cat > /tmp/admin-account.yaml << EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubernator
  namespace: default
EOF

    kubectl create \
        --filename /tmp/admin-account.yaml

    >   serviceaccount/kubernator created


    kubectl describe \
        ServiceAccount \
            kubernator

    >   Name:                kubernator
    >   Namespace:           default
    >   Labels:              <none>
    >   Annotations:         <none>
    >   Image pull secrets:  <none>
    >   Mountable secrets:   kubernator-token-r4k7w
    >   Tokens:              kubernator-token-r4k7w
    >   Events:              <none>


# -----------------------------------------------------
# Create a new ClusterRoleBinding.
# https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md#creating-a-clusterrolebinding
#[user@kubernator]

cat > /tmp/admin-binding.yaml << EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubernator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: kubernator
  namespace: default
EOF


    kubectl create \
        --filename /tmp/admin-binding.yaml

    >   clusterrolebinding.rbac.authorization.k8s.io/kubernator created


# -----------------------------------------------------
# Get the admin account token
#[user@kubernator]

    admintoken=$(
        kubectl \
            --output json \
            get secret \
                kubernator-token-r4k7w \
        | jq -r '.data.token | @base64d'
        )

    cat << EOF
---- ----
${admintoken:?}
---- ----
EOF

    >   yJhb....W7XA


# -----------------------------------------------------
# -----------------------------------------------------
# Try using the account token to login ...
#[user@desktop]

    firefox "https://aglais-001.metagrid.xyz/" &


# -----------------------------------------------------
# Try creating a new deployment.
#[user@desktop]

    [+][Create new resource]
        [Create from input]

        Copy/paste example
        https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/docs/examples/http-svc.yaml

    Yay - we can create new resources :-)

    ... and we canm delete them :-)






