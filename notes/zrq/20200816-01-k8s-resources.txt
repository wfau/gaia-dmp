#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#

    Previous tests:
    experiments/zrq/zeppelin/notebooks/test-note-003.zpln

        spark.driver.cores       2
        spark.driver.memory      2g
        spark.executor.cores     2
        spark.executor.memory    2g
        spark.executor.instances 2

        albert                - PASS 12s
        gaia-dr2-parquet-0-32 - PASS  4s
        gaia-dr2-parquet-0-16 - PASS  6s
        gaia-dr2-parquet-0-8  - PASS 12s

        gaia-dr2-parquet-0-4  - OutOfMemoryError: GC overhead limit exceeded
        gaia-dr2-parquet-1-4  - OutOfMemoryError: GC overhead limit exceeded


    To understand what is happening, we need to reproduce the
    OutOfMemoryError with a smaller data set.

    Be able to demonstrate do x and we get the error,
    do y and the error is fixed ..

    So far, we only get the error with 1/4 data sets and above.
    Problem is, we _always_ get the error with 1/4 data sets and above.
    Haven't found a way to fix it.

    Haven't found a way to reproduce the error with a smaller dataset.

    If we can do a 1/8 dataset with 2g memory,
    why can't we do a 1/4 dataset with 4g nodes ?

# -----------------------------------------------------

    Double to 8g Pods ..

        spark.driver.cores       4
        spark.driver.memory      8g
        spark.executor.cores     4
        spark.executor.memory    8g
        spark.executor.instances 4

    >   Every 2.0s: kubectl top node                                        kubernator: Sun Aug 16 04:06:10 2020
    >   
    >   NAME                                      CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
    >   tiberius-20200814-v7ysv35h66ur-master-0   76m          3%     1326Mi          22%
    >   tiberius-20200814-v7ysv35h66ur-node-0     40m          0%     3891Mi          8%
    >   tiberius-20200814-v7ysv35h66ur-node-1     48m          0%     1945Mi          4%
    >   tiberius-20200814-v7ysv35h66ur-node-2     40m          0%     4595Mi          10%
    >   tiberius-20200814-v7ysv35h66ur-node-3     32m          0%     2607Mi          5%
    >   tiberius-20200814-v7ysv35h66ur-node-4     52m          0%     4584Mi          10%
    >   tiberius-20200814-v7ysv35h66ur-node-5     49m          0%     4755Mi          10%


# -----------------------------------------------------

    Double to 16g Pods ..

        spark.driver.cores       4
        spark.driver.memory     16g
        spark.executor.cores     4
        spark.executor.memory   16g
        spark.executor.instances 4

    >   Every 2.0s: kubectl top node                                        kubernator: Sun Aug 16 04:09:32 2020
    >   
    >   NAME                                      CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
    >   tiberius-20200814-v7ysv35h66ur-master-0   90m          4%     1326Mi          22%
    >   tiberius-20200814-v7ysv35h66ur-node-0     1748m        12%    3569Mi          7%
    >   tiberius-20200814-v7ysv35h66ur-node-1     990m         7%     2863Mi          6%
    >   tiberius-20200814-v7ysv35h66ur-node-2     895m         6%     2853Mi          6%
    >   tiberius-20200814-v7ysv35h66ur-node-3     34m          0%     1345Mi          2%
    >   tiberius-20200814-v7ysv35h66ur-node-4     2101m        15%    5104Mi          11%
    >   tiberius-20200814-v7ysv35h66ur-node-5     978m         6%     2977Mi          6%

    >   Every 2.0s: kubectl top node                                        kubernator: Sun Aug 16 04:10:51 2020
    >   
    >   NAME                                      CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
    >   tiberius-20200814-v7ysv35h66ur-master-0   89m          4%     1326Mi          22%
    >   tiberius-20200814-v7ysv35h66ur-node-0     46m          0%     5872Mi          13%
    >   tiberius-20200814-v7ysv35h66ur-node-1     33m          0%     6381Mi          14%
    >   tiberius-20200814-v7ysv35h66ur-node-2     1789m        12%    5833Mi          12%
    >   tiberius-20200814-v7ysv35h66ur-node-3     33m          0%     1346Mi          2%
    >   tiberius-20200814-v7ysv35h66ur-node-4     54m          0%     6086Mi          13%
    >   tiberius-20200814-v7ysv35h66ur-node-5     46m          0%     6189Mi          13%


# -----------------------------------------------------

    Double to 32g Pods ..

        spark.driver.cores       4
        spark.driver.memory     32g
        spark.executor.cores     4
        spark.executor.memory   32g
        spark.executor.instances 4

    >   Every 2.0s: kubectl top node                                        kubernator: Sun Aug 16 04:17:03 2020
    >   
    >   NAME                                      CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
    >   tiberius-20200814-v7ysv35h66ur-master-0   80m          4%     1323Mi          22%
    >   tiberius-20200814-v7ysv35h66ur-node-0     52m          0%     9221Mi          20%
    >   tiberius-20200814-v7ysv35h66ur-node-1     56m          0%     9754Mi          21%
    >   tiberius-20200814-v7ysv35h66ur-node-2     706m         5%     3746Mi          8%
    >   tiberius-20200814-v7ysv35h66ur-node-3     36m          0%     1339Mi          2%
    >   tiberius-20200814-v7ysv35h66ur-node-4     63m          0%     9010Mi          19%
    >   tiberius-20200814-v7ysv35h66ur-node-5     52m          0%     9305Mi          20%

    >   Every 2.0s: kubectl top node                                        kubernator: Sun Aug 16 04:19:52 2020
    >   
    >   NAME                                      CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
    >   tiberius-20200814-v7ysv35h66ur-master-0   78m          3%     1324Mi          22%
    >   tiberius-20200814-v7ysv35h66ur-node-0     53m          0%     9217Mi          20%
    >   tiberius-20200814-v7ysv35h66ur-node-1     39m          0%     9751Mi          21%
    >   tiberius-20200814-v7ysv35h66ur-node-2     1989m        14%    11783Mi         26%
    >   tiberius-20200814-v7ysv35h66ur-node-3     35m          0%     1339Mi          2%
    >   tiberius-20200814-v7ysv35h66ur-node-4     58m          0%     9008Mi          19%
    >   tiberius-20200814-v7ysv35h66ur-node-5     81m          0%     9308Mi          20%

# -----------------------------------------------------

    Double to 64g Pods ..

        spark.driver.cores       4
        spark.driver.memory     64g
        spark.executor.cores     4
        spark.executor.memory   64g
        spark.executor.instances 4

    0/7 nodes are available: 1 Insufficient cpu, 7 Insufficient memory.


# -----------------------------------------------------

    It is *not* memory related.

    Allocating 32g of memory for each Pod.

        spark.driver.memory     32g
        spark.executor.memory   32g

    None of them are using more than 7g max.

    >   ....
    >   tiberius-20200814-v7ysv35h66ur-node-0     53m          0%     9217Mi          20%
    >   ....


# -----------------------------------------------------

    Back to 32g Pods..

        spark.driver.cores       4
        spark.driver.memory     32g
        spark.executor.cores     4
        spark.executor.memory   32g
        spark.executor.instances 4

    Change the dataset.

        df = sqlContext.read.parquet(
            "s3a://gaia-dr2-parquet-1-4/"
            )

    Save again, locks up on the 1/4 dataset.

# -----------------------------------------------------

    Back to 8g Pods and double the cores

        spark.driver.cores       8
        spark.driver.memory     8g
        spark.executor.cores     8
        spark.executor.memory   8g
        spark.executor.instances 4

    Same again, locks up on the 1/4 dataset.

# -----------------------------------------------------

    The resource we are not modifying is disc space ?




