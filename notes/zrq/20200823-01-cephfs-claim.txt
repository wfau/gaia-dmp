#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#

    Problem - mounting the persistent volume into the Spark nodes.

    Using config properties for the executor nodes works.

        spark.kubernetes.executor.volumes.persistentVolumeClaim.gaia-dr2.mount.path        /gaia/gaia-dr2
        spark.kubernetes.executor.volumes.persistentVolumeClaim.gaia-dr2.mount.readOnly    false
        spark.kubernetes.executor.volumes.persistentVolumeClaim.gaia-dr2.options.claimName gaia-dr2-volume-claim

    Using config properties executor nodes doesn't work.

        spark.kubernetes.driver.volumes.persistentVolumeClaim.gaia-dr2.mount.path          /gaia/gaia-dr2
        spark.kubernetes.driver.volumes.persistentVolumeClaim.gaia-dr2.mount.readOnly      false
        spark.kubernetes.driver.volumes.persistentVolumeClaim.gaia-dr2.options.claimName   gaia-dr2-volume-claim

    Zeppelin launches the Spark driver in client mode, in the same node as the Zeppelin interpreter.
    https://zeppelin.apache.org/docs/0.9.0-SNAPSHOT/quickstart/kubernetes.html

        "Zeppelin can run on clusters managed by Kubernetes. When Zeppelin runs in Pod,
         it creates pods for individual interpreter. Also Spark interpreter auto
         configured to use Spark on Kubernetes in client mode."

    Zeppelin launches the Spark interpreter using the interpreter-spec template
         aglais-zeppelin/k8s/interpreter/100-interpreter-spec.yaml

    The temaplate doesn't have anything for adding volume claims and mounts.

    Fix #1 - add a hard coded reference to the Gaia DR2 PersistentVolume.

    Fix #2 - add a generic template for PersistentVolumes.


# -----------------------------------------------------
# Update the interpreter spec template.
#[user@desktop]

    source "${HOME}/aglais.env"

    pushd "${ZEPPELIN_CODE:?}"

        gedit "k8s/interpreter/100-interpreter-spec.yaml" &

            ....
            ....


        git diff "k8s/interpreter/100-interpreter-spec.yaml"


    >   ....
    >   ....
    >        volumeMounts:
    >        - name: spark-home
    >          mountPath: /spark
    >   +    - name: gaia-dr2
    >   +      mountPath: /gaia/gaia-dr2
    >      initContainers:
    >      - name: spark-home-init
    >        image: {{zeppelin.k8s.spark.container.image}}
    >   @@ -77,6 +79,9 @@ spec:
    >      volumes:
    >      - name: spark-home
    >        emptyDir: {}
    >   +  - name: gaia-dr2
    >   +    persistentVolumeClaim:
    >   +      claimName: gaia-dr2-volume-claim
    >      {% endif %}
    >   ....
    >   ....

    popd


# -----------------------------------------------------
# Import the changed file into our build target.
#[user@desktop]

    source "${HOME}/aglais.env"

    ZEPPELIN_VERSION=0.9.0-SNAPSHOT
    ZEPPELIN_RESULT=${ZEPPELIN_CODE:?}/zeppelin-distribution/target/zeppelin-${ZEPPELIN_VERSION:?}/zeppelin-${ZEPPELIN_VERSION:?}

    cp "${ZEPPELIN_CODE:?}/k8s/interpreter/100-interpreter-spec.yaml" \
       "${ZEPPELIN_RESULT:?}/k8s/interpreter/100-interpreter-spec.yaml"


# -----------------------------------------------------
# Build our Zeppelin image.
#[user@desktop]

    buildsrc=2020.07.22
    buildtag=$(date '+%Y.%m.%d')
    buildtime=$(date '+%Y-%m-%dT%H:%M:%S')

    source "${HOME}/aglais.env"

    cp -f "${AGLAIS_CODE}/experiments/zrq/zeppelin/docker/log4j.properties" \
       "${ZEPPELIN_RESULT:?}"

    buildah bud \
        --tag aglais/zeppelin:latest \
        --tag aglais/zeppelin:${buildtag:?} \
        --build-arg "buildsrc=${buildsrc:?}" \
        --build-arg "buildtag=${buildtag:?}" \
        --build-arg "buildtime=${buildtime:?}" \
        --file "${AGLAIS_CODE:?}/experiments/zrq/zeppelin/docker/Dockerfile" \
        "${ZEPPELIN_RESULT:?}"


# -----------------------------------------------------
# Login to the Docker Hub.
#[user@desktop]

    podman login \
        --username $(secret docker.io.user) \
        --password $(secret docker.io.pass) \
        registry-1.docker.io

    >   Login Succeeded!


# -----------------------------------------------------
# Push our images to Docker Hub.
#[user@desktop]

    podman push "aglais/zeppelin:${buildtag:?}"

    >   Getting image source signatures
    >   Copying blob 968d3b985bf4 skipped: already exists
    >   Copying blob d908d9ad6713 [==>-----------------------------------] 11.0MiB / 124.1MiB
    >   Copying blob 377c01c3f4e3 skipped: already exists
    >   Copying blob 631dfaad8559 skipped: already exists
    >   Copying blob 4b820f29103a skipped: already exists
    >   Copying blob b258e3a0db8d skipped: already exists
    >   Copying blob 0ff8b6127fcc [--------------------------------------] 7.0MiB / 1.6GiB
    >   Copying blob e03fcd76b8ec skipped: already exists
    >   Copying blob 4001ed9bc75d skipped: already exists
    >   Copying blob 0b96b09d7ef9 skipped: already exists
    >   Copying blob ec4ea758b372 skipped: already exists

    # Eeek - 1.6GiB upload !!
    # Will take ages - sure there is a better way.


# -----------------------------------------------------
# Start with our exsiting image and just add the changed file.
#[user@desktop]

    builddir=$(mktemp -d)

    cp "${ZEPPELIN_CODE:?}/k8s/interpreter/100-interpreter-spec.yaml" \
       "${builddir:?}"

    cat > "${builddir:?}/Dockerfile" << 'EOF'
ARG buildsrc

FROM aglais/zeppelin:${buildsrc}
MAINTAINER Dave Morris <docker-admin@metagrid.co.uk>

ARG buildtag
ARG buildtime

LABEL maintainer="Dave Morris <docker-admin@metagrid.co.uk>"
LABEL buildtag="${buildtag}"
LABEL buildtime="${buildtime}"
LABEL gitrepo="https://github.com/wfau/aglais"

COPY 100-interpreter-spec.yaml /zeppelin/k8s/interpreter/

EOF

    buildsrc=2020.07.27
    buildtag=$(date '+%Y.%m.%d')
    buildtime=$(date '+%Y-%m-%dT%H:%M:%S')

    buildah bud \
        --format docker \
        --tag aglais/zeppelin-mod:latest \
        --tag aglais/zeppelin-mod:${buildtag:?} \
        --build-arg "buildsrc=${buildsrc:?}" \
        --build-arg "buildtag=${buildtag:?}" \
        --build-arg "buildtime=${buildtime:?}" \
        "${builddir:?}"

    >   STEP 1: FROM aglais/zeppelin:2020.07.27
    >   STEP 2: MAINTAINER Dave Morris <docker-admin@metagrid.co.uk>
    >   STEP 3: ARG buildtag
    >   STEP 4: ARG buildtime
    >   STEP 5: LABEL maintainer="Dave Morris <docker-admin@metagrid.co.uk>"
    >   STEP 6: LABEL buildtag="${buildtag}"
    >   STEP 7: LABEL buildtime="${buildtime}"
    >   STEP 8: LABEL gitrepo="https://github.com/wfau/aglais"
    >   STEP 9: COPY 100-interpreter-spec.yaml /k8s/interpreter/
    >   STEP 10: COMMIT aglais/zeppelin-mod:latest
    >   Getting image source signatures
    >   Copying blob d908d9ad6713 skipped: already exists
    >   ....
    >   ....
    >   Copying blob 0529d56c9f58 skipped: already exists
    >   Copying blob ae53b33548a2 done
    >   Copying config ddb8740b70 done
    >   Writing manifest to image destination
    >   Storing signatures
    >   --> ddb8740b708
    >   ddb8740b708e6b9edaaa6e3ac6d02106b301948c7f9afbaa1fef780988a69cdc


# -----------------------------------------------------
# Push our image to Docker Hub.
#[user@desktop]

    podman push "aglais/zeppelin-mod:${buildtag:?}"

    >   ....
    >   ....


    podman push "aglais/zeppelin-mod:latest"

    >   ....
    >   ....

    # Still managed to make an 128MiB upload out of a tiny change.
    # .. but much better than the 1.6GiB


# -----------------------------------------------------
# Final step is to update the Zeppelin deployment.
#[user@desktop]

    # Using an existing Zeppelin deployer from
    # notes/zrq/20200807-08-zeppelin-deploy.txt

    zpimage='aglais/zeppelin-mod:latest'

    source=/zeppelin/k8s/zeppelin-server.yaml
    deploy=/tmp/${zeppname:?}-server.yaml

    yq write \
        --inplace \
        --doc 0 \
        "${deploy:?}" \
        'data.ZEPPELIN_K8S_CONTAINER_IMAGE' \
            "${zpimage:?}"

    yq write \
        --inplace \
        --doc 2 \
        "${deploy:?}" \
        'spec.template.spec.containers.[0].image' \
            "${zpimage:?}"

    diff \
        --ignore-all-space \
        "${source:?}" \
        "${deploy:?}"

    >   31,32c31,32
    >   <   ZEPPELIN_K8S_SPARK_CONTAINER_IMAGE: spark:2.4.5
    >   <   ZEPPELIN_K8S_CONTAINER_IMAGE: apache/zeppelin:0.9.0-SNAPSHOT
    >   ---
    >   >   ZEPPELIN_K8S_SPARK_CONTAINER_IMAGE: aglais/pyspark-mod:latest
    >   >   ZEPPELIN_K8S_CONTAINER_IMAGE: aglais/zeppelin-mod:latest
    >   118c118
    >   <         image: apache/zeppelin:0.9.0-SNAPSHOT
    >   ---
    >   >         image: aglais/zeppelin-mod:latest
    >   ....
    >   ....



# -----------------------------------------------------
# Apply our updated template.
#[user@zepplinator]

    kubectl apply \
        --filename \
            "${deploy:?}"

    >   configmap/zeppelin-server-conf-map configured
    >   configmap/zeppelin-server-conf unchanged
    >   deployment.apps/zeppelin-server configured
    >   service/zeppelin-server unchanged
    >   serviceaccount/zeppelin-server unchanged
    >   role.rbac.authorization.k8s.io/zeppelin-server-role unchanged
    >   rolebinding.rbac.authorization.k8s.io/zeppelin-server-role-binding unchanged

    #
    # Error message in Kubernetes Dashboard:
    #   Failed to pull image "aglais/zeppelin-mod:latest": rpc error: code = Unknown desc = manifest for docker.io/aglais/zeppelin-mod:latest not found
    #

    # Is this because it was too soon since the uplaod.
    # .. or have we seen this before ?

    # Yep - seen this before.
    #   notes/zrq/20200724-02-zeppelin-k8s.txt
    #
    # The Nexus registry returns the wrong Content-Type.
    # https://github.com/containers/buildah/issues/575
    # https://github.com/containers/buildah/issues/575#issuecomment-384308227
    #
    #   "This confirms that buildah creates an OCI manifest, the Nexus registry (presumably) accepts it,
    #    but then returns it using a Docker schema2 Content-Type. That ultimately needs to be fixed in
    #    Nexus, either to reject OCI input as unrecognized, or to return it correctly (maybe converting
    #    to other formats if the client does not support reading OCI)."
    #
    #   "It seems, though, that buildah can be forced to upload using the Docker schema2, using something
    #    like buildah bud --format=docker. That could be a workaround in the meantime."
    #

    #
    # Added the --format=docker to our buildah command, re-built and uploaded again.
    # The format change could explian why we got a 128MiB upload for a tiny text file change.
    #
    # Yep - that fixed the issue.
    # Upload was much smaller, and updating the deployment worked.
    #



# -----------------------------------------------------
# Try accessing data on the PersistentVolume from Spark.
#[user@zeppelin]

    %spark.conf

    PYSPARK_PYTHON       python2
    spark.pyspark.python python2

    spark.driver.cores       2
    spark.driver.memory     20g

    spark.executor.cores     4
    spark.executor.memory    4g
    spark.executor.instances 4

    spark.kubernetes.executor.volumes.persistentVolumeClaim.gaia-dr2.mount.path        /gaia/gaia-dr2
    spark.kubernetes.executor.volumes.persistentVolumeClaim.gaia-dr2.mount.readOnly    false
    spark.kubernetes.executor.volumes.persistentVolumeClaim.gaia-dr2.options.claimName gaia-dr2-volume-claim

# -----------------------------------------------------

    %spark.pyspark

    df = sqlContext.read.parquet(
        "/gaia/gaia-dr2/gaia-dr2-16-0"
        )

    print "DF count: ", df.count()
    print "DF partitions: ", df.rdd.getNumPartitions()


#
# Our zeppelin-server Pod is deployed OK, but it is unable to launch a Spark interpreter Pod.
# Looked inside the zeppelin-server Pod and the 100-interpreter-spec.yaml doesn't look right.
#

# -----------------------------------------------------
# Check the template in the Pod.
#[user@kubernator]

    kubectl exec \
        --tty \
        --stdin \
        zeppelin-server-7d49b8f844-gb6s9 \
        -c zeppelin-server \
        -- \
            cat /zeppelin/k8s/interpreter/100-interpreter-spec.yaml

    >   ....
    >   ....
    >   kind: Pod
    >   apiVersion: v1
    >   metadata:
    >     namespace: {{zeppelin.k8s.namespace}}
    >     name: {{zeppelin.k8s.interpreter.pod.name}}
    >   ....
    >   ....
    >   spec:
    >     ....
    >     ....
    >     {% if zeppelin.k8s.interpreter.group.name == "spark" %}
    >       volumeMounts:
    >       - name: spark-home
    >         mountPath: /spark
    >     initContainers:
    >     - name: spark-home-init
    >       image: {{zeppelin.k8s.spark.container.image}}
    >       command: ["sh", "-c", "cp -r /opt/spark/* /spark/"]
    >       volumeMounts:
    >       - name: spark-home
    >         mountPath: /spark
    >       - name: gaia-dr2
    >         mountPath: /gaia/gaia-dr2
    >     volumes:
    >     - name: spark-home
    >       emptyDir: {}
    >     - name: gaia-dr2
    >       persistentVolumeClaim:
    >         claimName: gaia-dr2-volume-claim
    >     {% endif %}
    >   ....
    >   ....

    #
    # We put the PVC volume mount in the init container not in the main container.
    # We also used an absoulte path when we shouldn't have.
    # Not sure why it didn't load though.
    #

    #
    # Re-built the image, uploaded it, updated the deployment ...
    #

# -----------------------------------------------------
# Try accessing data on the PersistentVolume from Spark.
#[user@zeppelin]

    %spark.conf

    PYSPARK_PYTHON       python2
    spark.pyspark.python python2

    spark.driver.cores       2
    spark.driver.memory     20g

    spark.executor.cores     4
    spark.executor.memory    4g
    spark.executor.instances 4

    spark.kubernetes.executor.volumes.persistentVolumeClaim.gaia-dr2.mount.path        /gaia/gaia-dr2
    spark.kubernetes.executor.volumes.persistentVolumeClaim.gaia-dr2.mount.readOnly    false
    spark.kubernetes.executor.volumes.persistentVolumeClaim.gaia-dr2.options.claimName gaia-dr2-volume-claim

# -----------------------------------------------------

    %spark.pyspark

    df = sqlContext.read.parquet(
        "/gaia/gaia-dr2/gaia-dr2-full"
        )

    print "DF count: ", df.count()
    print "DF partitions: ", df.rdd.getNumPartitions()


    >   DF count:  1692919135
    >   DF partitions:  5985

    >   Took 11 sec. Last updated by anonymous at August 23 2020, 1:00:46 PM.

    #
    # Yay - WORKS :-D
    #

    #
    # TODO - Next, see if we can bring all the bits together to make a useable system.
    #



