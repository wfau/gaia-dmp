#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#

# -----------------------------------------------------

Experiment:

    Follow on from previous notes.
    20200902-01-manila-csi.txt

    Can we create a dynamic volume using the CSI driver ?

    Create a StorageClass for Manila CSI.
    Create a PersistentVolumeClaim using the StorageClass.
    Create a Pod that refers to the PersistentVolumeClaim.

Result:

    Fails to authenticate with the username/password credentials.

# -----------------------------------------------------
# Create a new set of secrets using username and password.
#[user@kubernator]

    username=........
    password=........
    domainname=default
    projectname=${cloudname:?}

    cat > "/tmp/claudyna-secrets.yaml" << EOF
apiVersion: v1
kind: Secret
metadata:
  name: claudyna-secrets
  namespace: default
stringData:
  # Mandatory
  os-authURL: "${authurl:?}"
  os-region: "${region:?}"

  # Authentication using username and passowrd
  os-userName: "${username:?}"
  os-password: "${password:?}"
  os-domainName: "${domainname:?}"
  os-projectName: "${projectname:?}"
EOF

    kubectl create \
        --filename "/tmp/claudyna-secrets.yaml"

    kubectl describe \
        Secret \
            claudyna-secrets

    >   Name:         claudyna-secrets
    >   Namespace:    default
    >   Labels:       <none>
    >   Annotations:  <none>
    >   
    >   Type:  Opaque
    >   
    >   Data
    >   ====
    >   os-authURL:      47 bytes
    >   os-domainName:   7 bytes
    >   os-password:     36 bytes
    >   os-projectName:  9 bytes
    >   os-region:       9 bytes
    >   os-userName:     17 bytes


# -----------------------------------------------------
# Create a storage class.
#[user@kubernator]

    cat > "/tmp/claudyna-class.yaml" << EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: claudyna-class
provisioner: cephfs.manila.csi.openstack.org
parameters:
  # Manila share type
  type: default

  csi.storage.k8s.io/provisioner-secret-name: claudyna-secrets
  csi.storage.k8s.io/provisioner-secret-namespace: default
  csi.storage.k8s.io/node-stage-secret-name: claudyna-secrets
  csi.storage.k8s.io/node-stage-secret-namespace: default
  csi.storage.k8s.io/node-publish-secret-name: claudyna-secrets
  csi.storage.k8s.io/node-publish-secret-namespace: default
EOF

    kubectl apply \
        --filename "/tmp/claudyna-class.yaml"

    kubectl describe \
        StorageClass \
            claudyna-class

    >   Name:                  claudyna-class
    >   IsDefaultClass:        No
    >   Annotations:           <none>
    >   Provisioner:           cephfs.manila.csi.openstack.org
    >   Parameters:            csi.storage.k8s.io/node-publish-secret-name=claudyna-secrets,csi.storage.k8s.io/node-publish-secret-namespace=default,csi.storage.k8s.io/node-stage-secret-name=claudyna-secrets,csi.storage.k8s.io/node-stage-secret-namespace=default,csi.storage.k8s.io/provisioner-secret-name=claudyna-secrets,csi.storage.k8s.io/provisioner-secret-namespace=default,type=default
    >   AllowVolumeExpansion:  <unset>
    >   MountOptions:          <none>
    >   ReclaimPolicy:         Delete
    >   VolumeBindingMode:     Immediate
    >   Events:                <none>


    >   Parameters:
    >       csi.storage.k8s.io/node-publish-secret-name=claudyna-secrets,
    >       csi.storage.k8s.io/node-publish-secret-namespace=default,
    >       csi.storage.k8s.io/node-stage-secret-name=claudyna-secrets,
    >       csi.storage.k8s.io/node-stage-secret-namespace=default,
    >       csi.storage.k8s.io/provisioner-secret-name=claudyna-secrets,
    >       csi.storage.k8s.io/provisioner-secret-namespace=default,
    >       type=default


# -----------------------------------------------------
# Create a volume claim.
#[user@kubernator]

    cat > "/tmp/claudyna-claim.yaml" << EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claudyna-claim
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: claudyna-class
EOF

    kubectl apply \
        --filename "/tmp/claudyna-claim.yaml"

    kubectl describe \
        PersistentVolumeClaim \
            claudyna-claim

    >   Name:          claudyna-claim
    >   Namespace:     default
    >   StorageClass:  claudyna-class
    >   Status:        Pending
    >   Volume:
    >   Labels:        <none>
    >   Annotations:   volume.beta.kubernetes.io/storage-provisioner: cephfs.manila.csi.openstack.org
    >   Finalizers:    [kubernetes.io/pvc-protection]
    >   Capacity:
    >   Access Modes:
    >   VolumeMode:    Filesystem
    >   Mounted By:    <none>
    >   Events:
    >     Type     Reason                Age                From                                                                                                       Message
    >     ----     ------                ----               ----                                                                                                       -------
    >     Normal   Provisioning          16s (x6 over 45s)  cephfs.manila.csi.openstack.org_csi-manila-cephfs-controllerplugin-0_1a1d51e1-fe1c-4a21-a2de-4d609b9bb56f  External provisioner is provisioning volume for claim "default/claudyna-claim"
    >     Warning  ProvisioningFailed    16s (x6 over 45s)  cephfs.manila.csi.openstack.org_csi-manila-cephfs-controllerplugin-0_1a1d51e1-fe1c-4a21-a2de-4d609b9bb56f  failed to provision volume with StorageClass "claudyna-class": rpc error: code = Unauthenticated desc = failed to create Manila v2 client: failed to authenticate: Authentication failed
    >     Normal   ExternalProvisioning  13s (x6 over 45s)  persistentvolume-controller                                                                                waiting for a volume to be created, either by external provisioner "cephfs.manila.csi.openstack.org" or manually created by system administrator

    #
    # Not looking that good :-(
    # Authentication failed
    #

# -----------------------------------------------------
# Create a Pod that mounts the volume.
#[user@kubernator]

    cat > /tmp/claudyna-pod.yaml << EOF
kind: Pod
apiVersion: v1
metadata:
  name: claudyna-pod
  namespace: default
spec:
  volumes:
    - name: share-data
      persistentVolumeClaim:
        claimName: claudyna-claim
        readOnly: false
    - name: local-data
      emptyDir: {}
  containers:
    - name: claudyna-container
      image: 'fedora:latest'
      volumeMounts:
        - name: share-data
          mountPath: /share-data
        - name: local-data
          mountPath: /local-data
      command: ["/bin/sh"]
      args:
        - "-c"
        - >-
          while true; do
          date >> /share-data/\$(hostname).log;
          sleep 1;
          done
EOF

    kubectl apply \
        --filename /tmp/claudyna-pod.yaml

    kubectl describe \
        Pod \
            claudyna-pod

    >   ....
    >   ....
    >   Volumes:
    >     share-data:
    >       Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    >       ClaimName:  claudyna-claim
    >       ReadOnly:   false
    >     local-data:
    >       Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    >       Medium:
    >       SizeLimit:  <unset>
    >     default-token-4dcpn:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  default-token-4dcpn
    >       Optional:    false
    >   ....
    >   ....
    >   Events:
    >     Type     Reason             Age                   From                Message
    >     ----     ------             ----                  ----                -------
    >     Warning  FailedScheduling   <unknown>             default-scheduler   error while running "VolumeBinding" filter plugin for pod "claudyna-pod": pod has unbound immediate PersistentVolumeClaims
    >     Warning  FailedScheduling   <unknown>             default-scheduler   error while running "VolumeBinding" filter plugin for pod "claudyna-pod": pod has unbound immediate PersistentVolumeClaims
    >     Normal   NotTriggerScaleUp  10s (x15 over 2m30s)  cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added): 1 max limit reached

    #
    # In this sequence, the PersistentVolume is created as soon as the PersistentVolumeClaims is defined.
    # Creating the PersistentVolume failed with Authentication errors.
    #

    #
    # That might be because the test user account doesn't have permission to create volumes.
    # TODO Find out how to grant permission ?
    #

    #
    # In the mean time, can we use our application credentials fopr this ?
    #


# -----------------------------------------------------
# Create a new set of secrets using application credentials from clouds.yaml.
#[user@kubernator]

    authurl=$(
        yq r /etc/openstack/clouds.yaml \
            'clouds.gaia-prod.auth.auth_url'
        )

    region=$(
        yq r /etc/openstack/clouds.yaml \
            'clouds.gaia-prod.region_name'
        )

    credentialID=$(
        yq r /etc/openstack/clouds.yaml \
            'clouds.gaia-prod.auth.application_credential_id'
        )

    credentialsecret=$(
        yq r /etc/openstack/clouds.yaml \
            'clouds.gaia-prod.auth.application_credential_secret'
        )


    cat > "/tmp/claudyna-secrets.yaml" << EOF
apiVersion: v1
kind: Secret
metadata:
  name: claudyna-secrets
  namespace: default
stringData:
  # Mandatory
  os-authURL: "${authurl:?}"
  os-region: "${region:?}"

  # Authentication using app credentials
  os-applicationCredentialID: "${credentialID:?}"
  os-applicationCredentialSecret: "${credentialsecret:?}"

EOF

    kubectl create \
        --filename "/tmp/claudyna-secrets.yaml"

    kubectl describe \
        Secret \
            claudyna-secrets

    >   Name:         claudyna-secrets
    >   Namespace:    default
    >   Labels:       <none>
    >   Annotations:  <none>
    >   
    >   Type:  Opaque
    >   
    >   Data
    >   ====
    >   os-applicationCredentialID:      32 bytes
    >   os-applicationCredentialSecret:  35 bytes
    >   os-authURL:                      47 bytes
    >   os-region:                       9 bytes

# -----------------------------------------------------
# Delete and re-create everything.
#[user@kubernator]

    kubectl delete \
        Pod \
            claudyna-pod

    kubectl delete \
        PersistentVolumeClaim \
            claudyna-claim

    kubectl delete \
        StorageClass \
            claudyna-class

    kubectl delete \
        Secret \
            claudyna-secrets

    kubectl create \
        --filename "/tmp/claudyna-secrets.yaml"

    kubectl apply \
        --filename "/tmp/claudyna-class.yaml"

    kubectl apply \
        --filename "/tmp/claudyna-claim.yaml"

    kubectl apply \
        --filename "/tmp/claudyna-pod.yaml"

# -----------------------------------------------------
# Check what was created.
#[user@kubernator]

    kubectl describe \
        Pod \
            claudyna-pod

    >   ....
    >   ....
    >   Events:
    >     Type     Reason             Age               From                Message
    >     ----     ------             ----              ----                -------
    >     Warning  FailedScheduling   <unknown>         default-scheduler   error while running "VolumeBinding" filter plugin for pod "claudyna-pod": pod has unbound immediate PersistentVolumeClaims
    >     Normal   NotTriggerScaleUp  8s (x2 over 18s)  cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added): 1 max limit reached


    kubectl describe \
        PersistentVolumeClaim \
            claudyna-claim

    >   ....
    >   ....
    >   Events:
    >     Type     Reason                Age                From                                                                                                       Message
    >     ----     ------                ----               ----                                                                                                       -------
    >     Normal   Provisioning          13s (x7 over 76s)  cephfs.manila.csi.openstack.org_csi-manila-cephfs-controllerplugin-0_1a1d51e1-fe1c-4a21-a2de-4d609b9bb56f  External provisioner is provisioning volume for claim "default/claudyna-claim"
    >     Warning  ProvisioningFailed    13s (x7 over 76s)  cephfs.manila.csi.openstack.org_csi-manila-cephfs-controllerplugin-0_1a1d51e1-fe1c-4a21-a2de-4d609b9bb56f  failed to provision volume with StorageClass "claudyna-class": rpc error: code = InvalidArgument desc = invalid OpenStack secrets: parameter 'os-authURL' requires exactly one of [os-password os-trustID] parameters
    >     Normal   ExternalProvisioning  1s (x6 over 76s)   persistentvolume-controller                                                                                waiting for a volume to be created, either by external provisioner "cephfs.manila.csi.openstack.org" or manually created by system administrator

    #
    # Same error as the static volume.
    #

    >   .... parameter 'os-authURL' requires exactly one of [os-password os-trustID] parameters

    #
    # If we use username/password authentication, we get a permission error.
    # Possibly because the test account does not have permission to provision new shares?
    # Nope - tested the user account using the GUI, able to login and create a share no problems.
    #

    #
    # If we use the application credentials, we get the missing parameter error.
    # Possibly due to the bug we are chasing in the Manila CSI source code.
    #

    #
    # Looks like we are on the right track.
    # Would have been nice to be able to create a share using the test account ....
    #

    #
    # While I was using thr GUI, I noticed something in the create process.
    # The share type option was 'cephfsnativetype', and we have default.
    # Try again ...
    #

# -----------------------------------------------------
# Update our storage class to 'cephfsnativetype'.
#[user@kubernator]

    cat > "/tmp/claudyna-class.yaml" << EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: claudyna-class
provisioner: cephfs.manila.csi.openstack.org
parameters:
  # Manila share type
  type: cephfsnativetype

  csi.storage.k8s.io/provisioner-secret-name: claudyna-secrets
  csi.storage.k8s.io/provisioner-secret-namespace: default
  csi.storage.k8s.io/node-stage-secret-name: claudyna-secrets
  csi.storage.k8s.io/node-stage-secret-namespace: default
  csi.storage.k8s.io/node-publish-secret-name: claudyna-secrets
  csi.storage.k8s.io/node-publish-secret-namespace: default
EOF

    kubectl apply \
        --filename "/tmp/claudyna-class.yaml"

    >   The StorageClass "claudyna-class" is invalid: parameters: Forbidden: updates to parameters are forbidden.


# -----------------------------------------------------
# Delete and re-create everything.
#[user@kubernator]

    kubectl delete \
        Pod \
            claudyna-pod

    kubectl delete \
        PersistentVolumeClaim \
            claudyna-claim

    kubectl delete \
        StorageClass \
            claudyna-class

    kubectl delete \
        Secret \
            claudyna-secrets

    kubectl create \
        --filename "/tmp/claudyna-secrets.yaml"

    kubectl apply \
        --filename "/tmp/claudyna-class.yaml"

    kubectl apply \
        --filename "/tmp/claudyna-claim.yaml"

    kubectl apply \
        --filename "/tmp/claudyna-pod.yaml"

# -----------------------------------------------------
# Check what was created.
#[user@kubernator]

    kubectl describe \
        Pod \
            claudyna-pod

    >   ....
    >   ....
    >   Events:
    >     Type     Reason            Age        From               Message
    >     ----     ------            ----       ----               -------
    >     Warning  FailedScheduling  <unknown>  default-scheduler  error while running "VolumeBinding" filter plugin for pod "claudyna-pod": pod has unbound immediate PersistentVolumeClaims
    >     Warning  FailedScheduling  <unknown>  default-scheduler  error while running "VolumeBinding" filter plugin for pod "claudyna-pod": pod has unbound immediate PersistentVolumeClaims


    kubectl describe \
        PersistentVolumeClaim \
            claudyna-claim

    >   Events:
    >     Type     Reason                Age               From                                                                                                       Message
    >     ----     ------                ----              ----                                                                                                       -------
    >     Normal   ExternalProvisioning  2s (x4 over 32s)  persistentvolume-controller                                                                                waiting for a volume to be created, either by external provisioner "cephfs.manila.csi.openstack.org" or manually created by system administrator
    >     Normal   Provisioning          1s (x6 over 32s)  cephfs.manila.csi.openstack.org_csi-manila-cephfs-controllerplugin-0_1a1d51e1-fe1c-4a21-a2de-4d609b9bb56f  External provisioner is provisioning volume for claim "default/claudyna-claim"
    >     Warning  ProvisioningFailed    1s (x6 over 32s)  cephfs.manila.csi.openstack.org_csi-manila-cephfs-controllerplugin-0_1a1d51e1-fe1c-4a21-a2de-4d609b9bb56f  failed to provision volume with StorageClass "claudyna-class": rpc error: code = InvalidArgument desc = invalid OpenStack secrets: parameter 'os-authURL' requires exactly one of [os-password os-trustID] parameters


# -----------------------------------------------------
# Delete and re-create using username and password.
#[user@kubernator]

    username=........
    password=........
    domainname=default
    projectname=${cloudname:?}

    cat > "/tmp/claudyna-secrets.yaml" << EOF
apiVersion: v1
kind: Secret
metadata:
  name: claudyna-secrets
  namespace: default
stringData:
  # Mandatory
  os-authURL: "${authurl:?}"
  os-region: "${region:?}"

  # Authentication using username and passowrd
  os-userName: "${username:?}"
  os-password: "${password:?}"
  os-domainName: "${domainname:?}"
  os-projectName: "${projectname:?}"
EOF

    kubectl delete \
        Secret \
            claudyna-secrets

    kubectl create \
        --filename "/tmp/claudyna-secrets.yaml"

    kubectl describe \
        Secret \
            claudyna-secrets

    >   ....
    >   ....
    >   Data
    >   ====
    >   os-projectName:  9 bytes
    >   os-region:       9 bytes
    >   os-userName:     17 bytes
    >   os-authURL:      47 bytes
    >   os-domainName:   7 bytes
    >   os-password:     36 bytes


# -----------------------------------------------------
# Delete and re-create everything.
#[user@kubernator]

    kubectl delete \
        Pod \
            claudyna-pod

    kubectl delete \
        PersistentVolumeClaim \
            claudyna-claim

    kubectl delete \
        StorageClass \
            claudyna-class

    kubectl delete \
        Secret \
            claudyna-secrets

    kubectl create \
        --filename "/tmp/claudyna-secrets.yaml"

    kubectl apply \
        --filename "/tmp/claudyna-class.yaml"

    kubectl apply \
        --filename "/tmp/claudyna-claim.yaml"


# -----------------------------------------------------
# Check what was created.
#[user@kubernator]

    kubectl describe \
        PersistentVolumeClaim \
            claudyna-claim

    >   ....
    >   ....
    >   Events:
    >     Type     Reason                Age              From                                                                                                       Message
    >     ----     ------                ----             ----                                                                                                       -------
    >     Normal   ExternalProvisioning  6s (x3 over 6s)  persistentvolume-controller                                                                                waiting for a volume to be created, either by external provisioner "cephfs.manila.csi.openstack.org" or manually created by system administrator
    >     Normal   Provisioning          1s (x4 over 6s)  cephfs.manila.csi.openstack.org_csi-manila-cephfs-controllerplugin-0_1a1d51e1-fe1c-4a21-a2de-4d609b9bb56f  External provisioner is provisioning volume for claim "default/claudyna-claim"
    >     Warning  ProvisioningFailed    1s (x4 over 6s)  cephfs.manila.csi.openstack.org_csi-manila-cephfs-controllerplugin-0_1a1d51e1-fe1c-4a21-a2de-4d609b9bb56f  failed to provision volume with StorageClass "claudyna-class": rpc error: code = Unauthenticated desc = failed to create Manila v2 client: failed to authenticate: Authentication failed


# -----------------------------------------------------
# Take a look at a Manila CSI nodeplugin Pod ....
#[user@kubernator]

    kubectl get pods

    >   NAME                                                        READY   STATUS              RESTARTS   AGE
    >   augusta-20200901-ingress-nginx-controller-76c7d4c9c-5jkq7   1/1     Running             0          29h
    >   csi-manila-cephfs-controllerplugin-0                        3/3     Running             0          29h
    >   csi-manila-cephfs-nodeplugin-2kd8l                          2/2     Running             0          29h
    >   csi-manila-cephfs-nodeplugin-f8fv8                          2/2     Running             0          29h
    >   csi-manila-cephfs-nodeplugin-ll4cv                          2/2     Running             0          29h
    >   csi-manila-cephfs-nodeplugin-s62r4                          2/2     Running             0          29h
    >   magruela-pod                                                0/1     ContainerCreating   0          3h1m
    >   valeria-20200901-kubernetes-dashboard-6f65478dfd-4fgcn      2/2     Running             0          29h


    kubectl describe pod \
        csi-manila-cephfs-nodeplugin-ll4cv


    >   ....
    >   ....
    >   Containers:
    >     cephfs-registrar:
    >       ....
    >       Image:         quay.io/k8scsi/csi-node-driver-registrar:v1.1.0
    >       ....
    >       Args:
    >         --v=5
    >         --csi-address=/csi/csi.sock
    >         --kubelet-registration-path=/var/lib/kubelet/plugins/cephfs.manila.csi.openstack.org/csi.sock
    >       ....
    >       Environment:
    >         KUBE_NODE_NAME:   (v1:spec.nodeName)
    >       Mounts:
    >         /csi from cephfs-plugin-dir (rw)
    >         /registration from registration-dir (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from csi-manila-cephfs-nodeplugin-token-glbcz (ro)
    >   
    >     cephfs-nodeplugin:
    >       ....
    >       Image:         k8scloudprovider/manila-csi-plugin:latest
    >       ....
    >       Command:
    >         /bin/sh
    >         -c
    >         /bin/manila-csi-plugin --v=5 --nodeid=$(NODE_ID) --endpoint=$(CSI_ENDPOINT) --drivername=$(DRIVER_NAME) --share-protocol-selector=$(MANILA_SHARE_PROTO) --fwdendpoint=$(FWD_CSI_ENDPOINT)
    >       ....
    >       Environment:
    >         DRIVER_NAME:         cephfs.manila.csi.openstack.org
    >         NODE_ID:              (v1:spec.nodeName)
    >         CSI_ENDPOINT:        unix:///var/lib/kubelet/plugins/cephfs.manila.csi.openstack.org/csi.sock
    >         FWD_CSI_ENDPOINT:    unix:///var/lib/kubelet/plugins/cephfs.csi.ceph.com/csi.sock
    >         MANILA_SHARE_PROTO:  CEPHFS
    >       Mounts:
    >         /var/lib/kubelet/plugins/cephfs.csi.ceph.com from cephfs-fwd-plugin-dir (rw)
    >         /var/lib/kubelet/plugins/cephfs.manila.csi.openstack.org from cephfs-plugin-dir (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from csi-manila-cephfs-nodeplugin-token-glbcz (ro)
    >   ....
    >   Volumes:
    >     registration-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/lib/kubelet/plugins_registry
    >       HostPathType:  Directory
    >     cephfs-plugin-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/lib/kubelet/plugins/cephfs.manila.csi.openstack.org
    >       HostPathType:  DirectoryOrCreate
    >     cephfs-fwd-plugin-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/lib/kubelet/plugins/cephfs.csi.ceph.com
    >       HostPathType:  DirectoryOrCreate
    >     csi-manila-cephfs-nodeplugin-token-glbcz:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  csi-manila-cephfs-nodeplugin-token-glbcz
    >       Optional:    false
    >   ....


# -----------------------------------------------------
# Take a look at a CephFS CSI nodeplugin Pod ....
#[user@kubernator]

    kubectl --namespace ceph-csi-cephfs get pods

    >   NAME                                           READY   STATUS    RESTARTS   AGE
    >   ceph-csi-cephfs-nodeplugin-6bt6t               3/3     Running   0          30h
    >   ceph-csi-cephfs-nodeplugin-74vlr               3/3     Running   0          30h
    >   ceph-csi-cephfs-nodeplugin-mwhqg               3/3     Running   0          30h
    >   ceph-csi-cephfs-nodeplugin-zz2hw               3/3     Running   0          30h
    >   ceph-csi-cephfs-provisioner-6c6c9c4f97-427n4   6/6     Running   0          30h
    >   ceph-csi-cephfs-provisioner-6c6c9c4f97-f5md2   6/6     Running   0          30h
    >   ceph-csi-cephfs-provisioner-6c6c9c4f97-pd7wz   6/6     Running   0          30h


    kubectl describe pod \
        --namespace ceph-csi-cephfs \
        ceph-csi-cephfs-nodeplugin-zz2hw

    >   ....
    >   Containers:
    >     driver-registrar:
    >       Container ID:  docker://d7a807b4c910bb68b666885dfb76263f5178bdd5186d594e7b5f314861a5f321
    >       Image:         quay.io/k8scsi/csi-node-driver-registrar:v1.3.0
    >       ....
    >       Args:
    >         --v=5
    >         --csi-address=/csi/csi.sock
    >         --kubelet-registration-path=/var/lib/kubelet/plugins/cephfs.csi.ceph.com/csi.sock
    >       ....
    >       Environment:
    >         KUBE_NODE_NAME:   (v1:spec.nodeName)
    >       Mounts:
    >         /csi from socket-dir (rw)
    >         /registration from registration-dir (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from ceph-csi-cephfs-nodeplugin-token-f5dch (ro)
    >   
    >     csi-cephfsplugin:
    >       Container ID:  docker://2c88114e3f5a9271576444e9a50cfb7772498849c6b1fc26ba5e790a65bfce57
    >       Image:         quay.io/cephcsi/cephcsi:v3.1.0
    >       ....
    >       Args:
    >         --nodeid=$(NODE_ID)
    >         --type=cephfs
    >         --nodeserver=true
    >         --pidlimit=-1
    >         --endpoint=$(CSI_ENDPOINT)
    >         --v=5
    >         --drivername=$(DRIVER_NAME)
    >       ....
    >       Environment:
    >         POD_IP:         (v1:status.podIP)
    >         DRIVER_NAME:   cephfs.csi.ceph.com
    >         NODE_ID:        (v1:spec.nodeName)
    >         CSI_ENDPOINT:  unix:///csi/csi.sock
    >       Mounts:
    >         /csi from socket-dir (rw)
    >         /dev from host-dev (rw)
    >         /etc/ceph-csi-config/ from ceph-csi-config (rw)
    >         /lib/modules from lib-modules (ro)
    >         /run/mount from host-mount (rw)
    >         /sys from host-sys (rw)
    >         /tmp/csi/keys from keys-tmp-dir (rw)
    >         /var/lib/kubelet/plugins from plugin-dir (rw)
    >         /var/lib/kubelet/pods from mountpoint-dir (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from ceph-csi-cephfs-nodeplugin-token-f5dch (ro)
    >   
    >     liveness-prometheus:
    >       Container ID:  docker://ea93113346b66979575fa1b82daa6020fd7c8dcaa2060276ba94084e674ea032
    >       Image:         quay.io/cephcsi/cephcsi:v3.1.0
    >       ....
    >       Args:
    >         --type=liveness
    >         --endpoint=$(CSI_ENDPOINT)
    >         --metricsport=8081
    >         --metricspath=/metrics
    >         --polltime=60s
    >         --timeout=3s
    >       ....
    >       Environment:
    >         CSI_ENDPOINT:  unix:///csi/csi.sock
    >         POD_IP:         (v1:status.podIP)
    >       Mounts:
    >         /csi from socket-dir (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from ceph-csi-cephfs-nodeplugin-token-f5dch (ro)
    >   ....
    >   Volumes:
    >     socket-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/lib/kubelet/plugins/cephfs.csi.ceph.com
    >       HostPathType:  DirectoryOrCreate
    >     registration-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/lib/kubelet/plugins_registry
    >       HostPathType:  Directory
    >     mountpoint-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/lib/kubelet/pods
    >       HostPathType:  DirectoryOrCreate
    >     plugin-dir:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /var/lib/kubelet/plugins
    >       HostPathType:  Directory
    >     host-sys:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /sys
    >       HostPathType:
    >     host-mount:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /run/mount
    >       HostPathType:
    >     lib-modules:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /lib/modules
    >       HostPathType:
    >     host-dev:
    >       Type:          HostPath (bare host directory volume)
    >       Path:          /dev
    >       HostPathType:
    >     ceph-csi-config:
    >       Type:      ConfigMap (a volume populated by a ConfigMap)
    >       Name:      ceph-csi-config-cephfs
    >       Optional:  false
    >     keys-tmp-dir:
    >       Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    >       Medium:     Memory
    >       SizeLimit:  <unset>
    >     ceph-csi-cephfs-nodeplugin-token-f5dch:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  ceph-csi-cephfs-nodeplugin-token-f5dch
    >       Optional:    false
    >   ....

    #
    # So .. should these two Pods be able to talk to each other ?
    # What is not clear - does the Manila plugin contain all the CephFS code it needs ?
    # Or does the Manila plugin depend on the CephFS plugin ?
    #



# -----------------------------------------------------
# Take a look at the Manila CSI nodeplugin logs ....
#[user@kubernator]

    kubectl logs \
        csi-manila-cephfs-nodeplugin-ll4cv \
            --container cephfs-nodeplugin

    >   
    >   I0901 11:39:10.272432       1 driver.go:124] Driver: cephfs.manila.csi.openstack.org
    >   I0901 11:39:10.272503       1 driver.go:125] Driver version: 0.9.0@latest
    >   I0901 11:39:10.272507       1 driver.go:126] CSI spec version: 1.2.0
    >   I0901 11:39:10.272512       1 driver.go:129] Operating on CEPHFS shares
    >   I0901 11:39:10.272524       1 driver.go:134] Topology awareness disabled
    >   I0901 11:39:10.272553       1 driver.go:197] Enabling controller service capability: CREATE_DELETE_VOLUME
    >   I0901 11:39:10.272560       1 driver.go:197] Enabling controller service capability: CREATE_DELETE_SNAPSHOT
    >   I0901 11:39:10.272564       1 driver.go:216] Enabling volume access mode: MULTI_NODE_MULTI_WRITER
    >   I0901 11:39:10.272570       1 driver.go:216] Enabling volume access mode: MULTI_NODE_SINGLE_WRITER
    >   I0901 11:39:10.272573       1 driver.go:216] Enabling volume access mode: MULTI_NODE_READER_ONLY
    >   I0901 11:39:10.272579       1 driver.go:216] Enabling volume access mode: SINGLE_NODE_WRITER
    >   I0901 11:39:10.272582       1 driver.go:216] Enabling volume access mode: SINGLE_NODE_READER_ONLY
    >   I0901 11:39:10.273275       1 connection.go:261] Probing CSI driver for readiness
    >   I0901 11:39:10.273322       1 builder.go:39] [ID:1] FWD GRPC call: /csi.v1.Identity/Probe
    >   I0901 11:39:10.273337       1 builder.go:40] [ID:1] FWD GRPC request: {}
    >   I0901 11:39:10.275948       1 builder.go:46] [ID:1] FWD GRPC response: {}
    >   I0901 11:39:10.276476       1 builder.go:39] [ID:2] FWD GRPC call: /csi.v1.Identity/GetPluginInfo
    >   I0901 11:39:10.276487       1 builder.go:40] [ID:2] FWD GRPC request: {}
    >   I0901 11:39:10.277978       1 builder.go:46] [ID:2] FWD GRPC response: {"name":"cephfs.csi.ceph.com","vendor_version":"v3.1.0"}
    >   I0901 11:39:10.280169       1 driver.go:262] proxying CSI driver cephfs.csi.ceph.com version v3.1.0
    >   I0901 11:39:10.280183       1 builder.go:39] [ID:3] FWD GRPC call: /csi.v1.Node/NodeGetCapabilities
    >   I0901 11:39:10.280194       1 builder.go:40] [ID:3] FWD GRPC request: {}
    >   I0901 11:39:10.283155       1 builder.go:46] [ID:3] FWD GRPC response: {"capabilities":[{"Type":{"Rpc":{"type":1}}},{"Type":{"Rpc":{"type":2}}}]}
    >   I0901 11:39:10.284286       1 driver.go:227] Enabling node service capability: STAGE_UNSTAGE_VOLUME
    >   I0901 11:39:10.284301       1 driver.go:227] Enabling node service capability: GET_VOLUME_STATS
    >   I0901 11:39:10.284644       1 driver.go:326] listening for connections on &net.UnixAddr{Name:"/var/lib/kubelet/plugins/cephfs.manila.csi.openstack.org/csi.sock", Net:"unix"}
    >   I0901 11:39:10.777113       1 driver.go:309] [ID:1] GRPC call: /csi.v1.Identity/GetPluginInfo
    >   I0901 11:39:10.777134       1 driver.go:310] [ID:1] GRPC request: {}
    >   I0901 11:39:10.777691       1 driver.go:315] [ID:1] GRPC response: {"name":"cephfs.manila.csi.openstack.org","vendor_version":"0.9.0@latest"}
    >   I0901 11:39:11.361385       1 driver.go:309] [ID:2] GRPC call: /csi.v1.Node/NodeGetInfo
    >   I0901 11:39:11.361411       1 driver.go:310] [ID:2] GRPC request: {}
    >   I0901 11:39:11.361948       1 driver.go:315] [ID:2] GRPC response: {"node_id":"tiberius-20200901-tzufyez4qfkn-node-0"}

    #
    # That looks like it managed to contact the CephFS driver :-)
    #


# -----------------------------------------------------
# Take a look at the CephFS CSI nodeplugin logs ....
#[user@kubernator]

    kubectl logs \
        --namespace ceph-csi-cephfs \
        ceph-csi-cephfs-nodeplugin-zz2hw \
            --container csi-cephfsplugin

    >   I0901 11:32:45.503255       1 cephcsi.go:123] Driver version: v3.1.0 and Git version: 5d48473582a31c21d9080d1d824db97ebf4a7a80
    >   I0901 11:32:45.503716       1 cephcsi.go:141] Initial PID limit is set to -1
    >   I0901 11:32:45.503769       1 cephcsi.go:150] Reconfigured PID limit to -1 (max)
    >   I0901 11:32:45.503778       1 cephcsi.go:169] Starting driver type: cephfs with name: cephfs.csi.ceph.com
    >   I0901 11:32:45.513303       1 volumemounter.go:87] loaded mounter: kernel
    >   I0901 11:32:45.522313       1 volumemounter.go:98] loaded mounter: fuse
    >   I0901 11:32:45.522869       1 server.go:118] Listening for connections on address: &net.UnixAddr{Name:"//csi/csi.sock", Net:"unix"}
    >   I0901 11:32:46.279785       1 utils.go:159] ID: 1 GRPC call: /csi.v1.Identity/GetPluginInfo
    >   I0901 11:32:46.280874       1 utils.go:160] ID: 1 GRPC request: {}
    >   I0901 11:32:46.280894       1 identityserver-default.go:36] ID: 1 Using default GetPluginInfo
    >   I0901 11:32:46.281415       1 utils.go:165] ID: 1 GRPC response: {"name":"cephfs.csi.ceph.com","vendor_version":"v3.1.0"}
    >   I0901 11:32:47.267419       1 utils.go:159] ID: 2 GRPC call: /csi.v1.Node/NodeGetInfo
    >   I0901 11:32:47.268065       1 utils.go:160] ID: 2 GRPC request: {}
    >   I0901 11:32:47.268089       1 nodeserver-default.go:57] ID: 2 Using default NodeGetInfo
    >   I0901 11:32:47.268837       1 utils.go:165] ID: 2 GRPC response: {"accessible_topology":{},"node_id":"tiberius-20200901-tzufyez4qfkn-node-0"}
    >   I0901 11:33:45.571422       1 utils.go:159] ID: 3 GRPC call: /csi.v1.Identity/Probe
    >   I0901 11:33:45.572063       1 utils.go:160] ID: 3 GRPC request: {}
    >   I0901 11:33:45.572613       1 utils.go:165] ID: 3 GRPC response: {}
    >   I0901 11:34:45.570855       1 utils.go:159] ID: 4 GRPC call: /csi.v1.Identity/Probe
    >   I0901 11:34:45.571399       1 utils.go:160] ID: 4 GRPC request: {}
    >   I0901 11:34:45.571795       1 utils.go:165] ID: 4 GRPC response: {}
    >   I0901 11:35:45.572294       1 utils.go:159] ID: 5 GRPC call: /csi.v1.Identity/Probe
    >   I0901 11:35:45.573483       1 utils.go:160] ID: 5 GRPC request: {}
    >   I0901 11:35:45.574026       1 utils.go:165] ID: 5 GRPC response: {}
    >   I0901 11:36:45.570857       1 utils.go:159] ID: 6 GRPC call: /csi.v1.Identity/Probe
    >   I0901 11:36:45.571426       1 utils.go:160] ID: 6 GRPC request: {}
    >   I0901 11:36:45.571839       1 utils.go:165] ID: 6 GRPC response: {}
    >   I0901 11:37:45.571018       1 utils.go:159] ID: 7 GRPC call: /csi.v1.Identity/Probe
    >   I0901 11:37:45.571516       1 utils.go:160] ID: 7 GRPC request: {}
    >   I0901 11:37:45.571838       1 utils.go:165] ID: 7 GRPC response: {}
    >   ....
    >   ....
    >   I0901 11:39:10.277031       1 utils.go:159] ID: 10 GRPC call: /csi.v1.Identity/GetPluginInfo
    >   I0901 11:39:10.277421       1 utils.go:160] ID: 10 GRPC request: {}
    >   I0901 11:39:10.277449       1 identityserver-default.go:36] ID: 10 Using default GetPluginInfo
    >   I0901 11:39:10.277819       1 utils.go:165] ID: 10 GRPC response: {"name":"cephfs.csi.ceph.com","vendor_version":"v3.1.0"}
    >   I0901 11:39:10.281013       1 utils.go:159] ID: 11 GRPC call: /csi.v1.Node/NodeGetCapabilities
    >   I0901 11:39:10.281503       1 utils.go:160] ID: 11 GRPC request: {}
    >   I0901 11:39:10.282909       1 utils.go:165] ID: 11 GRPC response: {"capabilities":[{"Type":{"Rpc":{"type":1}}},{"Type":{"Rpc":{"type":2}}}]}
    >   I0901 11:39:45.570746       1 utils.go:159] ID: 12 GRPC call: /csi.v1.Identity/Probe
    >   I0901 11:39:45.571275       1 utils.go:160] ID: 12 GRPC request: {}
    >   I0901 11:39:45.571808       1 utils.go:165] ID: 12 GRPC response: {}
    >   I0901 11:40:45.571468       1 utils.go:159] ID: 13 GRPC call: /csi.v1.Identity/Probe
    >   I0901 11:40:45.573181       1 utils.go:160] ID: 13 GRPC request: {}
    >   I0901 11:40:45.573712       1 utils.go:165] ID: 13 GRPC response: {}
    >   I0901 11:41:45.571432       1 utils.go:159] ID: 14 GRPC call: /csi.v1.Identity/Probe
    >   I0901 11:41:45.572391       1 utils.go:160] ID: 14 GRPC request: {}
    >   I0901 11:41:45.572891       1 utils.go:165] ID: 14 GRPC response: {}
    >   I0901 11:42:45.570822       1 utils.go:159] ID: 15 GRPC call: /csi.v1.Identity/Probe
    >   ....
    >   ....


# -----------------------------------------------------
# Take a look at a Manila CSI controllerplugin Pod ....
#[user@kubernator]

    kubectl get pods

    >   NAME                                                        READY   STATUS              RESTARTS   AGE
    >   augusta-20200901-ingress-nginx-controller-76c7d4c9c-5jkq7   1/1     Running             0          29h
    >   csi-manila-cephfs-controllerplugin-0                        3/3     Running             0          29h
    >   csi-manila-cephfs-nodeplugin-2kd8l                          2/2     Running             0          29h
    >   csi-manila-cephfs-nodeplugin-f8fv8                          2/2     Running             0          29h
    >   csi-manila-cephfs-nodeplugin-ll4cv                          2/2     Running             0          29h
    >   csi-manila-cephfs-nodeplugin-s62r4                          2/2     Running             0          29h
    >   magruela-pod                                                0/1     ContainerCreating   0          3h1m
    >   valeria-20200901-kubernetes-dashboard-6f65478dfd-4fgcn      2/2     Running             0          29h


    kubectl describe pod \
        csi-manila-cephfs-controllerplugin-0

    >   ....
    >   Containers:
    >     cephfs-provisioner:
    >       Container ID:  docker://357aefb759bc06471c64e8d5f167e10bfcb2eed3cb09f7101c4dbc5754ab9d77
    >       Image:         quay.io/k8scsi/csi-provisioner:v1.4.0
    >       ....
    >       Args:
    >         --v=5
    >         --csi-address=$(ADDRESS)
    >       ....
    >       Environment:
    >         ADDRESS:  unix:///var/lib/kubelet/plugins/cephfs.manila.csi.openstack.org/csi-controllerplugin.sock
    >       Mounts:
    >         /var/lib/kubelet/plugins/cephfs.manila.csi.openstack.org from cephfs-plugin-dir (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from csi-manila-cephfs-controllerplugin-token-c8lhh (ro)
    >     cephfs-snapshotter:
    >       Container ID:  docker://eee8bbc61b309c02f3b632fddccf6e0482c8de550195ec6a2432bf990effa15f
    >       Image:         quay.io/k8scsi/csi-snapshotter:v1.2.2
    >       ....
    >       Args:
    >         --v=5
    >         --csi-address=$(ADDRESS)
    >       ....
    >       Environment:
    >         ADDRESS:  unix:///var/lib/kubelet/plugins/cephfs.manila.csi.openstack.org/csi-controllerplugin.sock
    >       Mounts:
    >         /var/lib/kubelet/plugins/cephfs.manila.csi.openstack.org from cephfs-plugin-dir (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from csi-manila-cephfs-controllerplugin-token-c8lhh (ro)
    >     cephfs-nodeplugin:
    >       Container ID:  docker://96828d18e385c64dfaa3250b4f175564b65dc6c479be635d4cadbcfe50084038
    >       Image:         k8scloudprovider/manila-csi-plugin:latest
    >       ....
    >       Command:
    >         /bin/sh
    >         -c
    >         /bin/manila-csi-plugin --v=5 --nodeid=$(NODE_ID) --endpoint=$(CSI_ENDPOINT) --drivername=$(DRIVER_NAME) --share-protocol-selector=$(MANILA_SHARE_PROTO) --fwdendpoint=$(FWD_CSI_ENDPOINT)
    >       ....
    >       Environment:
    >         DRIVER_NAME:         cephfs.manila.csi.openstack.org
    >         NODE_ID:              (v1:spec.nodeName)
    >         CSI_ENDPOINT:        unix:///var/lib/kubelet/plugins/cephfs.manila.csi.openstack.org/csi-controllerplugin.sock
    >         FWD_CSI_ENDPOINT:    unix:///var/lib/kubelet/plugins/cephfs.csi.ceph.com/csi.sock
    >         MANILA_SHARE_PROTO:  CEPHFS
    >       Mounts:
    >         /var/lib/kubelet/plugins/cephfs.csi.ceph.com from cephfs-fwd-plugin-dir (rw)
    >         /var/lib/kubelet/plugins/cephfs.manila.csi.openstack.org from cephfs-plugin-dir (rw)
    >         /var/lib/kubelet/pods from pod-mounts (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from csi-manila-cephfs-controllerplugin-token-c8lhh (ro)
    >   ....


# -----------------------------------------------------
# Take a look at the Manila CSI controllerplugin logs ....
#[user@kubernator]

    kubectl logs \
        csi-manila-cephfs-controllerplugin-0 \
        --container cephfs-nodeplugin

    >   I0901 11:39:16.991411       1 driver.go:124] Driver: cephfs.manila.csi.openstack.org
    >   I0901 11:39:16.991457       1 driver.go:125] Driver version: 0.9.0@latest
    >   I0901 11:39:16.991462       1 driver.go:126] CSI spec version: 1.2.0
    >   I0901 11:39:16.991466       1 driver.go:129] Operating on CEPHFS shares
    >   I0901 11:39:16.991471       1 driver.go:134] Topology awareness disabled
    >   I0901 11:39:16.991585       1 driver.go:197] Enabling controller service capability: CREATE_DELETE_VOLUME
    >   I0901 11:39:16.991596       1 driver.go:197] Enabling controller service capability: CREATE_DELETE_SNAPSHOT
    >   I0901 11:39:16.991600       1 driver.go:216] Enabling volume access mode: MULTI_NODE_MULTI_WRITER
    >   I0901 11:39:16.991605       1 driver.go:216] Enabling volume access mode: MULTI_NODE_SINGLE_WRITER
    >   I0901 11:39:16.991608       1 driver.go:216] Enabling volume access mode: MULTI_NODE_READER_ONLY
    >   I0901 11:39:16.991611       1 driver.go:216] Enabling volume access mode: SINGLE_NODE_WRITER
    >   I0901 11:39:16.991617       1 driver.go:216] Enabling volume access mode: SINGLE_NODE_READER_ONLY
    >   I0901 11:39:16.992267       1 connection.go:261] Probing CSI driver for readiness
    >   I0901 11:39:16.992282       1 builder.go:39] [ID:1] FWD GRPC call: /csi.v1.Identity/Probe
    >   I0901 11:39:16.992290       1 builder.go:40] [ID:1] FWD GRPC request: {}
    >   I0901 11:39:16.995232       1 builder.go:46] [ID:1] FWD GRPC response: {}
    >   I0901 11:39:16.995705       1 builder.go:39] [ID:2] FWD GRPC call: /csi.v1.Identity/GetPluginInfo
    >   I0901 11:39:16.995710       1 builder.go:40] [ID:2] FWD GRPC request: {}
    >   I0901 11:39:16.997088       1 builder.go:46] [ID:2] FWD GRPC response: {"name":"cephfs.csi.ceph.com","vendor_version":"v3.1.0"}
    >   I0901 11:39:16.997613       1 driver.go:262] proxying CSI driver cephfs.csi.ceph.com version v3.1.0
    >   I0901 11:39:16.997621       1 builder.go:39] [ID:3] FWD GRPC call: /csi.v1.Node/NodeGetCapabilities
    >   I0901 11:39:16.997628       1 builder.go:40] [ID:3] FWD GRPC request: {}
    >   I0901 11:39:17.001171       1 builder.go:46] [ID:3] FWD GRPC response: {"capabilities":[{"Type":{"Rpc":{"type":1}}},{"Type":{"Rpc":{"type":2}}}]}
    >   I0901 11:39:17.002299       1 driver.go:227] Enabling node service capability: STAGE_UNSTAGE_VOLUME
    >   I0901 11:39:17.002361       1 driver.go:227] Enabling node service capability: GET_VOLUME_STATS
    >   I0901 11:39:17.002686       1 driver.go:326] listening for connections on &net.UnixAddr{Name:"/var/lib/kubelet/plugins/cephfs.manila.csi.openstack.org/csi-controllerplugin.sock", Net:"unix"}
    >   
    >   I0901 11:39:17.510574       1 driver.go:309] [ID:1] GRPC call: /csi.v1.Identity/Probe
    >   I0901 11:39:17.510610       1 driver.go:310] [ID:1] GRPC request: {}
    >   
    >   I0901 11:39:17.511508       1 builder.go:39] [ID:4] FWD GRPC call: /csi.v1.Identity/Probe
    >   I0901 11:39:17.511521       1 builder.go:40] [ID:4] FWD GRPC request: {}
    >   I0901 11:39:17.512912       1 builder.go:46] [ID:4] FWD GRPC response: {}
    >   I0901 11:39:17.513222       1 driver.go:315] [ID:1] GRPC response: {}
    >   
    >   I0901 11:39:17.514624       1 driver.go:309] [ID:2] GRPC call: /csi.v1.Identity/GetPluginInfo
    >   I0901 11:39:17.514668       1 driver.go:310] [ID:2] GRPC request: {}
    >   I0901 11:39:17.514978       1 driver.go:315] [ID:2] GRPC response: {"name":"cephfs.manila.csi.openstack.org","vendor_version":"0.9.0@latest"}
    >   
    >   I0901 11:39:17.517484       1 driver.go:309] [ID:3] GRPC call: /csi.v1.Identity/GetPluginCapabilities
    >   I0901 11:39:17.517518       1 driver.go:310] [ID:3] GRPC request: {}
    >   I0901 11:39:17.517892       1 driver.go:315] [ID:3] GRPC response: {"capabilities":[{"Type":{"Service":{"type":1}}}]}
    >   
    >   I0901 11:39:17.521783       1 driver.go:309] [ID:4] GRPC call: /csi.v1.Controller/ControllerGetCapabilities
    >   I0901 11:39:17.521805       1 driver.go:310] [ID:4] GRPC request: {}
    >   I0901 11:39:17.522174       1 driver.go:315] [ID:4] GRPC response: {"capabilities":[{"Type":{"Rpc":{"type":1}}},{"Type":{"Rpc":{"type":5}}}]}
    >   
    >   I0901 11:39:17.938830       1 driver.go:309] [ID:5] GRPC call: /csi.v1.Identity/GetPluginInfo
    >   I0901 11:39:17.938850       1 driver.go:310] [ID:5] GRPC request: {}
    >   I0901 11:39:17.939302       1 driver.go:315] [ID:5] GRPC response: {"name":"cephfs.manila.csi.openstack.org","vendor_version":"0.9.0@latest"}
    >   I0901 11:39:17.940670       1 driver.go:309] [ID:6] GRPC call: /csi.v1.Identity/Probe
    >   I0901 11:39:17.940680       1 driver.go:310] [ID:6] GRPC request: {}
    >   
    >   I0901 11:39:17.942283       1 builder.go:39] [ID:5] FWD GRPC call: /csi.v1.Identity/Probe
    >   I0901 11:39:17.942296       1 builder.go:40] [ID:5] FWD GRPC request: {}
    >   I0901 11:39:17.944425       1 builder.go:46] [ID:5] FWD GRPC response: {}
    >   I0901 11:39:17.944824       1 driver.go:315] [ID:6] GRPC response: {}
    >   
    >   I0901 11:39:17.946312       1 driver.go:309] [ID:7] GRPC call: /csi.v1.Controller/ControllerGetCapabilities
    >   I0901 11:39:17.946327       1 driver.go:310] [ID:7] GRPC request: {}
    >   I0901 11:39:17.946710       1 driver.go:315] [ID:7] GRPC response: {"capabilities":[{"Type":{"Rpc":{"type":1}}},{"Type":{"Rpc":{"type":5}}}]}
    >   
    >   I0902 15:27:02.087574       1 driver.go:309] [ID:8] GRPC call: /csi.v1.Controller/CreateVolume
    >   I0902 15:27:02.087628       1 driver.go:310] [ID:8] GRPC request: {"capacity_range":{"required_bytes":1073741824},"name":"pvc-cfe06796-7137-4413-a549-ee2d69f86acc","parameters":{"type":"default"},"secrets":"***stripped***","volume_capabilities":[{"AccessType":{"Mount":{"fs_type":"ext4"}},"access_mode":{"mode":5}}]}
    >   I0902 15:27:02.089691       1 openstack.go:505] Using user-agent manila-csi-plugin/latest gophercloud/2.0.0
    >   E0902 15:27:02.179832       1 driver.go:313] [ID:8] GRPC error: rpc error: code = Unauthenticated desc = failed to create Manila v2 client: failed to authenticate: Authentication failed
    >   
    >   I0902 15:27:02.187598       1 driver.go:309] [ID:9] GRPC call: /csi.v1.Controller/CreateVolume
    >   I0902 15:27:02.187673       1 driver.go:310] [ID:9] GRPC request: {"capacity_range":{"required_bytes":1073741824},"name":"pvc-cfe06796-7137-4413-a549-ee2d69f86acc","parameters":{"type":"default"},"secrets":"***stripped***","volume_capabilities":[{"AccessType":{"Mount":{"fs_type":"ext4"}},"access_mode":{"mode":5}}]}
    >   I0902 15:27:02.189172       1 openstack.go:505] Using user-agent manila-csi-plugin/latest gophercloud/2.0.0
    >   E0902 15:27:02.213513       1 driver.go:313] [ID:9] GRPC error: rpc error: code = Unauthenticated desc = failed to create Manila v2 client: failed to authenticate: Authentication failed
    >   ....
    >   ....
    >   ....
    >   ....
    >   I0902 15:45:34.235405       1 driver.go:309] [ID:19] GRPC call: /csi.v1.Controller/CreateVolume
    >   I0902 15:45:34.235437       1 driver.go:310] [ID:19] GRPC request: {"capacity_range":{"required_bytes":1073741824},"name":"pvc-e8902c45-e5a7-4213-beb9-daf06aa6ee13","parameters":{"type":"default"},"secrets":"***stripped***","volume_capabilities":[{"AccessType":{"Mount":{"fs_type":"ext4"}},"access_mode":{"mode":5}}]}
    >   E0902 15:45:34.236944       1 driver.go:313] [ID:19] GRPC error: rpc error: code = InvalidArgument desc = invalid OpenStack secrets: parameter 'os-authURL' requires exactly one of [os-password os-trustID] parameters
    >   
    >   I0902 15:45:35.243589       1 driver.go:309] [ID:20] GRPC call: /csi.v1.Controller/CreateVolume
    >   I0902 15:45:35.243647       1 driver.go:310] [ID:20] GRPC request: {"capacity_range":{"required_bytes":1073741824},"name":"pvc-e8902c45-e5a7-4213-beb9-daf06aa6ee13","parameters":{"type":"default"},"secrets":"***stripped***","volume_capabilities":[{"AccessType":{"Mount":{"fs_type":"ext4"}},"access_mode":{"mode":5}}]}
    >   E0902 15:45:35.245283       1 driver.go:313] [ID:20] GRPC error: rpc error: code = InvalidArgument desc = invalid OpenStack secrets: parameter 'os-authURL' requires exactly one of [os-password os-trustID] parameters
    >   
    >   I0902 15:45:37.254259       1 driver.go:309] [ID:21] GRPC call: /csi.v1.Controller/CreateVolume
    >   I0902 15:45:37.254285       1 driver.go:310] [ID:21] GRPC request: {"capacity_range":{"required_bytes":1073741824},"name":"pvc-e8902c45-e5a7-4213-beb9-daf06aa6ee13","parameters":{"type":"default"},"secrets":"***stripped***","volume_capabilities":[{"AccessType":{"Mount":{"fs_type":"ext4"}},"access_mode":{"mode":5}}]}
    >   E0902 15:45:37.256214       1 driver.go:313] [ID:21] GRPC error: rpc error: code = InvalidArgument desc = invalid OpenStack secrets: parameter 'os-authURL' requires exactly one of [os-password os-trustID] parameters
    >   
    >   I0902 15:45:41.265284       1 driver.go:309] [ID:22] GRPC call: /csi.v1.Controller/CreateVolume
    >   I0902 15:45:41.265314       1 driver.go:310] [ID:22] GRPC request: {"capacity_range":{"required_bytes":1073741824},"name":"pvc-e8902c45-e5a7-4213-beb9-daf06aa6ee13","parameters":{"type":"default"},"secrets":"***stripped***","volume_capabilities":[{"AccessType":{"Mount":{"fs_type":"ext4"}},"access_mode":{"mode":5}}]}
    >   E0902 15:45:41.268096       1 driver.go:313] [ID:22] GRPC error: rpc error: code = InvalidArgument desc = invalid OpenStack secrets: parameter 'os-authURL' requires exactly one of [os-password os-trustID] parameters
    >   ....
    >   ....
    >   ....
    >   ....
    >   I0902 16:33:51.167204       1 driver.go:309] [ID:44] GRPC call: /csi.v1.Controller/CreateVolume
    >   I0902 16:33:51.167235       1 driver.go:310] [ID:44] GRPC request: {"capacity_range":{"required_bytes":1073741824},"name":"pvc-c8eb485f-93aa-4bc9-87d9-6ee514a96d35","parameters":{"type":"cephfsnativetype"},"secrets":"***stripped***","volume_capabilities":[{"AccessType":{"Mount":{"fs_type":"ext4"}},"access_mode":{"mode":5}}]}
    >   I0902 16:33:51.168963       1 openstack.go:505] Using user-agent manila-csi-plugin/latest gophercloud/2.0.0
    >   E0902 16:33:51.207406       1 driver.go:313] [ID:44] GRPC error: rpc error: code = Unauthenticated desc = failed to create Manila v2 client: failed to authenticate: Authentication failed
    >   
    >   I0902 16:33:51.213182       1 driver.go:309] [ID:45] GRPC call: /csi.v1.Controller/CreateVolume
    >   I0902 16:33:51.213196       1 driver.go:310] [ID:45] GRPC request: {"capacity_range":{"required_bytes":1073741824},"name":"pvc-c8eb485f-93aa-4bc9-87d9-6ee514a96d35","parameters":{"type":"cephfsnativetype"},"secrets":"***stripped***","volume_capabilities":[{"AccessType":{"Mount":{"fs_type":"ext4"}},"access_mode":{"mode":5}}]}
    >   I0902 16:33:51.215654       1 openstack.go:505] Using user-agent manila-csi-plugin/latest gophercloud/2.0.0
    >   E0902 16:33:51.252417       1 driver.go:313] [ID:45] GRPC error: rpc error: code = Unauthenticated desc = failed to create Manila v2 client: failed to authenticate: Authentication failed
    >   
    >   I0902 16:33:52.214981       1 driver.go:309] [ID:46] GRPC call: /csi.v1.Controller/CreateVolume
    >   I0902 16:33:52.215001       1 driver.go:310] [ID:46] GRPC request: {"capacity_range":{"required_bytes":1073741824},"name":"pvc-c8eb485f-93aa-4bc9-87d9-6ee514a96d35","parameters":{"type":"cephfsnativetype"},"secrets":"***stripped***","volume_capabilities":[{"AccessType":{"Mount":{"fs_type":"ext4"}},"access_mode":{"mode":5}}]}
    >   I0902 16:33:52.216616       1 openstack.go:505] Using user-agent manila-csi-plugin/latest gophercloud/2.0.0
    >   E0902 16:33:52.240526       1 driver.go:313] [ID:46] GRPC error: rpc error: code = Unauthenticated desc = failed to create Manila v2 client: failed to authenticate: Authentication failed
    >   ....
    >   ....
    >   ....
    >   ....

    #
    # Different errors from different experiments.
    # I think the later ones are from the username/password failing.
    #




