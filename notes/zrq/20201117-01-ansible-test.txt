#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#

    Target:

        Run the Ansible deploy

    Result:

        Success


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "clouduser=${AGLAIS_USER:?}" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/openstack:/openstack:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/hadoop-yarn:/hadoop-yarn:ro,z" \
        atolmis/ansible-client:latest \
        bash

# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    /openstack/bin/delete-all.sh

    >   ....
    >   ....


# -----------------------------------------------------
# Run the main Ansible deployment.
#[root@ansibler]

    /hadoop-yarn/bin/create-all.sh

    >   ....
    >   ....


# -----------------------------------------------------
# Format the HDFS NameNode on master01.
#[root@ansibler]

    ssh master01 \
        '
        hdfs namenode -format
        '

    >   2020-11-17 06:09:54,941 INFO namenode.NameNode: STARTUP_MSG:
    >   /..
    >   STARTUP_MSG: Starting NameNode
    >   STARTUP_MSG:   host = master01/10.10.2.33
    >   STARTUP_MSG:   args = [-format]
    >   STARTUP_MSG:   version = 3.2.1
    >   ....
    >   ....
    >   2020-11-17 06:09:55,712 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
    >   2020-11-17 06:09:55,712 INFO namenode.NameNode: SHUTDOWN_MSG:
    >   /..
    >   SHUTDOWN_MSG: Shutting down NameNode at master01/10.10.2.33
    >   ************************************************************/


# -----------------------------------------------------
# Start the HDFS services.
#[root@ansibler]

    ssh master01 \
        '
        start-dfs.sh
        '

    >   Starting namenodes on [master01]
    >   Starting datanodes
    >   Starting secondary namenodes [aglais-20201117-master01.novalocal]
    >   aglais-20201117-master01.novalocal: Warning: Permanently added 'aglais-20201117-master01.novalocal,fe80::f816:3eff:fe42:18e%eth0' (ECDSA) to the list of known hosts.


# -----------------------------------------------------
# Check the HDFS status.
#[root@ansibler]

    ssh master01 \
        '
        hdfs dfsadmin -report
        '

    >   ....
    >   ....
    >   Name: 10.10.3.91:9866 (worker03)
    >   Hostname: worker03
    >   Decommission Status : Normal
    >   Configured Capacity: 549755813888 (512 GB)
    >   DFS Used: 4096 (4 KB)
    >   Non DFS Used: 17297408 (16.50 MB)
    >   DFS Remaining: 547590242304 (509.98 GB)
    >   DFS Used%: 0.00%
    >   DFS Remaining%: 99.61%
    >   Configured Cache Capacity: 0 (0 B)
    >   Cache Used: 0 (0 B)
    >   Cache Remaining: 0 (0 B)
    >   Cache Used%: 100.00%
    >   Cache Remaining%: 0.00%
    >   Xceivers: 1
    >   Last contact: Tue Nov 17 06:11:22 UTC 2020
    >   Last Block Report: Tue Nov 17 06:10:55 UTC 2020
    >   Num of Blocks: 0


# -----------------------------------------------------
# Create our HDFS log directory.
#[root@ansibler]

    ssh master01 \
        '
        hdfs dfs -mkdir /spark-log
        '


# -----------------------------------------------------
# Start the YARN services.
#[root@ansibler]

    ssh master01 \
        '
        start-yarn.sh
        '

    >   Starting resourcemanager
    >   Starting nodemanagers


# -----------------------------------------------------
# Run the SparkPi example from the Spark install instructtions.
# https://spark.apache.org/docs/3.0.0-preview2/running-on-yarn.html#launching-spark-on-yarn
#[root@ansibler]

    ssh master01 \
        '
        cd "${SPARK_HOME:?}"

        spark-submit \
            --class org.apache.spark.examples.SparkPi \
            --master yarn \
            --deploy-mode cluster \
            --driver-memory 1g \
            --executor-memory 1g \
            --executor-cores 1 \
            examples/jars/spark-examples*.jar \
                10
        '

    >   2020-11-17 06:13:43,427 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    >   2020-11-17 06:13:43,485 INFO client.RMProxy: Connecting to ResourceManager at master01/10.10.2.33:8032
    >   2020-11-17 06:13:43,740 INFO yarn.Client: Requesting a new application from cluster with 8 NodeManagers
    >   2020-11-17 06:13:44,155 INFO conf.Configuration: resource-types.xml not found
    >   2020-11-17 06:13:44,156 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
    >   2020-11-17 06:13:44,171 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
    >   2020-11-17 06:13:44,172 INFO yarn.Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead
    >   2020-11-17 06:13:44,172 INFO yarn.Client: Setting up container launch context for our AM
    >   2020-11-17 06:13:44,173 INFO yarn.Client: Setting up the launch environment for our AM container
    >   2020-11-17 06:13:44,180 INFO yarn.Client: Preparing resources for our AM container
    >   2020-11-17 06:13:44,215 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
    >   2020-11-17 06:13:46,240 INFO yarn.Client: Uploading resource file:/tmp/spark-649b21d8-fd79-40a2-b341-a7ce0a18ae9c/__spark_libs__742067473134630899.zip -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1605593508956_0001/__spark_libs__742067473134630899.zip
    >   2020-11-17 06:13:47,506 INFO yarn.Client: Uploading resource file:/opt/spark-3.0.1-bin-hadoop3.2/examples/jars/spark-examples_2.12-3.0.1.jar -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1605593508956_0001/spark-examples_2.12-3.0.1.jar
    >   2020-11-17 06:13:47,889 INFO yarn.Client: Uploading resource file:/tmp/spark-649b21d8-fd79-40a2-b341-a7ce0a18ae9c/__spark_conf__1684684612446701016.zip -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1605593508956_0001/__spark_conf__.zip
    >   2020-11-17 06:13:48,154 INFO spark.SecurityManager: Changing view acls to: fedora
    >   2020-11-17 06:13:48,158 INFO spark.SecurityManager: Changing modify acls to: fedora
    >   2020-11-17 06:13:48,158 INFO spark.SecurityManager: Changing view acls groups to:
    >   2020-11-17 06:13:48,159 INFO spark.SecurityManager: Changing modify acls groups to:
    >   2020-11-17 06:13:48,159 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fedora); groups with view permissions: Set(); users  with modify permissions: Set(fedora); groups with modify permissions: Set()
    >   2020-11-17 06:13:48,208 INFO yarn.Client: Submitting application application_1605593508956_0001 to ResourceManager
    >   2020-11-17 06:13:48,620 INFO impl.YarnClientImpl: Submitted application application_1605593508956_0001
    >   2020-11-17 06:13:49,626 INFO yarn.Client: Application report for application_1605593508956_0001 (state: ACCEPTED)
    >   2020-11-17 06:13:49,630 INFO yarn.Client:
    >   	 client token: N/A
    >   	 diagnostics: AM container is launched, waiting for AM container to Register with RM
    >   	 ApplicationMaster host: N/A
    >   	 ApplicationMaster RPC port: -1
    >   	 queue: default
    >   	 start time: 1605593628320
    >   	 final status: UNDEFINED
    >   	 tracking URL: http://master01:8088/proxy/application_1605593508956_0001/
    >   	 user: fedora
    >   2020-11-17 06:13:50,633 INFO yarn.Client: Application report for application_1605593508956_0001 (state: ACCEPTED)
    >   2020-11-17 06:13:51,636 INFO yarn.Client: Application report for application_1605593508956_0001 (state: ACCEPTED)
    >   2020-11-17 06:13:52,639 INFO yarn.Client: Application report for application_1605593508956_0001 (state: ACCEPTED)
    >   2020-11-17 06:13:53,642 INFO yarn.Client: Application report for application_1605593508956_0001 (state: ACCEPTED)
    >   2020-11-17 06:13:54,644 INFO yarn.Client: Application report for application_1605593508956_0001 (state: ACCEPTED)
    >   2020-11-17 06:13:55,647 INFO yarn.Client: Application report for application_1605593508956_0001 (state: ACCEPTED)
    >   2020-11-17 06:13:56,649 INFO yarn.Client: Application report for application_1605593508956_0001 (state: RUNNING)
    >   2020-11-17 06:13:56,650 INFO yarn.Client:
    >   	 client token: N/A
    >   	 diagnostics: N/A
    >   	 ApplicationMaster host: worker01
    >   	 ApplicationMaster RPC port: 36323
    >   	 queue: default
    >   	 start time: 1605593628320
    >   	 final status: UNDEFINED
    >   	 tracking URL: http://master01:8088/proxy/application_1605593508956_0001/
    >   	 user: fedora
    >   2020-11-17 06:13:57,652 INFO yarn.Client: Application report for application_1605593508956_0001 (state: RUNNING)
    >   2020-11-17 06:13:58,654 INFO yarn.Client: Application report for application_1605593508956_0001 (state: RUNNING)
    >   2020-11-17 06:13:59,657 INFO yarn.Client: Application report for application_1605593508956_0001 (state: RUNNING)
    >   2020-11-17 06:14:00,660 INFO yarn.Client: Application report for application_1605593508956_0001 (state: RUNNING)
    >   2020-11-17 06:14:01,662 INFO yarn.Client: Application report for application_1605593508956_0001 (state: RUNNING)
    >   2020-11-17 06:14:02,669 INFO yarn.Client: Application report for application_1605593508956_0001 (state: RUNNING)
    >   2020-11-17 06:14:03,674 INFO yarn.Client: Application report for application_1605593508956_0001 (state: RUNNING)
    >   2020-11-17 06:14:04,676 INFO yarn.Client: Application report for application_1605593508956_0001 (state: FINISHED)
    >   2020-11-17 06:14:04,677 INFO yarn.Client:
    >   	 client token: N/A
    >   	 diagnostics: N/A
    >   	 ApplicationMaster host: worker01
    >   	 ApplicationMaster RPC port: 36323
    >   	 queue: default
    >   	 start time: 1605593628320
    >   	 final status: SUCCEEDED
    >   	 tracking URL: http://master01:8088/proxy/application_1605593508956_0001/
    >   	 user: fedora
    >   2020-11-17 06:14:04,693 INFO yarn.Client: Deleted staging directory hdfs://master01:9000/user/fedora/.sparkStaging/application_1605593508956_0001
    >   2020-11-17 06:14:04,697 INFO util.ShutdownHookManager: Shutdown hook called
    >   2020-11-17 06:14:04,698 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-e47e9493-c7a9-40b8-9fc3-4aeb6017bc92
    >   2020-11-17 06:14:04,701 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-649b21d8-fd79-40a2-b341-a7ce0a18ae9c


# -----------------------------------------------------
# Create the Manila router.
#[root@ansibler]

    prefix=$(
        sed -n "
            s/^buildtag: *'\(.*\)'$/\1/p
            " /tmp/ansible-vars.yml
        )

    /openstack/bin/cephfs-router.sh \
        "${cloudname:?}" \
        "${prefix:?}"

    >   ---- ----
    >   Project [iris-gaia-prod][21b4ae3a2ea44bc5a9c14005ed2963af]
    >
    >   ---- ----
    >   Ceph router [ceph-router][f557744a-4937-4d17-893c-5d532fcb1f9c]
    >
    >   ---- ----
    >   Cluster subnet  [1ba9ac9b-0190-40a8-8b48-9d749fd3e2ad][aglais-20201117-internal-network-subnet]
    >   Cluster network [881c8fc8-f2e4-471f-888a-86f1ce037259]
    >
    >   ---- ----
    >   Cluster subnet port
    >   {
    >     "network_id": "881c8fc8-f2e4-471f-888a-86f1ce037259",
    >     "fixed_ips": [
    >       {
    >         "subnet_id": "1ba9ac9b-0190-40a8-8b48-9d749fd3e2ad",
    >         "ip_address": "10.10.0.110"
    >       }
    >     ]
    >   }
    >
    >   ---- ----
    >   Ceph router config
    >   {
    >     "external_gateway_info": {
    >       "network_id": "ecb791d5-1022-447a-a79c-8f38a0f5c990",
    >       "enable_snat": true,
    >       "external_fixed_ips": [
    >         {
    >           "subnet_id": "01b76c7c-3c1a-4c5c-9a5a-14bcf6c0c290",
    >           "ip_address": "10.218.1.172"
    >         }
    >       ]
    >     },
    >     "interfaces_info": [
    >       {
    >         "port_id": "25805b68-5b51-4001-8cdf-1c788da7471f",
    >         "ip_address": "10.10.0.110",
    >         "subnet_id": "1ba9ac9b-0190-40a8-8b48-9d749fd3e2ad"
    >       }
    >     ],
    >     "routes": []
    >   }
    >
    >   ---- ----
    >   Cluster router [aglais-20201117-internal-network-router][eaadf5dd-448b-48f0-85b7-34e10e1afc13]
    >
    >   ---- ----
    >   Cluster router config
    >   {
    >     "external_gateway_info": {
    >       "network_id": "a929e8db-1bf4-4a5f-a80c-fabd39d06a26",
    >       "enable_snat": true,
    >       "external_fixed_ips": [
    >         {
    >           "subnet_id": "273123bb-70f6-4f51-a406-7fc4b446532d",
    >           "ip_address": "128.232.227.150"
    >         }
    >       ]
    >     },
    >     "interfaces_info": [
    >       {
    >         "port_id": "d411d76e-0195-4922-9437-4b0645a9dfa9",
    >         "ip_address": "10.10.0.1",
    >         "subnet_id": "1ba9ac9b-0190-40a8-8b48-9d749fd3e2ad"
    >       }
    >     ],
    >     "routes": [
    >       {
    >         "nexthop": "10.10.0.110",
    >         "destination": "10.206.0.0/16"
    >       }
    >     ]
    >   }

# -----------------------------------------------------
# Mount the Gaia DR2 data.
#[root@ansibler]

    /hadoop-yarn/bin/cephfs-mount.sh \
        "${cloudname:?}" \
        'aglais-gaia-dr2' \
        '/data/gaia/dr2'

    >   Target [gaia-prod][aglais-gaia-dr2]
    >   Found  [2e46b5a5-c5d9-44c0-b11c-310c222f4818]
    >   ----
    >   Ceph path [/volumes/_nogroup/2cdefe41-6c04-4865-9144-c0a7a183b424]
    >   Ceph size [512]
    >   ----
    >   Ceph node [10.206.1.5:6789]
    >   Ceph node [10.206.1.6:6789]
    >   Ceph node [10.206.1.7:6789]
    >   ----
    >   Ceph user [aglais-gaia-dr2-ro]
    >   Ceph key  [AQDn........7sDQ==]
    >   ....
    >   ....


# -----------------------------------------------------
# Login to the worker node to test the mount.
#[root@ansibler]

    ssh worker01 \
        '
        date
        hostname
        echo "----"
        df -h  /data/gaia/dr2
        '

    >   Tue Nov 17 08:03:25 UTC 2020
    >   aglais-20201117-worker01.novalocal
    >   ----
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   ceph-fuse       512G  473G   40G  93% /data/gaia/dr2








