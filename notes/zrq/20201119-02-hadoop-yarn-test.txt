#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#

    Target:

        Run the Hadoop-Yarn Ansible deploy on a different project (gaia-dev).

    Result:

        Success :-)
        Openstack client re[ported errors while deleting volumes, but the were all deleted.
        CephFS mount of a Manila share from another project works.

# -----------------------------------------------------
# Update the project name.
#[user@desktop]

    cloudname=gaia-dev

    sed -i '
        s/^\(AGLAIS_CLOUD\)=.*$/\1='${cloudname:?}'/
        ' "${HOME}/aglais.env"


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "clouduser=${AGLAIS_USER:?}" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/openstack:/openstack:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/hadoop-yarn:/hadoop-yarn:ro,z" \
        atolmis/ansible-client:latest \
        bash

# -----------------------------------------------------
# List everything.
#[root@ansibler]

    /openstack/bin/list-all.sh \
        "${cloudname:?}"

--START--

---- ---- ----
File [list-all.sh]
Path [/openstack/bin]
---- ---- ----
Cloud name [gaia-dev]
---- ---- ----

---- ----
Clusters


---- ----
Servers
+--------------------------------------+------------------+--------+--------------------------------------------+---------------+-------------------+
| ID                                   | Name             | Status | Networks                                   | Image         | Flavor            |
+--------------------------------------+------------------+--------+--------------------------------------------+---------------+-------------------+
| 2f3a89ea-d61b-4980-967e-135e43cde77a | stv-dev-worker-8 | ACTIVE | stv-dev-network=10.0.0.5                   |               | general.v1.small  |
| e2a1cb6d-50ea-4969-ab22-3b14f0a8c1d7 | stv-dev-worker-7 | ACTIVE | stv-dev-network=10.0.0.13                  |               | general.v1.small  |
| cf99039a-9c37-48ae-9dfe-586bcad4c742 | stv-dev-storage  | ACTIVE | stv-dev-network=10.0.0.17                  |               | general.v1.tiny   |
| e949f4ad-f388-4a2c-b93e-b29cd62015d3 | stv-dev-worker-6 | ACTIVE | stv-dev-network=10.0.0.29                  |               | general.v1.small  |
| 165b4c28-9e81-4614-8f67-35bdef29bade | stv-dev-worker-2 | ACTIVE | stv-dev-network=10.0.0.4                   |               | general.v1.small  |
| 254902a8-7e08-493d-9565-79c96d111316 | stv-dev-worker-4 | ACTIVE | stv-dev-network=10.0.0.33                  |               | general.v1.small  |
| cb8fa8b7-a62a-4b8f-838e-f2909e06ce80 | stv-dev-worker-5 | ACTIVE | stv-dev-network=10.0.0.28                  |               | general.v1.small  |
| e76726db-4129-4e17-b365-797a0e049ee2 | stv-dev-worker-3 | ACTIVE | stv-dev-network=10.0.0.6                   |               | general.v1.small  |
| eaca1f6a-4fbc-44ca-9c86-1026e42dab9f | stv-dev-worker-1 | ACTIVE | stv-dev-network=10.0.0.16                  |               | general.v1.small  |
| 34103538-0ef6-4454-a6e9-65c035f0805a | stv-dev-zeppelin | ACTIVE | stv-dev-network=10.0.0.27, 128.232.224.69  | Fedora-30-1.2 | general.v1.medium |
| 46a5ec53-dd9b-4718-80e5-a33e571b8eac | stv-dev-master   | ACTIVE | stv-dev-network=10.0.0.14                  | Fedora-30-1.2 | general.v1.medium |
| a829b57b-32ba-4708-aa65-7ad643ed5fc6 | stv-dev-gateway  | ACTIVE | stv-dev-network=10.0.0.20, 128.232.227.134 | Fedora-30-1.2 | general.v1.tiny   |
+--------------------------------------+------------------+--------+--------------------------------------------+---------------+-------------------+

---- ----
Volumes
+--------------------------------------+------+--------+------+-------------------------------------------+
| ID                                   | Name | Status | Size | Attached to                               |
+--------------------------------------+------+--------+------+-------------------------------------------+
| ee5605f2-2f16-4b0b-8d61-2bf0e7f0bb6e |      | in-use |  500 | Attached to stv-dev-worker-8 on /dev/vda  |
| f4319224-c790-4db6-85a5-e69b8202009a |      | in-use |  500 | Attached to stv-dev-worker-7 on /dev/vda  |
| 49b92bc1-fa6a-4628-9826-2478421d221f |      | in-use | 1000 | Attached to stv-dev-storage on /dev/vda   |
| 5d6d4c9b-335b-443c-a901-34be64cae447 |      | in-use |  500 | Attached to stv-dev-worker-3 on /dev/vda  |
| 33bca8ab-a430-4c24-81b3-f1915bc16231 |      | in-use |  500 | Attached to stv-dev-worker-5 on /dev/vda  |
| 2189a41d-b05d-4fa1-8ecd-896ccb780814 |      | in-use |  500 | Attached to stv-dev-worker-6 on /dev/vda  |
| 1079eba6-2c72-4bd1-b566-b53a83c55afd |      | in-use |  500 | Attached to stv-dev-worker-4 on /dev/vda  |
| 780a8382-b7f6-4765-b389-fa57541fe5f8 |      | in-use |  500 | Attached to stv-dev-worker-1 on /dev/vda  |
| 0690b0bc-705b-4cf7-9ad0-65d0c47ee1ff |      | in-use |  500 | Attached to stv-dev-worker-2 on /dev/vda  |
+--------------------------------------+------+--------+------+-------------------------------------------+

---- ----
Floating addresses
+--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+
| ID                                   | Floating IP Address | Fixed IP Address | Port                                 | Floating Network                     | Project                          |
+--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+
| 03a3081c-e191-4f76-ae61-e4b930f39367 | 128.232.224.69      | 10.0.0.27        | e65b9dbe-074d-4150-aa9a-d400864dbf17 | a929e8db-1bf4-4a5f-a80c-fabd39d06a26 | 08e24c6d87f94740aa59c172462ed927 |
| 03c1bec8-ea95-4fdb-bf1c-9de9f9643894 | 128.232.227.134     | 10.0.0.20        | ed069f64-9c4d-4888-ac10-fe49fedf59de | a929e8db-1bf4-4a5f-a80c-fabd39d06a26 | 08e24c6d87f94740aa59c172462ed927 |
+--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+

---- ----
Routers
+--------------------------------------+----------------+--------+-------+----------------------------------+
| ID                                   | Name           | Status | State | Project                          |
+--------------------------------------+----------------+--------+-------+----------------------------------+
| 1160f466-f8d9-4f21-800e-92608b44a233 | stv-dev-router | ACTIVE | UP    | 08e24c6d87f94740aa59c172462ed927 |
+--------------------------------------+----------------+--------+-------+----------------------------------+

---- ----
Networks
+--------------------------------------+------------------+----------------------------------------------------------------------------+
| ID                                   | Name             | Subnets                                                                    |
+--------------------------------------+------------------+----------------------------------------------------------------------------+
| 66fc0328-33b6-4b23-bc4d-f0c4022c32fd | stv-dev-network  | d1be6910-1b7b-440b-bf5f-349441a5240c                                       |
| a929e8db-1bf4-4a5f-a80c-fabd39d06a26 | internet         | 180dc961-c52f-461a-b241-8f7a30d022a5, 273123bb-70f6-4f51-a406-7fc4b446532d |
| ecb791d5-1022-447a-a79c-8f38a0f5c990 | cumulus-internal | 01b76c7c-3c1a-4c5c-9a5a-14bcf6c0c290                                       |
+--------------------------------------+------------------+----------------------------------------------------------------------------+

---- ----
Subnets
+--------------------------------------+------------------+--------------------------------------+---------------+
| ID                                   | Name             | Network                              | Subnet        |
+--------------------------------------+------------------+--------------------------------------+---------------+
| 01b76c7c-3c1a-4c5c-9a5a-14bcf6c0c290 | cumulus-internal | ecb791d5-1022-447a-a79c-8f38a0f5c990 | 10.218.0.0/16 |
| d1be6910-1b7b-440b-bf5f-349441a5240c | stv-dev-subnet   | 66fc0328-33b6-4b23-bc4d-f0c4022c32fd | 10.0.0.0/24   |
+--------------------------------------+------------------+--------------------------------------+---------------+

---- ----
Security groups
+--------------------------------------+--------------------+------------------------+----------------------------------+------+
| ID                                   | Name               | Description            | Project                          | Tags |
+--------------------------------------+--------------------+------------------------+----------------------------------+------+
| 0d39003c-0cc8-44a7-883a-a4286c8f822c | stv-dev-master     |                        | 08e24c6d87f94740aa59c172462ed927 | []   |
| 45c02687-2b9c-4901-a003-35af70a6f404 | internal-bastion   |                        | 08e24c6d87f94740aa59c172462ed927 | []   |
| 51551d67-cd8b-4f3d-907a-c0484b1410af | external-bastion   |                        | 08e24c6d87f94740aa59c172462ed927 | []   |
| 5c9657e2-8b9a-44ca-b328-7ffc46a523e6 | internal-webserver |                        | 08e24c6d87f94740aa59c172462ed927 | []   |
| d917cfec-af58-4ee0-8730-62867d79a323 | default            | Default security group | 08e24c6d87f94740aa59c172462ed927 | []   |
| ef6ba4d9-09fc-41f9-9d63-f51840b22a30 | stv-dev-zeppelin   |                        | 08e24c6d87f94740aa59c172462ed927 | []   |
| f367ed06-728b-4d46-a850-d1565408c393 | stv-dev-worker     |                        | 08e24c6d87f94740aa59c172462ed927 | []   |
+--------------------------------------+--------------------+------------------------+----------------------------------+------+
--END--


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    /openstack/bin/delete-all.sh \
        "${cloudname:?}"

--START--

---- ---- ----
File [delete-all.sh]
Path [/openstack/bin]
---- ---- ----
Cloud name [gaia-dev]
---- ---- ----

---- ----
Deleting clusters

---- ----
Deleting servers
- Deleting server [2f3a89ea-d61b-4980-967e-135e43cde77a]
- Deleting server [e2a1cb6d-50ea-4969-ab22-3b14f0a8c1d7]
- Deleting server [cf99039a-9c37-48ae-9dfe-586bcad4c742]
- Deleting server [e949f4ad-f388-4a2c-b93e-b29cd62015d3]
- Deleting server [165b4c28-9e81-4614-8f67-35bdef29bade]
- Deleting server [254902a8-7e08-493d-9565-79c96d111316]
- Deleting server [cb8fa8b7-a62a-4b8f-838e-f2909e06ce80]
- Deleting server [e76726db-4129-4e17-b365-797a0e049ee2]
- Deleting server [eaca1f6a-4fbc-44ca-9c86-1026e42dab9f]
- Deleting server [34103538-0ef6-4454-a6e9-65c035f0805a]
- Deleting server [46a5ec53-dd9b-4718-80e5-a33e571b8eac]
- Deleting server [a829b57b-32ba-4708-aa65-7ad643ed5fc6]

---- ----
Deleting volumes
- Deleting volume [ee5605f2-2f16-4b0b-8d61-2bf0e7f0bb6e]
Failed to delete volume with name or ID 'ee5605f2-2f16-4b0b-8d61-2bf0e7f0bb6e': Invalid volume: Volume status must be available or error or error_restoring or error_extending or error_managing and must not be migrating, attached, belong to a group, have snapshots or be disassociated from snapshots after volume transfer. (HTTP 400) (Request-ID: req-411bdb46-23ca-4b33-9a76-bb2ae3c6acf9)
1 of 1 volumes failed to delete.
- Deleting volume [f4319224-c790-4db6-85a5-e69b8202009a]
Failed to delete volume with name or ID 'f4319224-c790-4db6-85a5-e69b8202009a': Invalid volume: Volume status must be available or error or error_restoring or error_extending or error_managing and must not be migrating, attached, belong to a group, have snapshots or be disassociated from snapshots after volume transfer. (HTTP 400) (Request-ID: req-5e588348-7093-447c-bdf4-54514f06fd99)
1 of 1 volumes failed to delete.
- Deleting volume [49b92bc1-fa6a-4628-9826-2478421d221f]
Failed to delete volume with name or ID '49b92bc1-fa6a-4628-9826-2478421d221f': Invalid volume: Volume status must be available or error or error_restoring or error_extending or error_managing and must not be migrating, attached, belong to a group, have snapshots or be disassociated from snapshots after volume transfer. (HTTP 400) (Request-ID: req-86be8d6a-b4df-4ad0-a8a3-4b6e733aedad)
1 of 1 volumes failed to delete.
- Deleting volume [5d6d4c9b-335b-443c-a901-34be64cae447]
Failed to delete volume with name or ID '5d6d4c9b-335b-443c-a901-34be64cae447': Invalid volume: Volume status must be available or error or error_restoring or error_extending or error_managing and must not be migrating, attached, belong to a group, have snapshots or be disassociated from snapshots after volume transfer. (HTTP 400) (Request-ID: req-ab1016d9-0ec2-4659-a65a-5c9b185ebce6)
1 of 1 volumes failed to delete.
- Deleting volume [33bca8ab-a430-4c24-81b3-f1915bc16231]
Failed to delete volume with name or ID '33bca8ab-a430-4c24-81b3-f1915bc16231': Invalid volume: Volume status must be available or error or error_restoring or error_extending or error_managing and must not be migrating, attached, belong to a group, have snapshots or be disassociated from snapshots after volume transfer. (HTTP 400) (Request-ID: req-67527428-1671-4cf3-ac8c-ff6cfdded93b)
1 of 1 volumes failed to delete.
- Deleting volume [2189a41d-b05d-4fa1-8ecd-896ccb780814]
Failed to delete volume with name or ID '2189a41d-b05d-4fa1-8ecd-896ccb780814': Invalid volume: Volume status must be available or error or error_restoring or error_extending or error_managing and must not be migrating, attached, belong to a group, have snapshots or be disassociated from snapshots after volume transfer. (HTTP 400) (Request-ID: req-523e558f-1f6c-457b-b1b6-032247fa0f4b)
1 of 1 volumes failed to delete.
- Deleting volume [1079eba6-2c72-4bd1-b566-b53a83c55afd]
Failed to delete volume with name or ID '1079eba6-2c72-4bd1-b566-b53a83c55afd': Invalid volume: Volume status must be available or error or error_restoring or error_extending or error_managing and must not be migrating, attached, belong to a group, have snapshots or be disassociated from snapshots after volume transfer. (HTTP 400) (Request-ID: req-02c0d60b-ae32-45b0-aea4-c30ce96e9259)
1 of 1 volumes failed to delete.
- Deleting volume [780a8382-b7f6-4765-b389-fa57541fe5f8]
Failed to delete volume with name or ID '780a8382-b7f6-4765-b389-fa57541fe5f8': Invalid volume: Volume status must be available or error or error_restoring or error_extending or error_managing and must not be migrating, attached, belong to a group, have snapshots or be disassociated from snapshots after volume transfer. (HTTP 400) (Request-ID: req-c599d571-4183-407d-be0d-70faec3df0fb)
1 of 1 volumes failed to delete.
- Deleting volume [0690b0bc-705b-4cf7-9ad0-65d0c47ee1ff]
Failed to delete volume with name or ID '0690b0bc-705b-4cf7-9ad0-65d0c47ee1ff': Invalid volume: Volume status must be available or error or error_restoring or error_extending or error_managing and must not be migrating, attached, belong to a group, have snapshots or be disassociated from snapshots after volume transfer. (HTTP 400) (Request-ID: req-79712f29-1087-4211-866c-f3e096a01774)
1 of 1 volumes failed to delete.

---- ----
Releasing addresses
- Releasing address [03a3081c-e191-4f76-ae61-e4b930f39367]
- Releasing address [03c1bec8-ea95-4fdb-bf1c-9de9f9643894]

---- ----
Deleting routers
- Router [1160f466-f8d9-4f21-800e-92608b44a233]
-- Deleting routes
-- Deleting ports
--- Deleting port [908ad700-e35c-49a6-bcac-f68169bf95ed]
- Deleting router [1160f466-f8d9-4f21-800e-92608b44a233]

---- ----
Deleting subnets
- Subnet [d1be6910-1b7b-440b-bf5f-349441a5240c]
-- Deleting subnet ports
--- Deleting subnet port [24209967-77cc-405f-a58e-0fc3b99daa85]
--- Deleting subnet port [7228da84-386e-4efa-b3b9-5386dce1e60e]
--- Deleting subnet port [a32363f8-5f91-45f6-b55f-38a3709c8fd2]
- Deleting subnet [d1be6910-1b7b-440b-bf5f-349441a5240c]

---- ----
Deleting networks
- Deleting network [66fc0328-33b6-4b23-bc4d-f0c4022c32fd]

---- ----
Deleting security groups
- Deleting security group [0d39003c-0cc8-44a7-883a-a4286c8f822c]
- Deleting security group [45c02687-2b9c-4901-a003-35af70a6f404]
- Deleting security group [51551d67-cd8b-4f3d-907a-c0484b1410af]
- Deleting security group [5c9657e2-8b9a-44ca-b328-7ffc46a523e6]
- Deleting security group [ef6ba4d9-09fc-41f9-9d63-f51840b22a30]
- Deleting security group [f367ed06-728b-4d46-a850-d1565408c393]

---- ----
Deleting clusters

---- ----
List servers

---- ----
List addresses

---- ----
List routers

---- ----
List networks
+--------------------------------------+------------------+----------------------------------------------------------------------------+
| ID                                   | Name             | Subnets                                                                    |
+--------------------------------------+------------------+----------------------------------------------------------------------------+
| a929e8db-1bf4-4a5f-a80c-fabd39d06a26 | internet         | 180dc961-c52f-461a-b241-8f7a30d022a5, 273123bb-70f6-4f51-a406-7fc4b446532d |
| ecb791d5-1022-447a-a79c-8f38a0f5c990 | cumulus-internal | 01b76c7c-3c1a-4c5c-9a5a-14bcf6c0c290                                       |
+--------------------------------------+------------------+----------------------------------------------------------------------------+
---- ----
List subnets
+--------------------------------------+------------------+--------------------------------------+---------------+
| ID                                   | Name             | Network                              | Subnet        |
+--------------------------------------+------------------+--------------------------------------+---------------+
| 01b76c7c-3c1a-4c5c-9a5a-14bcf6c0c290 | cumulus-internal | ecb791d5-1022-447a-a79c-8f38a0f5c990 | 10.218.0.0/16 |
+--------------------------------------+------------------+--------------------------------------+---------------+
---- ----
List security groups
+--------------------------------------+---------+------------------------+----------------------------------+------+
| ID                                   | Name    | Description            | Project                          | Tags |
+--------------------------------------+---------+------------------------+----------------------------------+------+
| d917cfec-af58-4ee0-8730-62867d79a323 | default | Default security group | 08e24c6d87f94740aa59c172462ed927 | []   |
+--------------------------------------+---------+------------------------+----------------------------------+------+
---- ----
List clusters

---- ----
Done
--END--


# -----------------------------------------------------
# Run the main Ansible deployment.
#[root@ansibler]

    /hadoop-yarn/bin/create-all.sh \
        "${cloudname:?}"

    >   ....
    >   ....


# -----------------------------------------------------
# Run the SparkPi example from the Spark install instructtions.
# https://spark.apache.org/docs/3.0.0-preview2/running-on-yarn.html#launching-spark-on-yarn
#[root@ansibler]

    ssh master01 \
        '
        cd "${SPARK_HOME:?}"

        spark-submit \
            --class org.apache.spark.examples.SparkPi \
            --master yarn \
            --deploy-mode cluster \
            --driver-memory 1g \
            --executor-memory 1g \
            --executor-cores 1 \
            examples/jars/spark-examples*.jar \
                10
        '

--START--
2020-11-20 18:12:51,767 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-20 18:12:51,834 INFO client.RMProxy: Connecting to ResourceManager at master01/10.10.3.120:8032
....
....
2020-11-20 18:12:57,457 INFO impl.YarnClientImpl: Submitted application application_1605895558973_0001
2020-11-20 18:12:58,462 INFO yarn.Client: Application report for application_1605895558973_0001 (state: ACCEPTED)
2020-11-20 18:12:58,465 INFO yarn.Client:
	 client token: N/A
	 diagnostics: [Fri Nov 20 18:12:58 +0000 2020] Scheduler has assigned a container for AM, waiting for AM container to be launched
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1605895977179
	 final status: UNDEFINED
	 tracking URL: http://master01:8088/proxy/application_1605895558973_0001/
	 user: fedora
2020-11-20 18:12:59,467 INFO yarn.Client: Application report for application_1605895558973_0001 (state: ACCEPTED)
....
....
2020-11-20 18:13:05,484 INFO yarn.Client: Application report for application_1605895558973_0001 (state: RUNNING)
2020-11-20 18:13:05,485 INFO yarn.Client:
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: worker01
	 ApplicationMaster RPC port: 46015
	 queue: default
	 start time: 1605895977179
	 final status: UNDEFINED
	 tracking URL: http://master01:8088/proxy/application_1605895558973_0001/
	 user: fedora
2020-11-20 18:13:06,487 INFO yarn.Client: Application report for application_1605895558973_0001 (state: RUNNING)
....
....
2020-11-20 18:13:12,504 INFO yarn.Client: Application report for application_1605895558973_0001 (state: FINISHED)
2020-11-20 18:13:12,504 INFO yarn.Client:
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: worker01
	 ApplicationMaster RPC port: 46015
	 queue: default
	 start time: 1605895977179
	 final status: SUCCEEDED
	 tracking URL: http://master01:8088/proxy/application_1605895558973_0001/
	 user: fedora
2020-11-20 18:13:12,517 INFO util.ShutdownHookManager: Shutdown hook called
2020-11-20 18:13:12,520 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-93754f6e-6e22-40b6-91b5-964099ef688a
2020-11-20 18:13:12,523 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-407ca3ac-59d3-4594-8dfc-38a250595373
--END--


# -----------------------------------------------------
# Login to the worker node to test the mount.
#[root@ansibler]

    ssh worker01 \
        '
        date
        hostname
        echo "----"
        df -h  /data/gaia/dr2
        '

--START--
Fri Nov 20 18:14:12 UTC 2020
aglais-20201120-worker01.novalocal
----
Filesystem      Size  Used Avail Use% Mounted on
ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
--END--


# -----------------------------------------------------
# Login to the worker node to test the mount.
#[root@ansibler]

    ssh worker01 \
        '
        date
        hostname
        echo "----"
        df -h  /user/nch
        '

--START--
Fri Nov 20 18:14:26 UTC 2020
aglais-20201120-worker01.novalocal
----
Filesystem      Size  Used Avail Use% Mounted on
ceph-fuse        10T     0   10T   0% /user/nch
--END--






