#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    Target:

        Run the Hadoop-Yarn Ansible deploy

    Result:

        Success.

    TODO:

        Remove /aglais from the data mount path in our Zeppelin container.
        Automatically add /data mounts to the Spark workers.
        Proxy pass for the Spark UI.


# -----------------------------------------------------
# Update the project name.
#[user@desktop]

    cloudname=gaia-prod

    sed -i '
        s/^\(AGLAIS_CLOUD\)=.*$/\1='${cloudname:?}'/
        ' "${HOME}/aglais.env"

# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "clouduser=${AGLAIS_USER:?}" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/openstack:/openstack:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/kubernetes:/kubernetes:rw,z" \
        atolmis/ansible-client:latest \
        bash

# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    /openstack/bin/delete-all.sh \
        "${cloudname:?}"

    >   ....
    >   ....


# -----------------------------------------------------
# Run the main Kubernetes deployment.
#[root@ansibler]

    /kubernetes/bin/create-all.sh \
        "${cloudname:?}"

    >   ....
    >   ....




# -----------------------------------------------------
# Get the ServiceAccount token.

    buildname="aglais-k8s-$(date '+%Y%m%d')"
    namespace=${buildname,,}


    secretname=$(
        kubectl \
            --output json \
            --namespace "${namespace:?}" \
            get \
                ServiceAccount \
                    "aglais-dashboard-kubernetes-dashboard" \
        | jq -r '.secrets[0].name'
        )

    kubectl \
        --output json \
        --namespace "${namespace:?}" \
        get secret \
            "${secretname:?}" \
    | jq -r '.data.token | @base64d'
























# -----------------------------------------------------
# -----------------------------------------------------
# Login to Zeppelin and test ...
#[user@desktop]

    firefox --new-window "https://zeppelin.metagrid.xyz/" &

    #
    # Yay - works.
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Mount the Gaia data in our Spark workers.
#[user@zeppelin]

# --------------------------------
%spark.conf

spark.executor.instances 10

spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-gaia-dr2.mount.path        /aglais/data/gaia/dr2
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-gaia-dr2.mount.readOnly    true
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-gaia-dr2.options.claimName aglais-gaia-dr2-claim

--START--
Took 0 sec. Last updated by user1 at November 23 2020, 6:14:04 AM.
--END--

# --------------------------------
%spark.pyspark

gs_df = sqlContext.read.parquet(
    "/aglais/data/gaia/dr2"
    )

print("DF count: ", gs_df.count())
print("DF partitions: ", gs_df.rdd.getNumPartitions())

--START--
Took 12 sec. Last updated by user1 at November 23 2020, 6:15:46 AM.
--END--




