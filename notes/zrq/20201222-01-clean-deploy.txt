#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#

    Target:

        New clean deployment.

    Result:

        Still got the K8s network issue.




# -----------------------------------------------------
# Update the project name.
#[user@desktop]

    cloudname=gaia-prod

    sed -i '
        s/^\(AGLAIS_CLOUD\)=.*$/\1='${cloudname:?}'/
        ' "${HOME}/aglais.env"


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name kubernator \
        --hostname kubernator \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/openstack:/openstack:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/kubernetes:/kubernetes:rw,z" \
        atolmis/openstack-client:latest \
        bash

# -----------------------------------------------------
# Delete everything.
#[root@kubernator]

    /openstack/bin/delete-all.sh \
        "${cloudname:?}"

    >   ....
    >   ....


# -----------------------------------------------------
# Run the main Kubernetes deployment.
#[root@kubernator]

    buildname="aglais-$(date '+%Y%m%d')"
    namespace=${buildname,,}

    /kubernetes/bin/create-all.sh \
        "${cloudname:?}" \
        "${buildname:?}" \
        "${namespace:?}"

    >   ....
    >   ----
    >   Installing dashboard Helm chart
    >   Namespace [aglais-20201222]
    >   Dash host [valeria.metagrid.xyz]
    >   Getting updates for unmanaged Helm repositories...
    >   ...Successfully got an update from the "https://kubernetes.github.io/dashboard" chart repository
    >   Saving 1 charts
    >   Downloading kubernetes-dashboard from repo https://kubernetes.github.io/dashboard
    >   Deleting outdated charts
    >   Release "aglais-dashboard" does not exist. Installing it now.
    >   Error: Internal error occurred: failed calling webhook "validate.nginx.ingress.kubernetes.io":
    >       Post https://aglais-ingress-nginx-controller-admission.aglais-20201222.svc:443/networking/v1beta1/ingresses?timeout=10s:
    >           dial tcp 10.254.159.0:443:
    >               connect:
    >                   connection refused
    >   ....

# -----------------------------------------------------
# Get the ServiceAccount token.
#[root@kubernator]

    secretname=$(
        kubectl \
            --output json \
            --namespace "${namespace:?}" \
            get ServiceAccount \
                "aglais-dashboard-kubernetes-dashboard" \
        | jq -r '.secrets[0].name'
        )

    kubectl \
        --output json \
        --namespace "${namespace:?}" \
        get Secret \
            "${secretname:?}" \
    | jq -r '.data.token | @base64d'


    >   ....
    >   ....


# -----------------------------------------------------
# Get the Ingress address.
#[root@kubernator]

    kubectl \
        --namespace "${namespace:?}" \
        get Ingress

    >   NAME                      HOSTS                   ADDRESS           PORTS     AGE
    >   zeppelin-server-ingress   zeppelin.metagrid.xyz   128.232.227.198   80, 443   6m9s


# -----------------------------------------------------
# -----------------------------------------------------

    #
    # Update our DNS ..
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Login to Dashboard and test ...
#[user@desktop]

    firefox --new-window "https://valeria.metagrid.xyz/" &

    #
    # FAIL - 404 error from nginx.
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Login to Zeppelin and test ...
#[user@desktop]

    firefox --new-window "https://zeppelin.metagrid.xyz/" &

    #
    # PASS - front page of Zeppelin works.
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Mount the Gaia data in our Spark workers.
#[user@zeppelin]

# --------------------------------
%spark.conf

spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-gaia-dr2.mount.path        /data/gaia/dr2
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-gaia-dr2.mount.readOnly    true
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-gaia-dr2.options.claimName aglais-gaia-dr2-claim

spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-nch.mount.path        /user/nch
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-nch.mount.readOnly    true
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-nch.options.claimName aglais-user-nch-claim

spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-stv.mount.path        /user/stv
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-stv.mount.readOnly    true
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-stv.options.claimName aglais-user-stv-claim

spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-zrq.mount.path        /user/zrq
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-zrq.mount.readOnly    false
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-zrq.options.claimName aglais-user-zrq-claim


# --------------------------------
%spark.pyspark

gaia_data = sqlContext.read.parquet(
    "/data/gaia/dr2"
    )

print("DF count: ",      gaia_data.count())
print("DF partitions: ", gaia_data.rdd.getNumPartitions())



    >   ....
    >   ....
    >   Py4JJavaError: An error occurred while calling o81.parquet.
    >   : org.apache.spark.SparkException: Job aborted due to stage failure:
    >       Task 0 in stage 1.0 failed 4 times, most recent failure:
    >           Lost task 0.3 in stage 1.0 (TID 7, 10.100.3.9, executor 1):
    >               org.apache.spark.SparkException:
    >                   Exception thrown in awaitResult:
    >   ....
    >   ....
    >   Caused by:
    >       java.io.FileNotFoundException:
    >           File file:/data/gaia/dr2/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet does not exist
    >   ....
    >   ....


# --------------------------------

%spark.pyspark

tmass_data = sqlContext.read.parquet(
    "/user/zrq/tmass/pqt"
    )

print("DF count: ",      tmass_data.count())
print("DF partitions: ", tmass_data.rdd.getNumPartitions())

    >   ....
    >   ....
    >   Py4JJavaError: An error occurred while calling o93.parquet.
    >   : org.apache.spark.SparkException:
    >       Job aborted due to stage failure:
    >           Task 0 in stage 2.0 failed 4 times, most recent failure:
    >               Lost task 0.3 in stage 2.0 (TID 11, 10.100.3.9, executor 1):
    >                   org.apache.spark.SparkException: Exception thrown in awaitResult:
    >   ....
    >   ....
    >   Caused by:
    >       java.io.FileNotFoundException:
    >           File file:/user/zrq/tmass/pqt/part-00000-5f562105-43bd-417e-80aa-d688ccb45ec0-c000.snappy.parquet does not exist
    >   ....
    >   ....


# -----------------------------------------------------
# -----------------------------------------------------
# List the active Pods.
#[root@kubernator]

    kubectl \
        --namespace "${namespace:?}" \
        get Pods

    >   NAME                                                    READY   STATUS    RESTARTS   AGE
    >   aglais-ceph-csi-cephfs-nodeplugin-2ttvh                 3/3     Running   0          37m
    >   aglais-ceph-csi-cephfs-nodeplugin-8r7mp                 3/3     Running   0          37m
    >   aglais-ceph-csi-cephfs-nodeplugin-jwlmt                 3/3     Running   0          37m
    >   aglais-ceph-csi-cephfs-nodeplugin-mzbt5                 3/3     Running   0          37m
    >   aglais-ceph-csi-cephfs-provisioner-f9ff8cd4c-5gzpx      6/6     Running   0          37m
    >   aglais-ceph-csi-cephfs-provisioner-f9ff8cd4c-ljg95      6/6     Running   0          37m
    >   aglais-ceph-csi-cephfs-provisioner-f9ff8cd4c-rv5gn      6/6     Running   0          37m
    >   aglais-dashboard-kubernetes-dashboard-65d5c6599-nv5nm   2/2     Running   0          37m
    >   aglais-gaia-dr2-testpod                                 1/1     Running   0          37m
    >   aglais-ingress-nginx-controller-54f444477b-fn8qv        1/1     Running   0          37m
    >   aglais-openstack-manila-csi-controllerplugin-0          3/3     Running   0          37m
    >   aglais-openstack-manila-csi-nodeplugin-68wp6            2/2     Running   0          37m
    >   aglais-openstack-manila-csi-nodeplugin-7ws4l            2/2     Running   0          37m
    >   aglais-openstack-manila-csi-nodeplugin-7xkmg            2/2     Running   0          37m
    >   aglais-openstack-manila-csi-nodeplugin-d5hlg            2/2     Running   0          37m
    >   aglais-user-nch-testpod                                 1/1     Running   0          36m
    >   aglais-user-stv-testpod                                 1/1     Running   0          36m
    >   aglais-user-zrq-testpod                                 1/1     Running   0          36m
    >   spark-ctvkkh                                            1/1     Running   0          13m
    >   zeppelin-9dabee768a669ffe-exec-1                        1/1     Running   0          13m
    >   zeppelin-9dabee768a669ffe-exec-2                        1/1     Running   0          13m
    >   zeppelin-server-deploy-5c8476cd58-dvknz                 3/3     Running   0          36m


# -----------------------------------------------------
# Login to one of the test Pods.
#[root@kubernator]

    kubectl \
        --namespace "${namespace:?}" \
        exec \
            --tty \
            --stdin \
            aglais-gaia-dr2-testpod \
            -- \
                bash

    ls -al /data/gaia/dr2/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet

    >   -rw-r--r--. 1 1000 1000 74114220 Oct 24 15:51 /data/gaia/dr2/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet


    file /data/gaia/dr2/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet

    >   /data/gaia/dr2/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet: Apache Parquet


    md5sum /data/gaia/dr2/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet

    >   3d71b06f5252f7d94c6e7fc8ad379d47  /data/gaia/dr2/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet


    stat /data/gaia/dr2/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet

    >     File: /data/gaia/dr2/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
    >     Size: 74114220  	Blocks: 144755     IO Block: 4194304 regular file
    >   Device: ech/236d	Inode: 2199023322347  Links: 1
    >   Access: (0644/-rw-r--r--)  Uid: ( 1000/ UNKNOWN)   Gid: ( 1000/ UNKNOWN)
    >   Access: 2020-10-24 15:51:22.001242926 +0000
    >   Modify: 2020-10-24 15:51:24.352250106 +0000
    >   Change: 2020-10-24 15:51:24.356500916 +0000


    cp /data/gaia/dr2/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet \
       /tmp


    stat /tmp/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet

    >     File: /tmp/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
    >     Size: 74114220  	Blocks: 144760     IO Block: 4096   regular file
    >   Device: 100000h/1048576d	Inode: 21293334    Links: 1
    >   Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)
    >   Access: 2020-12-22 18:36:39.565772994 +0000
    >   Modify: 2020-12-22 18:36:39.611772802 +0000
    >   Change: 2020-12-22 18:36:39.611772802 +0000
    >    Birth: 2020-12-22 18:36:39.565772994 +0000


    md5sum /tmp/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet

    >   3d71b06f5252f7d94c6e7fc8ad379d47  /tmp/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet

    #
    # So, as far as I can tell, the parquet file is healthy enough.
    # Which suggests something to do with the way it is mounted in the Spark container ..
    #

# -----------------------------------------------------
# See what we can find out about our Zeppelin and Spark Pods.
#[root@kubernator]

    kubectl \
        --namespace "${namespace:?}" \
        get Pod

    >   NAME                                                    READY   STATUS    RESTARTS   AGE
    >   aglais-ceph-csi-cephfs-nodeplugin-2ttvh                 3/3     Running   0          6h42m
    >   aglais-ceph-csi-cephfs-nodeplugin-8r7mp                 3/3     Running   0          6h42m
    >   aglais-ceph-csi-cephfs-nodeplugin-jwlmt                 3/3     Running   0          6h42m
    >   aglais-ceph-csi-cephfs-nodeplugin-mzbt5                 3/3     Running   0          6h42m
    >   aglais-ceph-csi-cephfs-provisioner-f9ff8cd4c-5gzpx      6/6     Running   0          6h42m
    >   aglais-ceph-csi-cephfs-provisioner-f9ff8cd4c-ljg95      6/6     Running   0          6h42m
    >   aglais-ceph-csi-cephfs-provisioner-f9ff8cd4c-rv5gn      6/6     Running   0          6h42m
    >   aglais-dashboard-kubernetes-dashboard-65d5c6599-nv5nm   2/2     Running   0          6h41m
    >   aglais-gaia-dr2-testpod                                 1/1     Running   0          6h41m
    >   aglais-ingress-nginx-controller-54f444477b-fn8qv        1/1     Running   0          6h42m
    >   aglais-openstack-manila-csi-controllerplugin-0          3/3     Running   0          6h42m
    >   aglais-openstack-manila-csi-nodeplugin-68wp6            2/2     Running   0          6h42m
    >   aglais-openstack-manila-csi-nodeplugin-7ws4l            2/2     Running   0          6h42m
    >   aglais-openstack-manila-csi-nodeplugin-7xkmg            2/2     Running   0          6h42m
    >   aglais-openstack-manila-csi-nodeplugin-d5hlg            2/2     Running   0          6h42m
    >   aglais-user-nch-testpod                                 1/1     Running   0          6h41m
    >   aglais-user-stv-testpod                                 1/1     Running   0          6h41m
    >   aglais-user-zrq-testpod                                 1/1     Running   0          6h40m
    >   spark-ctvkkh                                            1/1     Running   0          6h17m
    >   zeppelin-9dabee768a669ffe-exec-1                        1/1     Running   0          6h17m
    >   zeppelin-9dabee768a669ffe-exec-2                        1/1     Running   0          6h17m
    >   zeppelin-server-deploy-5c8476cd58-dvknz                 3/3     Running   0          6h40m

    #
    # Only one Spark worker Pod ?
    #


    kubectl \
        --namespace "${namespace:?}" \
        describe Pod \
            spark-ctvkkh

    >   Name:         spark-ctvkkh
    >   Namespace:    aglais-20201222
    >   Node:         aglais-20201222-cluster-mzzcgdpjkijp-node-2/10.0.0.13
    >   Start Time:   Tue, 22 Dec 2020 12:22:09 +0000
    >   Labels:       app=spark-ctvkkh
    >                 interpreterGroupId=spark-shared_process
    >                 interpreterSettingName=spark
    >   Annotations:  <none>
    >   Status:       Running
    >   IP:           10.100.5.9
    >   IPs:
    >     IP:  10.100.5.9
    >   Init Containers:
    >     spark-home-init:
    >       Container ID:  docker://17b62c26dd72d23a3c8e2b2b373f804a87cf15932606e062f04b8cbc40558c44
    >       Image:         aglais/pyspark-mod:latest
    >       Image ID:      docker-pullable://docker.io/aglais/pyspark-mod@sha256:cf55a2fd0b60a0bcb5de4b6cf1b03db2cf66e5c62ae44c6b93c16e6bda506093
    >       Port:          <none>
    >       Host Port:     <none>
    >       Command:
    >         sh
    >         -c
    >         cp -r /opt/spark/* /spark/
    >       State:          Terminated
    >         Reason:       Completed
    >         Exit Code:    0
    >         Started:      Tue, 22 Dec 2020 12:22:18 +0000
    >         Finished:     Tue, 22 Dec 2020 12:22:18 +0000
    >       Ready:          True
    >       Restart Count:  0
    >       Environment:    <none>
    >       Mounts:
    >         /spark from spark-home (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from default-token-nvvqk (ro)
    >   Containers:
    >     spark:
    >       Container ID:  docker://3c3cd3533c54740a91a4992367ef8964358f589a489acf2ffc9d7980ec9bc922
    >       Image:         aglais/zeppelin-dev:20201222-014518
    >       Image ID:      docker-pullable://docker.io/aglais/zeppelin-dev@sha256:164364ef0071a21cf2abf6f646a9be4f1c34d8682088ac97a04e3fbfeb0f1f9f
    >       Port:          <none>
    >       Host Port:     <none>
    >       Command:
    >         sh
    >         -c
    >         $(ZEPPELIN_HOME)/bin/interpreter.sh -d $(ZEPPELIN_HOME)/interpreter/spark -r 12321:12321 -c zeppelin-server-service.aglais-20201222.svc -p 12320 -i spark-shared_process -l /tmp/local-repo -g spark
    >       State:          Running
    >         Started:      Tue, 22 Dec 2020 12:22:19 +0000
    >       Ready:          True
    >       Restart Count:  0
    >       Limits:
    >         cpu:  1
    >       Requests:
    >         cpu:     1
    >         memory:  1408Mi
    >       Environment:
    >         PYSPARK_PYTHON:         python
    >         ZEPPELIN_HOME:          /zeppelin
    >         SPARK_SUBMIT_OPTIONS:   --master k8s://https://kubernetes.default.svc --deploy-mode client --driver-memory 1g --conf spark.kubernetes.namespace=aglais-20201222 --conf spark.executor.instances=1 --conf spark.kubernetes.driver.pod.name=spark-ctvkkh --conf spark.kubernetes.container.image=aglais/pyspark-mod:latest --conf spark.driver.bindAddress=0.0.0.0 --conf spark.driver.host=spark-ctvkkh.aglais-20201222.svc --conf spark.driver.port=22321 --conf spark.blockManager.port=22322
    >         SPARK_HOME:             /spark
    >         PYSPARK_DRIVER_PYTHON:  python
    >         SERVICE_DOMAIN:         local.zeppelin-project.org:8080
    >         INTERPRETER_GROUP_ID:   spark-shared_process
    >       Mounts:
    >         /data/gaia/dr2 from aglais-gaia-dr2 (ro)
    >         /spark from spark-home (rw)
    >         /user/nch from aglais-user-nch (rw)
    >         /user/stv from aglais-user-stv (rw)
    >         /user/zrq from aglais-user-zrq (rw)
    >         /var/run/secrets/kubernetes.io/serviceaccount from default-token-nvvqk (ro)
    >   Conditions:
    >     Type              Status
    >     Initialized       True
    >     Ready             True
    >     ContainersReady   True
    >     PodScheduled      True
    >   Volumes:
    >     spark-home:
    >       Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    >       Medium:
    >       SizeLimit:  <unset>
    >     aglais-gaia-dr2:
    >       Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    >       ClaimName:  aglais-gaia-dr2-claim
    >       ReadOnly:   false
    >     aglais-user-nch:
    >       Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    >       ClaimName:  aglais-user-nch-claim
    >       ReadOnly:   false
    >     aglais-user-stv:
    >       Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    >       ClaimName:  aglais-user-stv-claim
    >       ReadOnly:   false
    >     aglais-user-zrq:
    >       Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    >       ClaimName:  aglais-user-zrq-claim
    >       ReadOnly:   false
    >     default-token-nvvqk:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  default-token-nvvqk
    >       Optional:    false
    >   QoS Class:       Burstable
    >   Node-Selectors:  <none>
    >   Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
    >                    node.kubernetes.io/unreachable:NoExecute for 300s
    >   Events:          <none>


    #
    # Looks like this is a Zeppelin interpreter node.
    #

    >   ....
    >   Containers:
    >     spark:
    >       Container ID:  docker://3c3cd3533c54740a91a4992367ef8964358f589a489acf2ffc9d7980ec9bc922
    >       Image:         aglais/zeppelin-dev:20201222-014518
    >   ....

    #
    # The list of volumes looks OK.
    #

    >   ....
    >   Volumes:
    >     spark-home:
    >       Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    >       Medium:
    >       SizeLimit:  <unset>
    >     aglais-gaia-dr2:
    >       Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    >       ClaimName:  aglais-gaia-dr2-claim
    >       ReadOnly:   false
    >     aglais-user-nch:
    >       Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    >       ClaimName:  aglais-user-nch-claim
    >       ReadOnly:   false
    >     aglais-user-stv:
    >       Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    >       ClaimName:  aglais-user-stv-claim
    >       ReadOnly:   false
    >     aglais-user-zrq:
    >       Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    >       ClaimName:  aglais-user-zrq-claim
    >       ReadOnly:   false
    >     default-token-nvvqk:
    >       Type:        Secret (a volume populated by a Secret)
    >       SecretName:  default-token-nvvqk
    >       Optional:    false
    >   ....


# -----------------------------------------------------
# Login to the Spark interpreter Pod and check the Gaia data.
#[root@kubernator]

    kubectl \
        --namespace "${namespace:?}" \
        exec \
            --tty \
            --stdin \
            spark-ctvkkh \
            -- \
                bash

    ls -al /data/gaia/dr2/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet

    >   -rw-r--r--. 1 zeppelin 1000 74114220 Oct 24 15:51 /data/gaia/dr2/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet


    md5sum /data/gaia/dr2/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet

    >   3d71b06f5252f7d94c6e7fc8ad379d47  /data/gaia/dr2/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet

        #
        # The Zeppelin %pyspark interpreter can access the file and gets the same md5sum as the Manila testpod.
        # ....
        # So why do we get a FileNotFoundException ?
        #

    >   ....
    >   Caused by:
    >       java.io.FileNotFoundException:
    >           File file:/data/gaia/dr2/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet does not exist
    >   ....


# -----------------------------------------------------
# Check the Java version in the Spark interpreter Pod.
#[root@kubernator]

    #
    # Spark has been updated to work on JDK-11.
    # https://issues.apache.org/jira/browse/SPARK-24417
    # Need to check if we have all of those fixes in our version.
    #

    kubectl \
        --namespace "${namespace:?}" \
        exec \
            --tty \
            --stdin \
            spark-ctvkkh \
            -- \
                java --version

    >   openjdk 11.0.9.1 2020-11-04
    >   OpenJDK Runtime Environment (build 11.0.9.1+1-post-Debian-1deb10u2)
    >   OpenJDK 64-Bit Server VM (build 11.0.9.1+1-post-Debian-1deb10u2, mixed mode, sharing)


# -----------------------------------------------------
# Check the logs for the Spark interpreter Pod.
#[root@kubernator]

    kubectl \
        --namespace "${namespace:?}" \
        logs \
            spark-ctvkkh \
                -c spark

    >   Interpreter launch command:  /spark/bin/spark-submit --class org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer --driver-class-path ":/zeppelin/interpreter/spark/*::/zeppelin/interpreter/zeppelin-interpreter-shaded-0.9.0-aglais-0.0.1.jar:/zeppelin/interpreter/spark/spark-interpreter-0.9.0-aglais-0.0.1.jar" --driver-java-options " -Dfile.encoding=UTF-8 -Dlog4j.configuration='file:///zeppelin/conf/log4j.properties' -Dlog4j.configurationFile='file:///zeppelin/conf/log4j2.properties' -Dzeppelin.log.file='/zeppelin/logs/zeppelin-interpreter-spark-shared_process--spark-ctvkkh.log'" --master k8s://https://kubernetes.default.svc --deploy-mode client --driver-memory 1g --conf spark.kubernetes.namespace=aglais-20201222 --conf spark.executor.instances=1 --conf spark.kubernetes.driver.pod.name=spark-ctvkkh --conf spark.kubernetes.container.image=aglais/pyspark-mod:latest --conf spark.driver.bindAddress=0.0.0.0 --conf spark.driver.host=spark-ctvkkh.aglais-20201222.svc --conf spark.driver.port=22321 --conf spark.blockManager.port=22322 /zeppelin/interpreter/spark/spark-interpreter-0.9.0-aglais-0.0.1.jar zeppelin-server-service.aglais-20201222.svc 12320 "spark-shared_process" 12321:12321
    >   SLF4J: Class path contains multiple SLF4J bindings.
    >   SLF4J: Found binding in [jar:file:/zeppelin/interpreter/spark/spark-interpreter-0.9.0-aglais-0.0.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    >   SLF4J: Found binding in [jar:file:/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    >   SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
    >   SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
    >   WARNING: An illegal reflective access operation has occurred
    >   WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/spark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
    >   WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
    >   WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
    >   WARNING: All illegal access operations will be denied in a future release


    Looks like that warning happens a lot, but no one worries about it.
    https://www.howtoforge.com/how-to-install-apache-spark-on-debian-10/
    https://www.kaggle.com/srivignesh/an-introduction-to-pyspark-apache-spark-in-python

    Where do we go from here ....

    We still have a network error in the dashboard Pod.
    As far as I can tell, we haven't changed anything.
    All the Helm chart versions are the same.

    We have a FileNotFound Exception in Spark.
    Possibly due to a change in Java version from JDK-8 to JDK-11.
    That we can test by downgrading the base image to use an earlier version of Java.
    Might need to step down to an earlier version of Debian to do that.







