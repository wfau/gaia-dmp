#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Email from Nigel:

        Subject: Gaia IRIS platform live system zombified?

        Well it’s happened again. Trying to re-execute a cell in Zeppelin that took 5min first time around, and now it’s gone into navel-gazing mode again:
        while nothing is showing up in the application UI apart from the first stage (successfully completed 37 csv file reads apparently):
        and what’s more there’s nothing recent in the relevant file system, and something weird akin to a stale NFS handle (I know it’s ceph):

        Is it because I’m trying to overwrite I wonder… anyway if you have time to have a look at anything under the hood that you can see then
        let me know and I’ll kill everything that I can from my side, scrub the existing folder and try again.

    Target:

        Restart the Zeppelin and Spark processes on the live deployment.

    Result:

        Zeppelin and Yarn restarted ...


# -----------------------------------------------------
# Restart Zeppelin and tail the logs ..
#[user@desktop]

    ssh fedora@zeppelin.aglais.uk

        pushd zeppelin-0.8.2-bin-all/

            ./bin/zeppelin-daemon.sh restart

    >   Zeppelin stop           [  OK  ]
    >   Zeppelin start          [  OK  ]


            tail -f ./logs/zeppelin-fedora-gaia-prod-20210223-zeppelin.novalocal.log

    >   INFO [2021-03-04 03:27:33,720] ({main} AbstractConnector.java[doStart]:292) - Started ServerConnector@4c361f63{HTTP/1.1,[http/1.1]}{10.10.0.248:8080}
    >   INFO [2021-03-04 03:27:33,720] ({main} Server.java[doStart]:407) - Started @5857ms
    >   INFO [2021-03-04 03:27:33,721] ({main} ZeppelinServer.java[main]:249) - Done, zeppelin server started
    >   ....
    >   ....


# -----------------------------------------------------
# Restart Yarn and tail the logs ..
#[user@desktop]

    ssh fedora@zeppelin.aglais.uk

        ssh master01

    stop-yarn.sh

    >   Stopping nodemanagers
    >   worker03: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   worker02: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   worker01: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   worker04: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   Stopping resourcemanager


    start-yarn.sh

    >   Starting resourcemanager
    >   Starting nodemanagers


            tail -f /var/hadoop/logs/hadoop-fedora-resourcemanager-gaia-prod-20210223-master01.novalocal.log

    >   2021-03-04 03:27:58,551 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: worker03:33585 Node Transitioned from NEW to RUNNING
    >   2021-03-04 03:27:58,551 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: worker01:38579 Node Transitioned from NEW to RUNNING
    >   2021-03-04 03:27:58,558 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node worker03:33585 clusterResource: <memory:43008, vCores:13>
    >   2021-03-04 03:27:58,559 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node worker01:38579 clusterResource: <memory:86016, vCores:26>
    >   2021-03-04 03:27:58,589 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node worker02(cmPort: 36957 httpPort: 8042) registered with capability: <memory:43008, vCores:13>, assigned nodeId worker02:36957
    >   2021-03-04 03:27:58,589 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: worker02:36957 Node Transitioned from NEW to RUNNING
    >   2021-03-04 03:27:58,590 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node worker02:36957 clusterResource: <memory:129024, vCores:39>
    >   2021-03-04 03:27:58,677 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node worker04(cmPort: 41385 httpPort: 8042) registered with capability: <memory:43008, vCores:13>, assigned nodeId worker04:41385
    >   2021-03-04 03:27:58,677 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: worker04:41385 Node Transitioned from NEW to RUNNING
    >   2021-03-04 03:27:58,678 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node worker04:41385 clusterResource: <memory:172032, vCores:52>
    >   ....
    >   ....



