#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#


    Target:

        Test the data-partitioning notebooks.

        Gaia source read from CephFS network shares:

            'file:////user/nch/CSV/GEDR3/*.csv'
            'file:////data/gaia/edr3'

            CephFS service managed via Openstack Manila
            CephFS service provided by Cambridge storage system

        Partitioned data written to HDFS space:

            'hdfs://master01:9000/partitioned/gaia/edr3'

            HDFS system managed by Hadoop VMs
            HDFS data on Cinder volumes
            Cinder volumes provided by Ceph block volumes
            Ceph volumes provided by Cambridge storage system

        Hadoop temp space on local disc

            /var/hadoop/temp -> /mnt/local/vdb/hadoop/temp

            Local volumes are part of Openstack flavors
            Local volumes mounted as extra block devices in the VMs.
            Local volumes map to SSD SATA discs on the host machines

        Continuation from prev notes

            notes/zrq/20210308-02-live-deployment.txt

    Result:

        10% works
        100% fails - out of temp space


# -----------------------------------------------------
# Create the target data directories.
#[user@zeppelin]

    ssh master01 \
        '
        hdfs dfs -mkdir /partitioned
        hdfs dfs -mkdir /partitioned/gaia
        hdfs dfs -mkdir /partitioned/gaia/edr3
        '

# -----------------------------------------------------
# Create the test notebook ..
# Based on example from Nigel and Enrique.
#[user@zeppelin]


    %pyspark

    # number of buckets for our platform
    NUM_BUCKETS = 2048

    # the following based on example code kindly supplied by Enrique Utrilla:

    # Save a dataframe to a set of bucketed parquet files, repartitioning beforehand and sorting by source UID within the buckets:
    def saveToBinnedParquet(df, outputParquetPath, name, mode = "error", nBuckets = NUM_BUCKETS):
        df = df.repartition(nBuckets, "source_id")
        df.write.format("parquet") \
                .mode(mode) \
                .bucketBy(nBuckets, "source_id") \
                .sortBy("source_id") \
                .option("path", outputParquetPath) \
                .saveAsTable(name)




    %pyspark
    import sys

    # 1%:
    #gaia_source_df = sqlContext.read.option('mode','failfast').option('header', 'true').schema(gaia_source_schema).csv('file:////user/nch/CSV/GEDR3/*11.csv')
    # 10%:
    gaia_source_df = sqlContext.read.option('mode','failfast').option('header', 'true').schema(gaia_source_schema).csv('file:////user/nch/CSV/GEDR3/*1.csv')
    # full monty:
    #gaia_source_df = sqlContext.read.option('mode','failfast').option('header', 'true').schema(gaia_source_schema).csv('file:////user/nch/CSV/GEDR3/*.csv')

    saveToBinnedParquet(
        gaia_source_df,
        'hdfs://master01:9000/partitioned/gaia/edr3',
        name = 'gaia_source_bucketed_by_source_id',
        mode = 'overwrite'
        )



    10% Took 10 min 3 sec. Last updated by zrq at March 08 2021, 7:13:36 PM.

    100% failed after 4+hrs


# -----------------------------------------------------
# Check the Zeppelin interpreter logs.
#[fedora@zeppelin]



    >   ....
    >   ....
    >    INFO [2021-03-08 19:39:26,787] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Starting task 2694.2 in stage 185.0 (TID 611682, worker06, executor 15, partition 2694, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:26,788] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 4825.2 in stage 185.0 (TID 611657) in 7837 ms on worker06 (executor 15) (1152/11932)
    >    INFO [2021-03-08 19:39:27,216] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 2371.2 in stage 185.0 (TID 611683, worker03, executor 8, partition 2371, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:27,216] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 4536.2 in stage 185.0 (TID 611668) in 4376 ms on worker03 (executor 8) (1153/11932)
    >   ....
    >   ....
    >    INFO [2021-03-08 19:41:15,070] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Starting task 1871.1 in stage 185.0 (TID 612056, worker02, executor 9, partition 1871, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:41:15,070] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 1371.2 in stage 185.0 (TID 612040) in 4499 ms on worker02 (executor 9) (1526/11932)
    >    INFO [2021-03-08 19:41:15,897] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Disabling executor 8.
    >    INFO [2021-03-08 19:41:15,897] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 8 (epoch 79)
    >    INFO [2021-03-08 19:41:15,897] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Trying to remove executor 8 from BlockManagerMaster.
    >    INFO [2021-03-08 19:41:15,897] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(8, worker03, 36297, None)
    >    INFO [2021-03-08 19:41:15,897] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 8 successfully in removeExecutor
    >    INFO [2021-03-08 19:41:15,897] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 8 (epoch 79)
    >    WARN [2021-03-08 19:41:15,903] ({dispatcher-event-loop-12} Logging.scala[logWarning]:66) - Requesting driver to remove executor 11 for reason Container marked as failed: container_1615204633239_0002_01_000019 on host: worker07. Exit status: -100. Diagnostics: Container released on a *lost* node.
    >    WARN [2021-03-08 19:41:15,904] ({dispatcher-event-loop-12} Logging.scala[logWarning]:66) - Requesting driver to remove executor 15 for reason Container marked as failed: container_1615204633239_0002_01_000023 on host: worker06. Exit status: -100. Diagnostics: Container released on a *lost* node.
    >   ERROR [2021-03-08 19:41:15,904] ({dispatcher-event-loop-12} Logging.scala[logError]:70) - Lost executor 11 on worker07: Container marked as failed: container_1615204633239_0002_01_000019 on host: worker07. Exit status: -100. Diagnostics: Container released on a *lost* node.
    >    WARN [2021-03-08 19:41:15,904] ({dispatcher-event-loop-7} Logging.scala[logWarning]:66) - Requesting driver to remove executor 9 for reason Container marked as failed: container_1615204633239_0002_01_000014 on host: worker02. Exit status: -100. Diagnostics: Container released on a *lost* node.
    >    INFO [2021-03-08 19:41:15,904] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 2740), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,904] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 347), so marking it as still running.
    >   ....
    >   ....
    >    INFO [2021-03-08 19:41:15,982] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 3406), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,982] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 4838), so marking it as still running.
    >   ....
    >   ....
    >    INFO [2021-03-08 19:43:18,034] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.10.3.117:46678) with ID 16
    >    INFO [2021-03-08 19:43:18,035] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 3900.4 in stage 185.0 (TID 612057, worker02, executor 16, partition 3900, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:18,035] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 2194.2 in stage 185.0 (TID 612058, worker02, executor 16, partition 2194, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:18,035] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 1754.2 in stage 185.0 (TID 612059, worker02, executor 16, partition 1754, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:18,035] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 3055.2 in stage 185.0 (TID 612060, worker02, executor 16, partition 3055, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:18,131] ({dispatcher-event-loop-9} Logging.scala[logInfo]:54) - Registering block manager worker02:42479 with 6.8 GB RAM, BlockManagerId(16, worker02, 42479, None)
    >    INFO [2021-03-08 19:43:18,145] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.10.0.55:58852) with ID 17
    >    INFO [2021-03-08 19:43:18,145] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 4838.2 in stage 185.0 (TID 612061, worker03, executor 17, partition 4838, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:18,145] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 3406.2 in stage 185.0 (TID 612062, worker03, executor 17, partition 3406, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:18,145] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 5107.2 in stage 185.0 (TID 612063, worker03, executor 17, partition 5107, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:18,145] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 3641.3 in stage 185.0 (TID 612064, worker03, executor 17, partition 3641, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:18,252] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Registering block manager worker03:46317 with 6.8 GB RAM, BlockManagerId(17, worker03, 46317, None)
    >    INFO [2021-03-08 19:43:18,486] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_226_piece0 in memory on worker02:42479 (size: 11.9 KB, free: 6.8 GB)
    >    INFO [2021-03-08 19:43:18,659] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Added broadcast_226_piece0 in memory on worker03:46317 (size: 11.9 KB, free: 6.8 GB)
    >    INFO [2021-03-08 19:43:19,343] ({dispatcher-event-loop-5} Logging.scala[logInfo]:54) - Added broadcast_225_piece0 in memory on worker02:42479 (size: 25.5 KB, free: 6.8 GB)
    >    INFO [2021-03-08 19:43:19,478] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Added broadcast_225_piece0 in memory on worker03:46317 (size: 25.5 KB, free: 6.8 GB)
    >    INFO [2021-03-08 19:43:25,235] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 1755.2 in stage 185.0 (TID 612065, worker02, executor 16, partition 1755, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:25,236] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 4216.3 in stage 185.0 (TID 612066, worker02, executor 16, partition 4216, PROCESS_LOCAL, 8269 bytes)
    >   ....
    >   ....
    >    INFO [2021-03-08 19:43:25,236] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 1754.2 in stage 185.0 (TID 612059) in 7201 ms on worker02 (executor 16) (1/11932)
    >    INFO [2021-03-08 19:43:25,236] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 3900.4 in stage 185.0 (TID 612057) in 7201 ms on worker02 (executor 16) (2/11932)
    >    INFO [2021-03-08 19:43:25,272] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 4855.3 in stage 185.0 (TID 612067, worker03, executor 17, partition 4855, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:25,272] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 4272.3 in stage 185.0 (TID 612068, worker03, executor 17, partition 4272, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:25,273] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 5107.2 in stage 185.0 (TID 612063) in 7128 ms on worker03 (executor 17) (3/11932)
    >    INFO [2021-03-08 19:43:25,273] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 4838.2 in stage 185.0 (TID 612061) in 7128 ms on worker03 (executor 17) (4/11932)
    >   ....
    >   ....
    >    INFO [2021-03-08 19:45:12,884] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 1891.3 in stage 185.0 (TID 612265, worker02, executor 16, partition 1891, PROCESS_LOCAL, 8269 bytes)
    >    WARN [2021-03-08 19:45:12,887] ({task-result-getter-1} Logging.scala[logWarning]:66) - Lost task 3563.3 in stage 185.0 (TID 612257, worker02, executor 16): java.io.IOException: No space left on device
    >   	at java.io.FileOutputStream.writeBytes(Native Method)
    >   	at java.io.FileOutputStream.write(FileOutputStream.java:326)
    >   ....
    >   ....
    >    INFO [2021-03-08 19:45:15,456] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 4447.3 in stage 185.0 (TID 612271, worker03, executor 17, partition 4447, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:45:15,456] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 2748.3 in stage 185.0 (TID 612263) in 4528 ms on worker03 (executor 17) (204/11932)
    >    INFO [2021-03-08 19:45:16,029] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Disabling executor 16.
    >    INFO [2021-03-08 19:45:16,029] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 16 (epoch 83)
    >    INFO [2021-03-08 19:45:16,029] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Trying to remove executor 16 from BlockManagerMaster.
    >    INFO [2021-03-08 19:45:16,029] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(16, worker02, 42479, None)
    >    INFO [2021-03-08 19:45:16,029] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 16 successfully in removeExecutor
    >    INFO [2021-03-08 19:45:16,029] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 16 (epoch 83)
    >   ERROR [2021-03-08 19:45:16,033] ({dispatcher-event-loop-4} Logging.scala[logError]:70) - Lost executor 16 on worker02: Container marked as failed: container_1615204633239_0002_01_000025 on host: worker02. Exit status: -100. Diagnostics: Container released on a *lost* node.
    >    WARN [2021-03-08 19:45:16,033] ({dispatcher-event-loop-13} Logging.scala[logWarning]:66) - Requesting driver to remove executor 16 for reason Container marked as failed: container_1615204633239_0002_01_000025 on host: worker02. Exit status: -100. Diagnostics: Container released on a *lost* node.
    >    WARN [2021-03-08 19:45:16,034] ({dispatcher-event-loop-13} Logging.scala[logWarning]:66) - Requesting driver to remove executor 17 for reason Container marked as failed: container_1615204633239_0002_01_000026 on host: worker03. Exit status: -100. Diagnostics: Container released on a *lost* node.
    >    INFO [2021-03-08 19:45:16,034] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 3552), so marking it as still running.
    >    INFO [2021-03-08 19:45:16,034] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 3879), so marking it as still running.
    >   ....
    >   ....
    >    INFO [2021-03-08 19:45:16,044] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 2288), so marking it as still running.
    >    INFO [2021-03-08 19:45:16,044] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 4855), so marking it as still running.
    >    INFO [2021-03-08 19:45:16,044] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 17 (epoch 84)
    >    INFO [2021-03-08 19:45:16,044] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Trying to remove executor 17 from BlockManagerMaster.
    >    INFO [2021-03-08 19:45:16,044] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(17, worker03, 46317, None)
    >    INFO [2021-03-08 19:45:16,044] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 17 successfully in removeExecutor
    >    INFO [2021-03-08 19:45:16,045] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 17 (epoch 84)
    >    WARN [2021-03-08 19:45:16,144] ({rpc-server-4-6} TransportChannelHandler.java[exceptionCaught]:78) - Exception in connection from /10.10.0.55:58852
    >   java.io.IOException: Connection reset by peer
    >   	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
    >   	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
    >   	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
    >   	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
    >   	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:377)
    >   	at io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253)
    >   	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1133)
    >   	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)
    >   	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:148)
    >   	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
    >   	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
    >   	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
    >   	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >   	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	at java.lang.Thread.run(Thread.java:748)


# -----------------------------------------------------
# -----------------------------------------------------
# Check the logs on a worker.
#[fedora@worker07]

    pushd /var/hadoop/logs

        less hadoop-fedora-nodemanager-gaia-prod-20210308-worker07.novalocal.log

    >   ....
    >   ....
    >   2021-03-08 18:17:14,463 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 18:27:14,464 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   ....
    >   ....
    >   2021-03-08 19:31:14,237 WARN org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir error, used space above threshold of 90.0%, removing from list of valid directories
    >   2021-03-08 19:31:14,238 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) failed: 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:31:14,238 ERROR org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Most of the disks failed. 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:31:16,220 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000003 transitioned from RUNNING to KILLING
    >   2021-03-08 19:31:16,220 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1615204633239_0002_01_000003
    >   2021-03-08 19:31:16,295 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1615204633239_0002_01_000003 is : 143
    >   2021-03-08 19:31:16,308 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher: Got exception while cleaning container container_1615204633239_0002_01_000003. Ignoring.
    >   2021-03-08 19:31:16,308 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000003 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
    >   2021-03-08 19:31:16,309 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/container_1615204633239_0002_01_000003
    >   2021-03-08 19:31:16,309 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=fedora       OPERATION=Container Finished - Killed   TARGET=ContainerImpl    RESULT=SUCCESS  APPID=application_1615204633239_0002    CONTAINERID=container_1615204633239_0002_01_000003
    >   2021-03-08 19:31:16,310 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000003 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
    >   2021-03-08 19:31:16,310 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1615204633239_0002_01_000003 from application application_1615204633239_0002
    >   2021-03-08 19:31:16,312 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1615204633239_0002_01_000003
    >   2021-03-08 19:31:16,312 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1615204633239_0002
    >   2021-03-08 19:31:18,314 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1615204633239_0002_01_000003]
    >   2021-03-08 19:33:14,231 INFO org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir passed disk check, adding to list of valid directories.
    >   2021-03-08 19:33:14,232 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) turned good: 1/1 local-dirs are good: /var/hadoop/temp/nm-local-dir; 1/1 log-dirs are good: /var/hadoop/logs/userlogs
    >   ....
    >   ....


# -----------------------------------------------------
# Check the available disc space.
#[fedora@worker07]

    ls -al /var/hadoop/temp

    >   lrwxrwxrwx. 1 root root 26 Mar  8 11:44 /var/hadoop/temp -> /mnt/local/vdb/hadoop/temp


    df -h /var/hadoop/temp/nm-local-dir

    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdb         59G   51G  5.1G  91% /mnt/local/vdb


    du -h /var/hadoop/temp/

    >   ....
    >   ....
    >   26G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-0ae9fbe4-f7de-46ce-84c4-e0670f6de717
    >   51G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002
    >   51G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache
    >   51G     /var/hadoop/temp/nm-local-dir/usercache/fedora
    >   51G     /var/hadoop/temp/nm-local-dir/usercache
    >   51G     /var/hadoop/temp/nm-local-dir
    >   51G     /var/hadoop/temp/


# -----------------------------------------------------
# -----------------------------------------------------
# Check the logs on another worker.
#[fedora@worker05]

    pushd /var/hadoop/logs

        less hadoop-fedora-nodemanager-gaia-prod-20210308-worker05.novalocal.log

    >   ....
    >   ....
    >   2021-03-08 19:17:14,497 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:27:14,497 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:29:14,261 WARN org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir error, used space above threshold of 90.0%, removing from list of valid directories
    >   2021-03-08 19:29:14,718 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) failed: 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir :
    >    used space above threshold of 90.0% ] ;
    >   2021-03-08 19:29:14,718 ERROR org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Most of the disks failed. 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-
    >   local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:29:15,614 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000006 transitioned from RUNNING to KILLING
    >   2021-03-08 19:29:15,614 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1615204633239_0002_01_000006
    >   2021-03-08 19:29:15,672 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1615204633239_0002_01_000006 is : 143
    >   2021-03-08 19:29:15,685 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher: Got exception while cleaning container container_1615204633239_0002_01_000006. Ignoring.
    >   2021-03-08 19:29:15,685 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000006 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
    >   2021-03-08 19:29:15,686 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/container_1615204633239_0002
    >   _01_000006
    >   2021-03-08 19:29:15,686 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=fedora       OPERATION=Container Finished - Killed   TARGET=ContainerImpl    RESULT=SUCCESS  APPID=application_1615204633239_0002    CONTAINERID=c
    >   ontainer_1615204633239_0002_01_000006
    >   2021-03-08 19:29:15,686 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000006 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
    >   2021-03-08 19:29:15,686 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1615204633239_0002_01_000006 from application application_1615204633239_0002
    >   2021-03-08 19:29:15,687 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1615204633239_0002_01_000006
    >   2021-03-08 19:29:15,687 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1615204633239_0002
    >   2021-03-08 19:29:18,692 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1615204633239_0002_01_000006]
    >   2021-03-08 19:31:14,256 INFO org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir passed disk check, adding to list of valid directories.
    >   2021-03-08 19:31:14,257 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) turned good: 1/1 local-dirs are good: /var/hadoop/temp/nm-local-dir; 1/1 log-dirs are good: /var/hadoop/logs/userlogs
    >   2021-03-08 19:31:18,286 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1615204633239_0002_000001 (auth:SIMPLE)
    >   2021-03-08 19:31:18,295 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1615204633239_0002_01_000015 by user fedora
    >   2021-03-08 19:31:18,296 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=fedora       IP=10.10.2.213  OPERATION=Start Container Request       TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_16152046332
    >   39_0002    CONTAINERID=container_1615204633239_0002_01_000015
    >
    >   2021-03-08 19:31:18,298 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000015 transitioned from NEW to LOCALIZING
    >   2021-03-08 19:31:18,298 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1615204633239_0002
    >   2021-03-08 19:31:18,298 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000015 transitioned from LOCALIZING to SCHEDULED
    >   2021-03-08 19:31:18,298 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler: Starting container [container_1615204633239_0002_01_000015]
    >   2021-03-08 19:31:18,311 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000015 transitioned from SCHEDULED to RUNNING
    >   2021-03-08 19:31:18,311 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1615204633239_0002_01_000015
    >   2021-03-08 19:31:18,313 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/container_1615204633239_0002_01_000015/default_container_executor.sh]
    >   2021-03-08 19:31:19,997 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: container_1615204633239_0002_01_000015's ip = 10.10.3.12, and hostname = worker05
    >   2021-03-08 19:31:20,005 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Skipping monitoring container container_1615204633239_0002_01_000015 since CPU usage is not yet available.
    >   2021-03-08 19:37:14,256 WARN org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir error, used space above threshold of 90.0%, removing from list of valid directories
    >   2021-03-08 19:37:14,256 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) failed: 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:37:14,256 ERROR org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Most of the disks failed. 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:37:14,497 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:37:15,282 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000015 transitioned from RUNNING to KILLING
    >   2021-03-08 19:37:15,283 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1615204633239_0002_01_000015
    >   2021-03-08 19:37:15,324 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1615204633239_0002_01_000015 is : 143
    >   2021-03-08 19:37:15,335 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher: Got exception while cleaning container container_1615204633239_0002_01_000015. Ignoring.
    >   2021-03-08 19:37:15,336 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000015 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
    >   2021-03-08 19:37:15,336 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=fedora       OPERATION=Container Finished - Killed   TARGET=ContainerImpl    RESULT=SUCCESS  APPID=application_1615204633239_0002    CONTAINERID=container_1615204633239_0002_01_000015
    >   2021-03-08 19:37:15,336 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/container_1615204633239_0002_01_000015
    >   2021-03-08 19:37:15,336 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000015 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
    >   2021-03-08 19:37:15,337 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1615204633239_0002_01_000015 from application application_1615204633239_0002
    >   2021-03-08 19:37:15,337 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1615204633239_0002_01_000015
    >   2021-03-08 19:37:15,337 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1615204633239_0002
    >   2021-03-08 19:37:16,339 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1615204633239_0002_01_000015]
    >   2021-03-08 19:47:14,498 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:57:14,499 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   ....
    >   ....
    >   2021-03-08 20:27:14,499 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 20:37:14,500 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0


# Check the available disc space.
#[fedora@worker05]

    ls -al /var/hadoop/temp

    >   lrwxrwxrwx. 1 root root 26 Mar  8 11:44 /var/hadoop/temp -> /mnt/local/vdb/hadoop/temp


    df -h /var/hadoop/temp/nm-local-dir

    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdb         59G   53G  3.9G  94% /mnt/local/vdb


    du -h /var/hadoop/temp/

    >   ....
    >   ....
    >   414M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-f65cd889-ebc8-4d40-a3f7-39a38d6eb5d2/0c
    >   761M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-f65cd889-ebc8-4d40-a3f7-39a38d6eb5d2/33
    >   35G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-f65cd889-ebc8-4d40-a3f7-39a38d6eb5d2
    >   52G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002
    >   52G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache
    >   52G     /var/hadoop/temp/nm-local-dir/usercache/fedora
    >   52G     /var/hadoop/temp/nm-local-dir/usercache
    >   52G     /var/hadoop/temp/nm-local-dir
    >   52G     /var/hadoop/temp/



# -----------------------------------------------------
# -----------------------------------------------------
# Check the logs on another worker.
#[fedora@worker03]

    pushd /var/hadoop/logs

        less hadoop-fedora-nodemanager-gaia-prod-20210308-worker03.novalocal.log

    >   ....
    >   ....
    >   2021-03-08 19:45:14,139 WARN org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir error, used space above threshold of 90.0%, removing from list of valid directories
    >   2021-03-08 19:45:14,139 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) failed: 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:45:14,139 ERROR org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Most of the disks failed. 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:45:15,543 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000026 transitioned from RUNNING to KILLING
    >   2021-03-08 19:45:15,543 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1615204633239_0002_01_000026
    >   2021-03-08 19:45:15,577 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1615204633239_0002_01_000026 is : 143
    >   2021-03-08 19:45:15,591 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher: Got exception while cleaning container container_1615204633239_0002_01_000026. Ignoring.
    >   2021-03-08 19:45:15,592 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000026 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
    >   2021-03-08 19:45:15,592 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/container_1615204633239_0002_01_000026
    >   2021-03-08 19:45:15,592 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=fedora       OPERATION=Container Finished - Killed   TARGET=ContainerImpl    RESULT=SUCCESS  APPID=application_1615204633239_0002    CONTAINERID=container_1615204633239_0002_01_000026
    >   2021-03-08 19:45:15,592 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000026 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
    >   2021-03-08 19:45:15,592 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1615204633239_0002_01_000026 from application application_1615204633239_0002
    >   2021-03-08 19:45:15,593 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1615204633239_0002_01_000026
    >   2021-03-08 19:45:15,593 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1615204633239_0002
    >   2021-03-08 19:45:16,595 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1615204633239_0002_01_000026]
    >   ....
    >   ....


# -----------------------------------------------------
# Check the available disc space.
#[fedora@worker03]

    ls -al /var/hadoop/temp

    >   lrwxrwxrwx. 1 root root 26 Mar  8 11:44 /var/hadoop/temp -> /mnt/local/vdb/hadoop/temp


    df -h /var/hadoop/temp/nm-local-dir

    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdb         59G   51G  5.5G  91% /mnt/local/vdb


    du -h /var/hadoop/temp/

    >   ....
    >   ....
    >   555M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-b59abf63-5994-42b8-8640-6ea8aa01dafe/38
    >   622M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-b59abf63-5994-42b8-8640-6ea8aa01dafe/02
    >   690M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-b59abf63-5994-42b8-8640-6ea8aa01dafe/03
    >   554M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-b59abf63-5994-42b8-8640-6ea8aa01dafe/1c
    >   693M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-b59abf63-5994-42b8-8640-6ea8aa01dafe/0f
    >   553M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-b59abf63-5994-42b8-8640-6ea8aa01dafe/08
    >   30G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-b59abf63-5994-42b8-8640-6ea8aa01dafe
    >   21M     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/spark-87942b10-5b09-4aa8-9bf3-68b100939d9c
    >   21M     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/spark-d9756900-68b6-4338-800e-3e817e17099c
    >   50G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002
    >   50G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache
    >   51G     /var/hadoop/temp/nm-local-dir/usercache/fedora
    >   51G     /var/hadoop/temp/nm-local-dir/usercache
    >   51G     /var/hadoop/temp/nm-local-dir
    >   51G     /var/hadoop/temp/



# -----------------------------------------------------
# -----------------------------------------------------
# Check the logs on another worker.
#[fedora@worker01]

    pushd /var/hadoop/logs

        less hadoop-fedora-nodemanager-gaia-prod-20210308-worker01.novalocal.log

    >   ....
    >   2021-03-08 19:31:18,910 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1615204633239_0002_01_000002]
    >   2021-03-08 19:33:14,299 INFO org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir passed disk check, adding to list of valid directories.
    >   2021-03-08 19:33:14,300 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) turned good: 1/1 local-dirs are good: /var/hadoop/temp/nm-local-dir; 1/1 log-dirs are good: /var/hadoop/logs/userlogs
    >   2021-03-08 19:33:17,363 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1615204633239_0002_000001 (auth:SIMPLE)
    >   2021-03-08 19:33:17,368 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1615204633239_0002_01_000022 by user fedora
    >   2021-03-08 19:33:17,370 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=fedora       IP=10.10.2.213  OPERATION=Start Container Request       TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_16152046332
    >   39_0002    CONTAINERID=container_1615204633239_0002_01_000022
    >   2021-03-08 19:33:17,370 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1615204633239_0002_01_000022 to application application_1615204633239_0002
    >   2021-03-08 19:33:17,372 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000022 transitioned from NEW to LOCALIZING
    >   2021-03-08 19:33:17,372 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1615204633239_0002
    >   2021-03-08 19:33:17,373 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000022 transitioned from LOCALIZING to SCHEDULED
    >   2021-03-08 19:33:17,373 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler: Starting container [container_1615204633239_0002_01_000022]
    >   2021-03-08 19:33:17,387 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000022 transitioned from SCHEDULED to RUNNING
    >   2021-03-08 19:33:17,388 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1615204633239_0002_01_000022
    >   2021-03-08 19:33:17,390 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/container_1615204633239_0002_01_000022/default_container_executor.sh]
    >   2021-03-08 19:33:18,306 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: container_1615204633239_0002_01_000022's ip = 10.10.1.196, and hostname = worker01
    >   2021-03-08 19:33:18,315 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Skipping monitoring container container_1615204633239_0002_01_000022 since CPU usage is not yet available.
    >   2021-03-08 19:35:14,298 WARN org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir error, used space above threshold of 90.0%, removing from list of valid directories
    >   2021-03-08 19:35:14,299 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) failed: 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:35:14,300 ERROR org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Most of the disks failed. 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:35:16,193 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000022 transitioned from RUNNING to KILLING
    >   2021-03-08 19:35:16,193 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1615204633239_0002_01_000022
    >   2021-03-08 19:35:16,300 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1615204633239_0002_01_000022 is : 143
    >   2021-03-08 19:35:16,300 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher: Got exception while cleaning container container_1615204633239_0002_01_000022. Ignoring.
    >   2021-03-08 19:35:16,300 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000022 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
    >   2021-03-08 19:35:16,301 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/container_1615204633239_0002_01_000022
    >   2021-03-08 19:35:16,301 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=fedora       OPERATION=Container Finished - Killed   TARGET=ContainerImpl    RESULT=SUCCESS  APPID=application_1615204633239_0002    CONTAINERID=container_1615204633239_0002_01_000022
    >   2021-03-08 19:35:16,301 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000022 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
    >   2021-03-08 19:35:16,301 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1615204633239_0002_01_000022 from application application_1615204633239_0002
    >   2021-03-08 19:35:16,301 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1615204633239_0002_01_000022
    >   2021-03-08 19:35:16,301 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1615204633239_0002
    >   2021-03-08 19:35:17,304 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1615204633239_0002_01_000022]
    >   2021-03-08 19:37:14,558 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:47:14,558 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:57:14,559 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 20:07:14,559 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 20:17:14,559 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 20:27:14,559 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   ....


# -----------------------------------------------------
# Check the available disc space.
#[fedora@worker01]

    df -h /var/hadoop/temp/nm-local-dir

    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdb         59G   54G  2.6G  96% /mnt/local/vdb


    du -h /var/hadoop/temp/

    >   ....
    >   ....
    >   139M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-61e99471-185a-45f0-9d55-258738003e28/3b
    >   139M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-61e99471-185a-45f0-9d55-258738003e28/3c
    >   139M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-61e99471-185a-45f0-9d55-258738003e28/04
    >   5.3G    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-61e99471-185a-45f0-9d55-258738003e28
    >   53G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002
    >   53G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache
    >
    >   469M	/var/hadoop/temp/nm-local-dir/usercache/fedora/filecache
    >   54G     /var/hadoop/temp/nm-local-dir/usercache/fedora
    >   54G     /var/hadoop/temp/nm-local-dir/usercache
    >   54G     /var/hadoop/temp/nm-local-dir
    >   54G     /var/hadoop/temp/

