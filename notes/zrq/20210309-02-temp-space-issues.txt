#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Test the data-partitioning notebooks.

        Continuation from prev notes

            notes/zrq/20210308-02-live-deployment.txt

    Result:

        Work in progress




# -----------------------------------------------------
# Create the target data directories.
#[user@zeppelin]

    ssh master01 \
        '
        hdfs dfs -mkdir /partitioned
        hdfs dfs -mkdir /partitioned/gaia
        hdfs dfs -mkdir /partitioned/gaia/edr3
        '

# -----------------------------------------------------
# Create the test notebook ..
# Based on example from Nigel and Enrique.
#[user@zeppelin]


        %pyspark

        # number of buckets for our platform
        NUM_BUCKETS = 2048

        # the following based on example code kindly supplied by Enrique Utrilla:

        # Save a dataframe to a set of bucketed parquet files, repartitioning beforehand and sorting by source UID within the buckets:
        def saveToBinnedParquet(df, outputParquetPath, name, mode = "error", nBuckets = NUM_BUCKETS):
            df = df.repartition(nBuckets, "source_id")
            df.write.format("parquet") \
                    .mode(mode) \
                    .bucketBy(nBuckets, "source_id") \
                    .sortBy("source_id") \
                    .option("path", outputParquetPath) \
                    .saveAsTable(name)



        %pyspark
        import sys

        # 1%:
        #gaia_source_df = sqlContext.read.option('mode','failfast').option('header', 'true').schema(gaia_source_schema).csv('file:////user/nch/CSV/GEDR3/*11.csv')
        # 10%:
        #gaia_source_df = sqlContext.read.option('mode','failfast').option('header', 'true').schema(gaia_source_schema).csv('file:////user/nch/CSV/GEDR3/*1.csv')
        # full monty:
        gaia_source_df = sqlContext.read.option('mode','failfast').option('header', 'true').schema(gaia_source_schema).csv('file:////user/nch/CSV/GEDR3/*.csv')

        saveToBinnedParquet(
            gaia_source_df,
            'hdfs://master01:9000/partitioned/gaia/edr3',
            name = 'gaia_source_bucketed_by_source_id',
            mode = 'overwrite'
            )




    10% Took 8 min 18 sec. Last updated by zrq at March 09 2021, 4:21:17 AM.
