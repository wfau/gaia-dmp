#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Request to change the live deplyment configuration to 16 tiny worker nodes.
        https://github.com/wfau/aglais/issues/444

    Result:

        Fails to run test notebook.
        AttributeError: 'NoneType' object has no attribute 'count'


# -----------------------------------------------------
# Create a new branch.
#[user@desktop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

            nextbranch=$(date '+%Y%m%d')-zrq-deployment

            git checkout master

            git checkout -b "${nextbranch:?}"

            git push --set-upstream origin "${nextbranch:?}"

    popd


# -----------------------------------------------------
# Edit the configuration.
#[user@desktop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        gedit deployments/hadoop-yarn/ansible/hosts.yml

                 workers:
                     hosts:
        -                worker[01:04]:
        +                worker[01:16]:
                     vars:
                         login:  'fedora'
                         image:  'Fedora-30-1.2'
        -                flavor: 'general.v1.medium'
        +                flavor: 'general.v1.tiny'


    popd


# -----------------------------------------------------
# Create a container to work with.
# (*) explicitly set the clound name
#[user@desktop]

    source "${HOME:?}/aglais.env"

    AGLAIS_CLOUD=gaia-prod

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        atolmis/ansible-client:2020.12.02 \
        bash


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"

--START--
real    3m16.744s
user    1m8.875s
sys     0m9.966s
--END--


# -----------------------------------------------------
# Create everything.
#[root@ansibler]

    time \
        /deployments/hadoop-yarn/bin/create-all.sh \
            "${cloudname:?}"

--START--
---- ---- ----
Config yml [/tmp/aglais-config.yml]
Cloud name [gaia-prod]
Build name [aglais-20210422]
Deployment [gaia-prod-20210422]
---- ---- ----
....
....
TASK [Create Cinder volumes for [worker06]] *******
....
failed: [localhost] (
    item={
        'type': 'cinder',
        'size': 1024,
        'format': 'btrfs',
        'mntpath': '/mnt/cinder/vdc',
        'devname': 'vdc'
        }
    ) => {
        "ansible_loop_var": "item",
        "changed": false,
        "item": {
            "devname": "vdc",
            "format": "btrfs",
            "mntpath": "/mnt/cinder/vdc",
            "size": 1024,
            "type": "cinder"
            },
        "msg": "Error in creating volume:
            Client Error for url: https://cumulus.openstack.hpc.cam.ac.uk:8776/v3/21b4ae3a2ea44bc5a9c14005ed2963af/volumes,
            VolumeSizeExceedsAvailableQuota:
                Requested volume or snapshot exceeds allowed gigabytes quota.
                Requested 1024G, quota is 5120G and 5120G has been consumed."
                }
--END--

--START--
real    21m12.637s
user    4m46.044s
sys     1m13.076s
--END--


# -----------------------------------------------------
# -----------------------------------------------------
# Edit the configuration.
#[user@desktop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        gedit deployments/hadoop-yarn/ansible/hosts.yml

                 workers:
                     hosts:
                         worker[01:16]:
                     vars:
                         login:  'fedora'
                         image:  'Fedora-30-1.2'
                         flavor: 'general.v1.tiny'
                         discs:
                         - type: 'local'
                           format: 'ext4'
                           mntpath: "/mnt/local/vdb"
                           devname: 'vdb'
                         - type: 'cinder'
        -                  size: 1024
        +                  size: 200
                           format: 'btrfs'
                           mntpath: "/mnt/cinder/vdc"
                           devname: 'vdc'


    popd


# -----------------------------------------------------
# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"

--START--
real    3m24.619s
user    1m17.286s
sys     0m10.896s
--END--


# -----------------------------------------------------
# Create everything.
#[root@ansibler]

    time \
        /deployments/hadoop-yarn/bin/create-all.sh \
            "${cloudname:?}"

--START--
---- ---- ----
Config yml [/tmp/aglais-config.yml]
Cloud name [gaia-prod]
Build name [aglais-20210422]
Deployment [gaia-prod-20210422]
---- ---- ----
....
....
TASK [Mount [ext4] [/dev/vdb] at [/mnt/local/vdb]] ****
fatal: [worker04]: FAILED! => {"changed": false, "msg": "Error mounting /mnt/local/vdb: mount: /mnt/local/vdb: wrong fs type, bad option, bad superblock on /dev/vdb, missing codepage or helper program, or other error.\n"}
fatal: [worker01]: FAILED! => {"changed": false, "msg": "Error mounting /mnt/local/vdb: mount: /mnt/local/vdb: wrong fs type, bad option, bad superblock on /dev/vdb, missing codepage or helper program, or other error.\n"}
fatal: [worker05]: FAILED! => {"changed": false, "msg": "Error mounting /mnt/local/vdb: mount: /mnt/local/vdb: wrong fs type, bad option, bad superblock on /dev/vdb, missing codepage or helper program, or other error.\n"}
fatal: [worker02]: FAILED! => {"changed": false, "msg": "Error mounting /mnt/local/vdb: mount: /mnt/local/vdb: wrong fs type, bad option, bad superblock on /dev/vdb, missing codepage or helper program, or other error.\n"}
fatal: [worker03]: FAILED! => {"changed": false, "msg": "Error mounting /mnt/local/vdb: mount: /mnt/local/vdb: wrong fs type, bad option, bad superblock on /dev/vdb, missing codepage or helper program, or other error.\n"}
fatal: [worker07]: FAILED! => {"changed": false, "msg": "Error mounting /mnt/local/vdb: mount: /mnt/local/vdb: wrong fs type, bad option, bad superblock on /dev/vdb, missing codepage or helper program, or other error.\n"}
fatal: [worker10]: FAILED! => {"changed": false, "msg": "Error mounting /mnt/local/vdb: mount: /mnt/local/vdb: wrong fs type, bad option, bad superblock on /dev/vdb, missing codepage or helper program, or other error.\n"}
fatal: [worker09]: FAILED! => {"changed": false, "msg": "Error mounting /mnt/local/vdb: mount: /mnt/local/vdb: wrong fs type, bad option, bad superblock on /dev/vdb, missing codepage or helper program, or other error.\n"}
fatal: [worker08]: FAILED! => {"changed": false, "msg": "Error mounting /mnt/local/vdb: mount: /mnt/local/vdb: wrong fs type, bad option, bad superblock on /dev/vdb, missing codepage or helper program, or other error.\n"}
fatal: [worker11]: FAILED! => {"changed": false, "msg": "Error mounting /mnt/local/vdb: mount: /mnt/local/vdb: wrong fs type, bad option, bad superblock on /dev/vdb, missing codepage or helper program, or other error.\n"}
fatal: [worker12]: FAILED! => {"changed": false, "msg": "Error mounting /mnt/local/vdb: mount: /mnt/local/vdb: wrong fs type, bad option, bad superblock on /dev/vdb, missing codepage or helper program, or other error.\n"}
fatal: [worker15]: FAILED! => {"changed": false, "msg": "Error mounting /mnt/local/vdb: mount: /mnt/local/vdb: wrong fs type, bad option, bad superblock on /dev/vdb, missing codepage or helper program, or other error.\n"}
fatal: [worker13]: FAILED! => {"changed": false, "msg": "Error mounting /mnt/local/vdb: mount: /mnt/local/vdb: wrong fs type, bad option, bad superblock on /dev/vdb, missing codepage or helper program, or other error.\n"}
fatal: [worker14]: FAILED! => {"changed": false, "msg": "Error mounting /mnt/local/vdb: mount: /mnt/local/vdb: wrong fs type, bad option, bad superblock on /dev/vdb, missing codepage or helper program, or other error.\n"}
fatal: [worker16]: FAILED! => {"changed": false, "msg": "Error mounting /mnt/local/vdb: mount: /mnt/local/vdb: wrong fs type, bad option, bad superblock on /dev/vdb, missing codepage or helper program, or other error.\n"}
....
....
PLAY RECAP ********************************************
localhost                  : ok=107  changed=80   unreachable=0    failed=0    skipped=2    rescued=0    ignored=0
master01                   : ok=70   changed=52   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
worker01                   : ok=12   changed=8    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0
worker02                   : ok=12   changed=8    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0
worker03                   : ok=12   changed=8    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0
worker04                   : ok=12   changed=8    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0
worker05                   : ok=12   changed=8    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0
worker06                   : ok=6    changed=3    unreachable=0    failed=1    skipped=0    rescued=0    ignored=1
worker07                   : ok=12   changed=8    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0
worker08                   : ok=12   changed=8    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0
worker09                   : ok=12   changed=8    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0
worker10                   : ok=12   changed=8    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0
worker11                   : ok=12   changed=8    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0
worker12                   : ok=12   changed=8    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0
worker13                   : ok=12   changed=8    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0
worker14                   : ok=12   changed=8    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0
worker15                   : ok=12   changed=8    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0
worker16                   : ok=12   changed=8    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0
zeppelin                   : ok=76   changed=62   unreachable=0    failed=0    skipped=1    rescued=0    ignored=0
--END--

--START--
real    36m39.576s
user    8m4.963s
sys     1m57.923s
--END--

    #
    # Found the reason for the error.
    # Tiny flavor don't have an ephemeral disc.
    # They only have a single 12G disc as part of the VM image.
    #

# -----------------------------------------------------
# List the available block devices.
#[root@ansibler]

    ssh zeppelin lsblk

--START--
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda    252:0    0   20G  0 disk
└─vda1 252:1    0   20G  0 part /
vdb    252:16   0   60G  0 disk /mnt/local/vdb
vdc    252:32   0  512G  0 disk /mnt/cinder/vdb
--END--


    ssh worker01 lsblk

--START--
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda    252:0    0   12G  0 disk
└─vda1 252:1    0   12G  0 part /
vdb    252:16   0  200G  0 disk
--END--


# -----------------------------------------------------
# -----------------------------------------------------
# Edit the configuration.
#[user@desktop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        gedit deployments/hadoop-yarn/ansible/hosts.yml

                ....
                ....

                hddatalink: "/var/hadoop/data"
            -   hddatadest: "/mnt/cinder/vdc/hadoop/data"
            +   hddatadest: "/mnt/cinder/data-two/hadoop/data"

                hdtemplink: "/var/hadoop/temp"
            -   hdtempdest: "/mnt/cinder/vdc/hadoop/temp"
            +   hdtempdest: "/mnt/cinder/data-two/hadoop/temp"

                hdlogslink: "/var/hadoop/logs"
            -   hdlogsdest: "/mnt/cinder/vdc/hadoop/logs"
            +   hdlogsdest: "/mnt/cinder/data-two/hadoop/logs"

                ....
                ....

                hdfsmetalink: "/var/hdfs/meta"
            -   hdfsmetadest: "/mnt/cinder/vdc/hdfs/meta"
            +   hdfsmetadest: "/mnt/cinder/data-two/hdfs/meta"

                hdfslogslink: "/var/hdfs/logs"
            -   hdfslogsdest: "/mnt/cinder/vdc/hdfs/logs"
            +   hdfslogsdest: "/mnt/cinder/data-two/hdfs/logs"

                hdfsdatalink: "/var/hdfs/data"
            -   hdfsdatadest: "/mnt/cinder/vdc/hdfs/data"
            +   hdfsdatadest: "/mnt/cinder/data-two/hdfs/data"

                ....
                ....

                zeppelin:
                    login:  'fedora'
                    image:  'Fedora-30-1.2'
                    flavor: 'general.v1.medium'
                    discs:
                      - type: 'local'
                        format: 'ext4'
            -           mntpath: "/mnt/local/vdb"
            +           mntpath: "/mnt/local/data-one"
                        devname: 'vdb'
                      - type: 'cinder'
                        size: 512
                        format: 'btrfs'
            -           mntpath: "/mnt/cinder/vdc"
            +           mntpath: "/mnt/cinder/data-two"
                        devname: 'vdc'

                workers:
                    hosts:
                         worker[01:16]:
                    vars:
                         login:  'fedora'
                         image:  'Fedora-30-1.2'
                         flavor: 'general.v1.tiny'
                         discs:
            -            - type: 'local'
            -              format: 'ext4'
            -              mntpath: "/mnt/local/data-one"
            -              devname: 'vdb'
                         - type: 'cinder'
                           size: 200
                           format: 'btrfs'
            -              mntpath: "/mnt/cinder/vdc"
            +              mntpath: "/mnt/cinder/data-two"
            -              devname: 'vdc'
            +              devname: 'vdb'


    popd


    #
    # There are other issues too.
    # zeppelin, master and workers have different number of discs.
    # Hadoop data ends up in different places.
    # Do we actually need Hadoop data on all of the nodes ?
    # Do we actually need Hadoop temp on all of the nodes ?
    #

    #
    # This should be good enough for now, but we need to have separate paths for zeppelin, master and workers.
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"

--START--
real    4m11.968s
user    1m34.159s
sys     0m13.614s
--END--


# -----------------------------------------------------
# Create everything.
#[root@ansibler]

    time \
        /deployments/hadoop-yarn/bin/create-all.sh \
            "${cloudname:?}"

--START--
---- ---- ----
Config yml [/tmp/aglais-config.yml]
Cloud name [gaia-prod]
Build name [aglais-20210422]
Deployment [gaia-prod-20210422]
---- ---- ----
....
....
--END--

--START--
real    74m26.023s
user    16m56.925s
sys     6m45.457s
--END--


# -----------------------------------------------------
# Add the Zeppelin user accounts.
#[root@ansibler]

    ssh zeppelin

        pushd zeppelin-0.8.2-bin-all/
            pushd conf/

                # Manual edit to add names and passwords
                vi shiro.ini

                    ....
                    ....
            popd

            # Restart Zeppelin for the changes to take.
            ./bin/zeppelin-daemon.sh restart

        popd


# -----------------------------------------------------
# Check the deployment status.
#[root@ansibler]

    cat '/tmp/aglais-status.yml'

--START--
aglais:
  status:
    deployment:
      type: hadoop-yarn
      name: gaia-prod-20210422
      date: 20210422T152121
  spec:
    openstack:
      cloud: gaia-prod
--END--


# -----------------------------------------------------
# Get the public IP address of our Zeppelin node.
#[root@ansibler]

    deployname=$(
        yq read \
            '/tmp/aglais-status.yml' \
                'aglais.status.deployment.name'
        )

    zeppelinid=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            server list \
                --format json \
        | jq -r '.[] | select(.Name == "'${deployname:?}'-zeppelin") | .ID'
        )

    zeppelinip=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            server show \
                --format json \
                "${zeppelinid:?}" \
        | jq -r '.addresses' \
        | sed '
            s/[[:space:]]//
            s/.*=\(.*\)/\1/
            s/.*,\(.*\)/\1/
            '
        )

cat << EOF
Zeppelin ID [${zeppelinid:?}]
Zeppelin IP [${zeppelinip:?}]
EOF

--START--
Zeppelin ID [40c6d6ec-6c79-4bb0-8728-9360b1d0de0d]
Zeppelin IP [128.232.227.227]
--END--


# -----------------------------------------------------
# Update our DNS entries.
#[root@ansibler]

    ssh root@infra-ops.aglais.uk

        vi /var/aglais/dnsmasq/hosts/gaia-prod.hosts

        ~   128.232.227.227  zeppelin.gaia-prod.aglais.uk


        podman kill --signal SIGHUP dnsmasq

        podman logs dnsmasq | tail

--START--
dnsmasq[1]: cleared cache
dnsmasq[1]: read /etc/dnsmasq/hosts/gaia-prod.hosts - 1 addresses
dnsmasq[1]: read /etc/dnsmasq/hosts/gaia-test.hosts - 1 addresses
dnsmasq[1]: read /etc/dnsmasq/hosts/gaia-dev.hosts - 1 addresses
--END--


# -----------------------------------------------------
# Check our DNS entries.
#[root@ansibler]

    sudo dnf install -y bind-utils

    dig '@infra-ops.aglais.uk' "zeppelin.${cloudname:?}.aglais.uk"

--START--
....
;; QUESTION SECTION:
;zeppelin.gaia-prod.aglais.uk.	IN	A

;; ANSWER SECTION:
zeppelin.gaia-prod.aglais.uk. 300 IN	A	128.232.227.227
....
--END--


# -----------------------------------------------------
# Login to our Zeppelin node and check the data shares.
#[root@ansibler]

    sharelist='/deployments/common/manila/datashares.yaml'

    for shareid in $(
        yq read "${sharelist:?}" 'shares.[*].id'
        )
    do
        echo ""
        echo "---- ----"
        echo "Share [${shareid:?}]"
        echo "----"

        sharename=$(yq read "${sharelist:?}" "shares.(id==${shareid:?}).sharename")
        mountpath=$(yq read "${sharelist:?}" "shares.(id==${shareid:?}).mountpath")

        ssh 'zeppelin' \
            "
            date
            hostname
            echo '----'
            df -h  '${mountpath:?}'
            echo '----'
            ls -al '${mountpath:?}' | tail
            "
    done

--START--
---- ----
Share [GDR2]
----
Fri Apr 23 00:45:37 UTC 2021
gaia-prod-20210422-zeppelin.novalocal
----
Filesystem      Size  Used Avail Use% Mounted on
ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
----
-rw-r--r--. 1 fedora fedora     30825240 Oct 24 17:59 part-06504-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
-rw-r--r--. 1 fedora fedora     31802127 Oct 24 17:59 part-06505-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
-rw-r--r--. 1 fedora fedora     31538538 Oct 24 17:59 part-06506-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
-rw-r--r--. 1 fedora fedora     31218434 Oct 24 17:59 part-06507-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
-rw-r--r--. 1 fedora fedora     30815074 Oct 24 17:59 part-06508-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
-rw-r--r--. 1 fedora fedora     30406730 Oct 24 17:59 part-06509-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
-rw-r--r--. 1 fedora fedora     29995058 Oct 24 17:59 part-06510-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
-rw-r--r--. 1 fedora fedora     29447614 Oct 24 17:59 part-06511-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
-rw-r--r--. 1 fedora fedora     28448646 Oct 24 17:59 part-06512-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet
-rw-r--r--. 1 fedora fedora      6317774 Oct 24 17:59 part-06513-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet

---- ----
Share [GEDR3]
----
Fri Apr 23 00:45:38 UTC 2021
gaia-prod-20210422-zeppelin.novalocal
----
Filesystem      Size  Used Avail Use% Mounted on
ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
----
-rw-r--r--. 1 root root     36858229 Jan 11 22:27 part-11922-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
-rw-r--r--. 1 root root     35391788 Jan 11 22:27 part-11923-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
-rw-r--r--. 1 root root     39969879 Jan 11 22:27 part-11924-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
-rw-r--r--. 1 root root     38923149 Jan 11 22:27 part-11925-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
-rw-r--r--. 1 root root     36280019 Jan 11 22:27 part-11926-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
-rw-r--r--. 1 root root     39559908 Jan 11 22:27 part-11927-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
-rw-r--r--. 1 root root     34715127 Jan 11 22:27 part-11928-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
-rw-r--r--. 1 root root     35453747 Jan 11 22:27 part-11929-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
-rw-r--r--. 1 root root     30599245 Jan 11 22:27 part-11930-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
-rw-r--r--. 1 root root     10852913 Jan 11 22:27 part-11931-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet

---- ----
Share [ALLWISE]
----
Fri Apr 23 00:45:41 UTC 2021
gaia-prod-20210422-zeppelin.novalocal
----
Filesystem      Size  Used Avail Use% Mounted on
ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
----
-rw-r--r--. 1 root root     21195981 Jan 11 21:26 part-09124-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
-rw-r--r--. 1 root root     20760761 Jan 11 21:26 part-09125-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
-rw-r--r--. 1 root root     37549253 Jan 11 21:26 part-09126-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
-rw-r--r--. 1 root root     32687920 Jan 11 21:26 part-09127-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
-rw-r--r--. 1 root root     30215740 Jan 11 21:26 part-09128-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
-rw-r--r--. 1 root root     26528776 Jan 11 21:26 part-09129-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
-rw-r--r--. 1 root root     36999673 Jan 11 21:26 part-09130-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
-rw-r--r--. 1 root root     30382801 Jan 11 21:26 part-09131-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
-rw-r--r--. 1 root root     31622359 Jan 11 21:26 part-09132-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet
-rw-r--r--. 1 root root      9956618 Jan 11 21:26 part-09133-6f95fee1-90c7-4207-911a-ebcc0ef05615-c000.snappy.parquet

---- ----
Share [PS1]
----
Fri Apr 23 00:45:43 UTC 2021
gaia-prod-20210422-zeppelin.novalocal
----
Filesystem      Size  Used Avail Use% Mounted on
ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
----
-rw-r--r--. 1 root root     27803868 Jan 11 19:43 part-07723-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
-rw-r--r--. 1 root root     22025506 Jan 11 19:43 part-07724-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
-rw-r--r--. 1 root root     25756891 Jan 11 19:43 part-07725-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
-rw-r--r--. 1 root root     31396660 Jan 11 19:43 part-07726-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
-rw-r--r--. 1 root root     26859792 Jan 11 19:44 part-07727-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
-rw-r--r--. 1 root root     24735889 Jan 11 19:44 part-07728-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
-rw-r--r--. 1 root root     25470955 Jan 11 19:44 part-07729-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
-rw-r--r--. 1 root root     25640631 Jan 11 19:44 part-07730-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
-rw-r--r--. 1 root root     22504695 Jan 11 19:44 part-07731-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet
-rw-r--r--. 1 root root     13200198 Jan 11 19:44 part-07732-22b55fbd-2678-4993-8e3a-3f384b1854bc-c000.snappy.parquet

---- ----
Share [2MASS]
----
Fri Apr 23 00:45:44 UTC 2021
gaia-prod-20210422-zeppelin.novalocal
----
Filesystem      Size  Used Avail Use% Mounted on
ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
----
-rw-r--r--. 1 root root    16875933 Jan 11 17:44 part-01176-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
-rw-r--r--. 1 root root    31847987 Jan 11 17:44 part-01177-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
-rw-r--r--. 1 root root    33978033 Jan 11 17:45 part-01178-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
-rw-r--r--. 1 root root    33170642 Jan 11 17:45 part-01179-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
-rw-r--r--. 1 root root    33115257 Jan 11 17:45 part-01180-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
-rw-r--r--. 1 root root    33854964 Jan 11 17:45 part-01181-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
-rw-r--r--. 1 root root    31874821 Jan 11 17:45 part-01182-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
-rw-r--r--. 1 root root    33091386 Jan 11 17:45 part-01183-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
-rw-r--r--. 1 root root    31078087 Jan 11 17:45 part-01184-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
-rw-r--r--. 1 root root    14460710 Jan 11 17:45 part-01185-ce75a128-1cde-4ce1-90fc-4a36208209b2-c000.snappy.parquet
--END--


# -----------------------------------------------------
# Login to our Zeppelin node and check the user shares.
#[root@ansibler]

    sharelist='/deployments/common/manila/usershares.yaml'

    for shareid in $(
        yq read "${sharelist:?}" 'shares.[*].id'
        )
    do
        echo ""
        echo "---- ----"
        echo "Share [${shareid:?}]"
        echo "----"

        sharename=$(yq read "${sharelist:?}" "shares.(id==${shareid:?}).sharename")
        mountpath=$(yq read "${sharelist:?}" "shares.(id==${shareid:?}).mountpath")

        ssh 'zeppelin' \
            "
            date
            hostname
            echo '----'
            df -h  '${mountpath:?}'
            echo '----'
            ls -al '${mountpath:?}' | tail
            "
    done

--START--
---- ----
Share [nch]
----
Fri Apr 23 00:46:11 UTC 2021
gaia-prod-20210422-zeppelin.novalocal
----
Filesystem      Size  Used Avail Use% Mounted on
ceph-fuse        10T  7.0T  3.1T  70% /user/nch
----
total 6
drwxrwxrwx. 4 nch  nch  7650919610626 Jan 24 18:43 .
drwxr-xr-x. 5 root root          4096 Apr 22 16:35 ..
drwxrwxr-x. 9 nch  nch  4119733449949 Mar 28 10:32 CSV
drwxrwxr-x. 7 nch  nch  3531186160276 Mar 31 09:02 PARQUET
-rw-rw-r--. 1 nch  nch            401 Oct 30 14:28 test.log

---- ----
Share [zrq]
----
Fri Apr 23 00:46:11 UTC 2021
gaia-prod-20210422-zeppelin.novalocal
----
Filesystem      Size  Used Avail Use% Mounted on
ceph-fuse       1.0T   30G  995G   3% /user/zrq
----
total 7
drwxrwxrwx. 6 zrq  zrq  31872343185 Mar 14 03:44 .
drwxr-xr-x. 5 root root        4096 Apr 22 16:35 ..
drwxrwxr-x. 9 zrq  zrq      2167622 Feb 22 16:25 notebooks
drwxrwxr-x. 3 zrq  zrq    493465585 Mar 14 03:54 owncloud
-rw-rw-r--. 1 zrq  zrq          158 Mar 14 03:44 owncloud.env
drwxr-xr-x. 3 zrq  zrq  31376671528 Dec  3 07:05 tmass
drwxrwxr-x. 2 zrq  zrq        38292 Mar 11 18:24 transfer

---- ----
Share [stv]
----
Fri Apr 23 00:46:11 UTC 2021
gaia-prod-20210422-zeppelin.novalocal
----
Filesystem      Size  Used Avail Use% Mounted on
ceph-fuse       1.0T     0  1.0T   0% /user/stv
----
total 5
drwxrwxrwx. 2 stv  stv     0 Dec  3 04:39 .
drwxr-xr-x. 5 root root 4096 Apr 22 16:35 ..
--END--


# -----------------------------------------------------
# Login to our Zeppelin node and check the Spark, HDFS and Hadoop directories.
#[root@ansibler]

    ssh zeppelin \
        '
        date
        hostname

        echo  ""
        echo  "----"
        echo  "/var/spark"
        ls -l "/var/spark"
        echo  "----"
        df -h "/var/spark/temp"

        echo  ""
        echo  "----"
        echo  "/var/hadoop"
        ls -l "/var/hadoop"
        echo  "----"
        df -h "/var/hadoop/data"
        df -h "/var/hadoop/logs"
        df -h "/var/hadoop/temp"
        '

--START--
....
....
--END--


# -----------------------------------------------------
# Login to a worker node and check the Spark, HDFS and Hadoop directories.
#[root@ansibler]

    ssh worker01 \
        '
        date
        hostname

        echo  ""
        echo  "----"
        echo  "/var/hdfs"
        ls -l "/var/hdfs"
        df -h "/var/hdfs/data"

        echo  ""
        echo  "----"
        echo  "/var/hadoop"
        ls -l "/var/hadoop"
        df -h "/var/hadoop/data"
        '

--START--
Fri Apr 23 00:49:22 UTC 2021
gaia-prod-20210422-worker01.novalocal

----
/var/hdfs
total 0
lrwxrwxrwx. 1 root root 30 Apr 22 16:03 data -> /mnt/cinder/data-two/hdfs/data
lrwxrwxrwx. 1 root root 30 Apr 22 16:05 logs -> /mnt/cinder/data-two/hdfs/logs
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdb        200G   17M  198G   1% /mnt/cinder/data-two

----
/var/hadoop
total 0
lrwxrwxrwx. 1 root root 32 Apr 22 15:57 data -> /mnt/cinder/data-two/hadoop/data
lrwxrwxrwx. 1 root root 32 Apr 22 15:58 logs -> /mnt/cinder/data-two/hadoop/logs
lrwxrwxrwx. 1 root root 32 Apr 22 16:00 temp -> /mnt/cinder/data-two/hadoop/temp
Filesystem      Size  Used Avail Use% Mounted on
/dev/vdb        200G   17M  198G   1% /mnt/cinder/data-two
--END--


# -----------------------------------------------------
# -----------------------------------------------------
# Login via Firefox
#[user@desktop]

    firefox --new-window "http://zeppelin.aglais.uk:8080/" &


# -----------------------------------------------------
# -----------------------------------------------------


    Import our Random Forest notebook from GitHub, clear the output and run all the cells ...

    Good astrometric solutions via ML Random Forest classifier
    https://raw.githubusercontent.com/wfau/aglais-notebooks/main/2FRPC4BFS/note.json

    # Notebook fails with obscure Python errors.
    # AttributeError: 'NoneType' object has no attribute 'count'

    # Suspect this is caused by the configuration failing rather than the notebook.
    # Need to compare with original config.



