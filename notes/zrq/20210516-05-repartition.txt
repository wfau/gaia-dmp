#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Run the GEDR3-8192 repartition process ..

    Result:

        Workers fail due to lack of disc space.


# -----------------------------------------------------
# Check the free space on the HDFS filesystem
#[user@zeppelin]

    hadoop fs -ls hdfs://master01:9000/

    >   drwxr-xr-x   - fedora supergroup          0 2021-05-15 13:58 hdfs://master01:9000/partitioned
    >   drwxr-xr-x   - fedora supergroup          0 2021-05-16 15:40 hdfs://master01:9000/spark-log
    >   drwxr-xr-x   - fedora supergroup          0 2021-04-28 01:19 hdfs://master01:9000/user

    hadoop fs -du -h -v hdfs://master01:9000/

    >   SIZE     DISK_SPACE_CONSUMED_WITH_ALL_REPLICAS  FULL_PATH_NAME
    >   561.7 G    1.6 T                                hdfs://master01:9000/partitioned
    >    10.4 G   31.6 G                                hdfs://master01:9000/spark-log
    >   266.2 G  798.5 G                                hdfs://master01:9000/user


    hadoop fs -df -h hdfs://master01:9000/

    >   Filesystem            Size   Used  Available  Use%
    >   hdfs://master01:9000   4 T  2.5 T      1.5 T   62%


# -----------------------------------------------------
# Check the target directory is empty
#[user@zeppelin]


    hadoop fs -ls hdfs://master01:9000/partitioned/GEDR3-8192

    >   -


# -----------------------------------------------------
# -----------------------------------------------------

    Restart the interpreter
    Clear the output
    Run the notebook ...

    First cell : Took 0 sec. Last updated by zrq at May 17 2021, 12:11:37 AM.
    Last  cell :







    Zeppelin log

    >   ....
    >    INFO [2021-05-16 23:12:19,341] ({Thread-38} Logging.scala[logInfo]:54) - Warehouse path is 'file:/home/fedora/zeppelin-0.8.2-bin-all/notebook/spark-warehouse/'.
    >    INFO [2021-05-16 23:12:19,349] ({Thread-38} Logging.scala[logInfo]:54) - Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.
    >    INFO [2021-05-16 23:12:19,350] ({Thread-38} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@7857f675{/SQL,null,AVAILABLE,@Spark}
    >    INFO [2021-05-16 23:12:19,350] ({Thread-38} Logging.scala[logInfo]:54) - Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.
    >    INFO [2021-05-16 23:12:19,351] ({Thread-38} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@43f074d2{/SQL/json,null,AVAILABLE,@Spark}
    >    INFO [2021-05-16 23:12:19,351] ({Thread-38} Logging.scala[logInfo]:54) - Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.
    >    INFO [2021-05-16 23:12:19,351] ({Thread-38} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@3118de38{/SQL/execution,null,AVAILABLE,@Spark}
    >    INFO [2021-05-16 23:12:19,352] ({Thread-38} Logging.scala[logInfo]:54) - Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.
    >    INFO [2021-05-16 23:12:19,352] ({Thread-38} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@42a3c467{/SQL/execution/json,null,AVAILABLE,@Spark}
    >    INFO [2021-05-16 23:12:19,353] ({Thread-38} Logging.scala[logInfo]:54) - Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.
    >    INFO [2021-05-16 23:12:19,354] ({Thread-38} ContextHandler.java[doStart]:781) - Started o.s.j.s.ServletContextHandler@6feffc69{/static/sql,null,AVAILABLE,@Spark}
    >    INFO [2021-05-16 23:12:19,850] ({Thread-38} Logging.scala[logInfo]:54) - Registered StateStoreCoordinator endpoint
    >   ....

    >   ....
    >    INFO [2021-05-16 23:13:09,819] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 6471.0 in stage 0.0 (TID 6471) in 8 ms on worker03 (executor 1) (6469/10000)
    >    INFO [2021-05-16 23:13:09,819] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 6481.0 in stage 0.0 (TID 6481, worker02, executor 3, partition 6481, PROCESS_LOCAL, 7984 bytes)
    >    INFO [2021-05-16 23:13:09,820] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 6467.0 in stage 0.0 (TID 6467) in 11 ms on worker02 (executor 3) (6470/10000)
    >    INFO [2021-05-16 23:13:09,820] ({dispatcher-event-loop-9} Logging.scala[logInfo]:54) - Starting task 6482.0 in stage 0.0 (TID 6482, worker02, executor 3, partition 6482, PROCESS_LOCAL, 7984 bytes)
    >    INFO [2021-05-16 23:13:09,821] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 6470.0 in stage 0.0 (TID 6470) in 11 ms on worker02 (executor 3) (6471/10000)
    >    INFO [2021-05-16 23:13:09,821] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 6483.0 in stage 0.0 (TID 6483, worker02, executor 3, partition 6483, PROCESS_LOCAL, 7984 bytes)
    >    INFO [2021-05-16 23:13:09,821] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 6472.0 in stage 0.0 (TID 6472) in 10 ms on worker02 (executor 3) (6472/10000)
    >    INFO [2021-05-16 23:13:09,822] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Starting task 6484.0 in stage 0.0 (TID 6484, worker04, executor 2, partition 6484, PROCESS_LOCAL, 7984 bytes)
    >    INFO [2021-05-16 23:13:09,822] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 6473.0 in stage 0.0 (TID 6473) in 10 ms on worker04 (executor 2) (6473/10000)
    >    INFO [2021-05-16 23:13:09,822] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 6485.0 in stage 0.0 (TID 6485, worker02, executor 3, partition 6485, PROCESS_LOCAL, 8089 bytes)
    >    INFO [2021-05-16 23:13:09,823] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 6469.0 in stage 0.0 (TID 6469) in 13 ms on worker02 (executor 3) (6474/10000)
    >    INFO [2021-05-16 23:13:09,823] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 6486.0 in stage 0.0 (TID 6486, worker03, executor 1, partition 6486, PROCESS_LOCAL, 7984 bytes)
    >    INFO [2021-05-16 23:13:09,823] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 6476.0 in stage 0.0 (TID 6476) in 8 ms on worker03 (executor 1) (6475/10000)
    >    INFO [2021-05-16 23:13:09,823] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 6487.0 in stage 0.0 (TID 6487, worker04, executor 2, partition 6487, PROCESS_LOCAL, 7984 bytes)
    >    INFO [2021-05-16 23:13:09,824] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 6488.0 in stage 0.0 (TID 6488, worker03, executor 1, partition 6488, PROCESS_LOCAL, 7984 bytes)
    >    INFO [2021-05-16 23:13:09,824] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 6477.0 in stage 0.0 (TID 6477) in 8 ms on worker03 (executor 1) (6476/10000)
    >    INFO [2021-05-16 23:13:09,824] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 6474.0 in stage 0.0 (TID 6474) in 11 ms on worker04 (executor 2) (6477/10000)
    >    INFO [2021-05-16 23:13:09,826] ({dispatcher-event-loop-9} Logging.scala[logInfo]:54) - Starting task 6489.0 in stage 0.0 (TID 6489, worker03, executor 1, partition 6489, PROCESS_LOCAL, 7984 bytes)
    >    INFO [2021-05-16 23:13:09,826] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 6479.0 in stage 0.0 (TID 6479) in 8 ms on worker03 (executor 1) (6478/10000)
    >    INFO [2021-05-16 23:13:09,827] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 6490.0 in stage 0.0 (TID 6490, worker04, executor 2, partition 6490, PROCESS_LOCAL, 8089 bytes)
    >    INFO [2021-05-16 23:13:09,827] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 6475.0 in stage 0.0 (TID 6475) in 13 ms on worker04 (executor 2) (6479/10000)
    >    INFO [2021-05-16 23:13:09,828] ({dispatcher-event-loop-11} Logging.scala[logInfo]:54) - Starting task 6491.0 in stage 0.0 (TID 6491, worker04, executor 2, partition 6491, PROCESS_LOCAL, 7984 bytes)
    >    INFO [2021-05-16 23:13:09,828] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 6478.0 in stage 0.0 (TID 6478) in 10 ms on worker04 (executor 2) (6480/10000)
    >    INFO [2021-05-16 23:13:09,829] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Starting task 6492.0 in stage 0.0 (TID 6492, worker02, executor 3, partition 6492, PROCESS_LOCAL, 7984 bytes)
    >    INFO [2021-05-16 23:13:09,829] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 6481.0 in stage 0.0 (TID 6481) in 10 ms on worker02 (executor 3) (6481/10000)
    >    INFO [2021-05-16 23:13:09,830] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 6493.0 in stage 0.0 (TID 6493, worker02, executor 3, partition 6493, PROCESS_LOCAL, 7984 bytes)
    >   ....



    Worker application log

    >   ....
    >   2021-05-16 23:13:27,608 INFO executor.Executor: Running task 35.0 in stage 2.0 (TID 10036)
    >   2021-05-16 23:13:27,612 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-04995-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-51033092, partition values: [empty row]
    >   2021-05-16 23:13:28,142 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-08514-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-51045122, partition values: [empty row]
    >   2021-05-16 23:13:28,834 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-08637-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-51047569, partition values: [empty row]
    >   2021-05-16 23:13:28,889 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-04131-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-51036130, partition values: [empty row]
    >   2021-05-16 23:13:29,224 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-08721-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-51031821, partition values: [empty row]
    >   2021-05-16 23:13:31,399 INFO executor.Executor: Finished task 30.0 in stage 2.0 (TID 10031). 1619 bytes result sent to driver
    >   2021-05-16 23:13:31,401 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10040
    >   2021-05-16 23:13:31,401 INFO executor.Executor: Running task 39.0 in stage 2.0 (TID 10040)
    >   2021-05-16 23:13:31,405 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-09094-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-51021729, partition values: [empty row]
    >   2021-05-16 23:13:31,525 INFO executor.Executor: Finished task 31.0 in stage 2.0 (TID 10032). 1619 bytes result sent to driver
    >   2021-05-16 23:13:31,527 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10041
    >   2021-05-16 23:13:31,527 INFO executor.Executor: Running task 40.0 in stage 2.0 (TID 10041)
    >   2021-05-16 23:13:31,531 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-06088-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-51015572, partition values: [empty row]
    >   2021-05-16 23:13:31,643 INFO executor.Executor: Finished task 33.0 in stage 2.0 (TID 10034). 1619 bytes result sent to driver
    >   2021-05-16 23:13:31,645 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10043
    >   2021-05-16 23:13:31,645 INFO executor.Executor: Running task 42.0 in stage 2.0 (TID 10043)
    >   2021-05-16 23:13:31,649 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-09660-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-51010742, partition values: [empty row]
    >   2021-05-16 23:13:32,810 INFO executor.Executor: Finished task 35.0 in stage 2.0 (TID 10036). 1619 bytes result sent to driver
    >   2021-05-16 23:13:32,812 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10047
    >   2021-05-16 23:13:32,812 INFO executor.Executor: Running task 46.0 in stage 2.0 (TID 10047)
    >   2021-05-16 23:13:32,817 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-05050-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-51002984, partition values: [empty row]
    >   2021-05-16 23:13:33,036 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-04132-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-51014121, partition values: [empty row]
    >   2021-05-16 23:13:33,225 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-06091-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-51018183, partition values: [empty row]
    >   2021-05-16 23:13:34,035 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-06610-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-50999868, partition values: [empty row]
    >   2021-05-16 23:13:34,793 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-02315-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-51010214, partition values: [empty row]
    >   2021-05-16 23:13:35,645 INFO executor.Executor: Finished task 39.0 in stage 2.0 (TID 10040). 1619 bytes result sent to driver
    >   2021-05-16 23:13:35,647 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10050
    >   2021-05-16 23:13:35,647 INFO executor.Executor: Running task 49.0 in stage 2.0 (TID 10050)
    >   2021-05-16 23:13:35,651 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-00418-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-50994489, partition values: [empty row]
    >   2021-05-16 23:13:36,410 INFO executor.Executor: Finished task 40.0 in stage 2.0 (TID 10041). 1619 bytes result sent to driver
    >   2021-05-16 23:13:36,412 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10054
    >   2021-05-16 23:13:36,412 INFO executor.Executor: Running task 53.0 in stage 2.0 (TID 10054)
    >   2021-05-16 23:13:36,416 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-05697-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-50986316, partition values: [empty row]
    >   2021-05-16 23:13:36,506 INFO executor.Executor: Finished task 46.0 in stage 2.0 (TID 10047). 1619 bytes result sent to driver
    >   2021-05-16 23:13:36,507 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10055
    >   ....


    CephFS checksum - worker

    >   ....
    >   2021-05-16T23:11:30+00:00
    >   30207357408a5842feff1599a37e7da8  part-01262-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 0.81
    >   user 0.08
    >   sys 0.00
    >   ----
    >   2021-05-16T23:11:51+00:00
    >   2eb13962ec4145129f7bd09d5e2bb967  part-01263-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 0.91
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-16T23:12:12+00:00
    >   aa3fd4233a486fabb41b747e1a27dfc5  part-01264-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 0.78
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-16T23:12:32+00:00
    >   e906caeac5232382030e31e2e2660512  part-01265-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 1.46
    >   user 0.06
    >   sys 0.01
    >   ----
    >   2021-05-16T23:12:54+00:00
    >   8d12f94eb37f3ac6371ed04acebb7b74  part-01266-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 0.80
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-16T23:13:15+00:00
    >   c5d5bf795d4729e07bbf946aeadaa108  part-01267-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 1.66
    >   user 0.06
    >   sys 0.02
    >   ----
    >   2021-05-16T23:13:36+00:00
    >   330c933698f75d6e421743fa9ca4c272  part-01268-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 1.78
    >   user 0.08
    >   sys 0.01
    >   ----
    >   2021-05-16T23:13:58+00:00
    >   4b5f9f15692d270dbffaf70138f58f17  part-01269-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 3.41
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-16T23:14:22+00:00
    >   b0578675ae1c3ed46843e4a50e534458  part-01270-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 6.94
    >   user 0.08
    >   sys 0.01
    >   ....


    >   ....
    >   2021-05-16T23:21:09+00:00
    >   c2ad705cad88d9aece20d4cb2401e75a  part-01287-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 5.17
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-16T23:21:35+00:00
    >   e9e253793981b3a14963b2cc46f7bdb4  part-01288-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 2.52
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-16T23:21:57+00:00
    >   35a0171364ac9dd4d7506d93f989f31f  part-01289-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 2.84
    >   user 0.06
    >   sys 0.02
    >   ----
    >   2021-05-16T23:22:20+00:00
    >   3eb01160fadef636e23afb0283b54a97  part-01290-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 3.30
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-16T23:22:43+00:00
    >   0280e2628d10ebd78522b58ce7bc2ec0  part-01291-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 2.38
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-16T23:23:06+00:00
    >   3cd0e75c524330ce58c1dc7897b2ad60  part-01292-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 2.46
    >   user 0.06
    >   sys 0.02
    >   ----
    >   2021-05-16T23:23:28+00:00
    >   45be723a4b829478bd58030464cdee53  part-01293-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 2.11
    >   user 0.08
    >   sys 0.01
    >   ----
    >   2021-05-16T23:23:50+00:00
    >   a8a6efa3f53c397f85d0d12bac62bf08  part-01294-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 2.95
    >   user 0.06
    >   sys 0.01
    >   ----
    >   2021-05-16T23:24:13+00:00
    >   6022976ccde70e59cfd98c4f37252b66  part-01295-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 1.70
    >   user 0.06
    >   sys 0.02
    >   ----
    >   2021-05-16T23:24:35+00:00
    >   f749d5ef200fdbb346ae74e450eac425  part-01296-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 4.13
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-16T23:24:59+00:00
    >   3e574b15221591c2b9743b2bc9084759  part-01297-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 3.25
    >   user 0.07
    >   sys 0.02
    >   ----
    >   2021-05-16T23:25:22+00:00
    >   528a956d2ef91124ab63f64232e8fa0c  part-01298-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 3.56
    >   user 0.08
    >   sys 0.01
    >   ....


    HDFS checksum

    >   ....
    >   2021-05-16T23:13:43+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00010-dc9b579c-2247-48f9-8f92-6ffbc0384845_00010.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000d721f6f05c99c1d087dda326dcd656b8
    >   real 1.88
    >   user 2.57
    >   sys 0.24
    >   ----
    >   2021-05-16T23:14:04+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00011-dc9b579c-2247-48f9-8f92-6ffbc0384845_00011.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000aafa1814f533778621a608ff7413a3cd
    >   real 1.77
    >   user 2.34
    >   sys 0.23
    >   ----
    >   2021-05-16T23:14:26+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00012-dc9b579c-2247-48f9-8f92-6ffbc0384845_00012.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000d3c3a35a9d4c3a4bed8561bf3eb97271
    >   real 1.77
    >   user 2.28
    >   sys 0.19
    >   ----
    >   2021-05-16T23:14:48+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00013-dc9b579c-2247-48f9-8f92-6ffbc0384845_00013.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	000002000000000000040000641b0e15cea607b71f49d7f5efe6b968
    >   real 1.65
    >   user 2.33
    >   sys 0.21
    >   ....


    >   ....
    >   2021-05-16T23:14:04+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00011-dc9b579c-2247-48f9-8f92-6ffbc0384845_00011.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000aafa1814f533778621a608ff7413a3cd
    >   real 1.77
    >   user 2.34
    >   sys 0.23
    >   ----
    >   2021-05-16T23:14:26+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00012-dc9b579c-2247-48f9-8f92-6ffbc0384845_00012.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000d3c3a35a9d4c3a4bed8561bf3eb97271
    >   real 1.77
    >   user 2.28
    >   sys 0.19
    >   ----
    >   2021-05-16T23:14:48+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00013-dc9b579c-2247-48f9-8f92-6ffbc0384845_00013.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	000002000000000000040000641b0e15cea607b71f49d7f5efe6b968
    >   real 1.65
    >   user 2.33
    >   sys 0.21
    >   ----
    >   2021-05-16T23:15:10+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00014-dc9b579c-2247-48f9-8f92-6ffbc0384845_00014.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	0000020000000000000400001c567aaf867b108dbf46ebac18a598e6
    >   real 2.44
    >   user 2.26
    >   sys 0.20
    >   ----
    >   2021-05-16T23:15:32+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00015-dc9b579c-2247-48f9-8f92-6ffbc0384845_00015.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	00000200000000000004000017665ab2ec3f50ad300ff16024080d8c
    >   real 2.26
    >   user 2.36
    >   sys 0.24
    >   ----
    >   2021-05-16T23:15:54+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00016-dc9b579c-2247-48f9-8f92-6ffbc0384845_00016.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	000002000000000000040000b65bc5a8d6a4e2bf95772cefcc879dca
    >   real 2.68
    >   user 2.23
    >   sys 0.19
    >   ----
    >   2021-05-16T23:16:17+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00017-dc9b579c-2247-48f9-8f92-6ffbc0384845_00017.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	0000020000000000000400000cfe63d283726f65ea431906e5900b33
    >   real 2.05
    >   user 2.22
    >   sys 0.23
    >   ----
    >   2021-05-16T23:16:39+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00018-dc9b579c-2247-48f9-8f92-6ffbc0384845_00018.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	00000200000000000004000036654d8d669d086844c74cdbf382753a
    >   real 2.26
    >   user 2.30
    >   sys 0.24
    >   ----
    >   2021-05-16T23:17:01+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00019-dc9b579c-2247-48f9-8f92-6ffbc0384845_00019.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	000002000000000000040000fd7ff5dca21093b8004c83b99beecd2e
    >   real 1.82
    >   user 2.27
    >   sys 0.21
    >   ....

    >   ....
    >   2021-05-16T23:21:46+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00032-dc9b579c-2247-48f9-8f92-6ffbc0384845_00032.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000acf02692653740a407f128dc3819f14c
    >   real 1.90
    >   user 2.12
    >   sys 0.20
    >   ----
    >   2021-05-16T23:22:08+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00033-dc9b579c-2247-48f9-8f92-6ffbc0384845_00033.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000a551100469eeb19274d192b6cd6be517
    >   real 1.88
    >   user 2.36
    >   sys 0.22
    >   ----
    >   2021-05-16T23:22:30+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00034-dc9b579c-2247-48f9-8f92-6ffbc0384845_00034.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000218407e9e225b0d5ba45db349df92b94
    >   real 1.68
    >   user 2.11
    >   sys 0.24
    >   ----
    >   2021-05-16T23:22:52+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00035-dc9b579c-2247-48f9-8f92-6ffbc0384845_00035.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C00000200000000000004000023b5bf24204607ff5af0db1e16923ae8
    >   real 1.70
    >   user 2.36
    >   sys 0.19
    >   ----
    >   2021-05-16T23:23:14+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00036-dc9b579c-2247-48f9-8f92-6ffbc0384845_00036.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C0000020000000000000400000d92a2e5168b97058b0c37403ca3d983
    >   real 1.89
    >   user 2.27
    >   sys 0.17
    >   ----
    >   2021-05-16T23:23:36+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00037-dc9b579c-2247-48f9-8f92-6ffbc0384845_00037.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C0000020000000000000400007c01a368284c8a4551c51562555dffff
    >   real 1.56
    >   user 2.08
    >   sys 0.17
    >   ----
    >   2021-05-16T23:23:57+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00038-dc9b579c-2247-48f9-8f92-6ffbc0384845_00038.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000c2cb960538240577c93c7c5634adc06f
    >   real 1.70
    >   user 2.24
    >   sys 0.19
    >   ----
    >   2021-05-16T23:24:19+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00039-dc9b579c-2247-48f9-8f92-6ffbc0384845_00039.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000da8a588d257f46482407896ccc027cb6
    >   real 1.66
    >   user 2.10
    >   sys 0.17
    >   ....













3.0 in stage 2.0 (TID 12284, worker04, executor 2, partition 2283, PROCESS_LOCAL, 8450 bytes)
 INFO [2021-05-16 23:39:59,253] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 2276.0 in stage 2.0 (TID 12277) in 5713 ms on worker04 (executor 2) (2272/5720)
 INFO [2021-05-16 23:40:00,892] ({dispatcher-event-loop-9} Logging.scala[logInfo]:54) - Starting task 2284.0 in stage 2.0 (TID 12285, worker04, executor 2, partition 2284, PROCESS_LOCAL, 8450 bytes)
 INFO [2021-05-16 23:40:00,892] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 2278.0 in stage 2.0 (TID 12279) in 5490 ms on worker04 (executor 2) (2273/5720)
 INFO [2021-05-16 23:40:01,412] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Starting task 2285.0 in stage 2.0 (TID 12286, worker03, executor 1, partition 2285, PROCESS_LOCAL, 8450 bytes)
 INFO [2021-05-16 23:40:01,413] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 2267.0 in stage 2.0 (TID 12268) in 15636 ms on worker03 (executor 1) (2274/5720)
 INFO [2021-05-16 23:40:01,445] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Starting task 2286.0 in stage 2.0 (TID 12287, worker03, executor 1, partition 2286, PROCESS_LOCAL, 8450 bytes)
 INFO [2021-05-16 23:40:01,445] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 2266.0 in stage 2.0 (TID 12267) in 15739 ms on worker03 (executor 1) (2275



2021-05-16 23:40:17,258 INFO executor.Executor: Finished task 2302.0 in stage 2.0 (TID 12303). 1619 bytes result sent to driver
2021-05-16 23:40:17,260 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 12306
2021-05-16 23:40:17,260 INFO executor.Executor: Running task 2305.0 in stage 2.0 (TID 12306)
2021-05-16 23:40:17,262 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-02245-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-49877098, partition values: [empty row]
2021-05-16 23:40:17,761 INFO executor.Executor: Finished task 2301.0 in stage 2.0 (TID 12302). 1619 bytes result sent to driver
2021-05-16 23:40:17,763 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 12307
2021-05-16 23:40:17,763 INFO executor.Executor: Running task 2306.0 in stage 2.0 (TID 12307)
2021-05-16 23:40:17,765 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-07218-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-49877046, partition values: [empty row]

master
----
2021-05-16T23:39:25+00:00
c2735db4458af32d4b79cec0d26ac8f8  part-00028-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
real 4.88
user 0.07
sys 0.01
----
2021-05-16T23:39:50+00:00
9717ff9066d8fb35903581db0574b6e8  part-00029-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
real 2.53
user 0.07
sys 0.01


worker
----
2021-05-16T23:39:18+00:00
3f066f0e86b01ce49cdf1ba8f1d00242  part-01335-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
real 2.56
user 0.07
sys 0.02
----
2021-05-16T23:39:41+00:00
44e694df33fa03b8edf4252f8074e896  part-01336-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
real 4.98
user 0.06
sys 0.01
----
2021-05-16T23:40:06+00:00
7531095762967e6793db7273b4c35faf  part-01337-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
real 1.68
user 0.06
sys 0.02
----
2021-05-16T23:40:28+00:00
5d83ca7311b34e4b7415fa88639716e6  part-01338-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
real 2.00
user 0.07
sys 0.01

HDFS
----

2021-05-16T23:40:21+00:00
hdfs://master01:9000/partitioned/GEDR3-4096/part-00083-dc9b579c-2247-48f9-8f92-6ffbc0384845_00083.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C0000020000000000000400005c8bd52943e3ee5e03ce6fb4a15fe1b8
real 1.56
user 2.07
sys 0.20
----
2021-05-16T23:40:43+00:00
hdfs://master01:9000/partitioned/GEDR3-4096/part-00084-dc9b579c-2247-48f9-8f92-6ffbc0384845_00084.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000fb1fe6ec7ed76cbb428019d9d67b454f
real 2.13
user 2.19
sys 0.22
----
2021-05-16T23:41:05+00:00
hdfs://master01:9000/partitioned/GEDR3-4096/part-00085-dc9b579c-2247-48f9-8f92-6ffbc0384845_00085.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000f356e672bdbf078f8305105748cc3172
real 2.10
user 2.12
sys 0.28
----
2021-05-16T23:41:27+00:00
hdfs://master01:9000/partitioned/GEDR3-4096/part-00086-dc9b579c-2247-48f9-8f92-6ffbc0384845_00086.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C0000020000000000000400009a16e0a38acfd7f342dd9d4af8c401a5
real 2.04
user 2.27
sys 0.20














Zeppelin
 INFO [2021-05-17 00:27:22,908] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 5.0 in stage 3.0 (TID 15726, worker02, executor 3, partition 5, PROCESS_LOCAL, 7778 bytes)
 INFO [2021-05-17 00:27:22,908] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 6.0 in stage 3.0 (TID 15727, worker03, executor 1, partition 6, PROCESS_LOCAL, 7778 bytes)
 INFO [2021-05-17 00:27:22,908] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 7.0 in stage 3.0 (TID 15728, worker04, executor 2, partition 7, PROCESS_LOCAL, 7778 bytes)
 INFO [2021-05-17 00:27:22,909] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 8.0 in stage 3.0 (TID 15729, worker02, executor 3, partition 8, PROCESS_LOCAL, 7778 bytes)
 INFO [2021-05-17 00:27:22,909] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 9.0 in stage 3.0 (TID 15730, worker03, executor 1, partition 9, PROCESS_LOCAL, 7778 bytes)
 INFO [2021-05-17 00:27:22,909] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 10.0 in stage 3.0 (TID 15731, worker04, executor 2, partition 10, PROCESS_LOCAL, 7778 bytes)
 INFO [2021-05-17 00:27:22,909] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 11.0 in stage 3.0 (TID 15732, worker02, executor 3, partition 11, PROCESS_LOCAL, 7778 bytes)
 INFO [2021-05-17 00:27:22,921] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Added broadcast_4_piece0 in memory on worker04:36857 (size: 177.2 KB, free: 6.8 GB)
 INFO [2021-05-17 00:27:22,923] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Added broadcast_4_piece0 in memory on worker03:45001 (size: 177.2 KB, free: 6.8 GB)
 INFO [2021-05-17 00:27:22,929] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Added broadcast_4_piece0 in memory on worker02:46645 (size: 177.2 KB, free: 6.8 GB)
 INFO [2021-05-17 00:27:23,004] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 0 to 10.10.3.80:54432
 INFO [2021-05-17 00:27:23,004] ({dispatcher-event-loop-11} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 0 to 10.10.3.14:54370
 INFO [2021-05-17 00:27:23,009] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Asked to send map output locations for shuffle 0 to 10.10.2.218:34002

worker
2021-05-17 00:26:54,938 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-10651-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-23659122, partition values: [empty row]
2021-05-17 00:26:54,984 INFO executor.Executor: Finished task 5715.0 in stage 2.0 (TID 15716). 1619 bytes result sent to driver
2021-05-17 00:26:55,664 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-10641-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-23849859, partition values: [empty row]
2021-05-17 00:26:55,715 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-10656-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-23722841, partition values: [empty row]
2021-05-17 00:26:56,189 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-11931-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-10852913, partition values: [empty row]
2021-05-17 00:26:56,810 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-10648-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-23686531, partition values: [empty row]
2021-05-17 00:26:57,130 INFO executor.Executor: Finished task 5719.0 in stage 2.0 (TID 15720). 1619 bytes result sent to driver
2021-05-17 00:26:57,487 INFO executor.Executor: Finished task 5717.0 in stage 2.0 (TID 15718). 1619 bytes result sent to driver
2021-05-17 00:26:58,896 INFO executor.Executor: Finished task 5718.0 in stage 2.0 (TID 15719). 1619 bytes result sent to driver
2021-05-17 00:27:22,909 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15721
2021-05-17 00:27:22,909 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15724
2021-05-17 00:27:22,909 INFO executor.Executor: Running task 3.0 in stage 3.0 (TID 15724)
2021-05-17 00:27:22,909 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15727
2021-05-17 00:27:22,909 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 15721)
2021-05-17 00:27:22,909 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15730
2021-05-17 00:27:22,910 INFO executor.Executor: Running task 9.0 in stage 3.0 (TID 15730)
2021-05-17 00:27:22,911 INFO executor.Executor: Running task 6.0 in stage 3.0 (TID 15727)
2021-05-17 00:27:22,914 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
2021-05-17 00:27:22,914 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
2021-05-17 00:27:22,921 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 177.2 KB, free 6.8 GB)
2021-05-17 00:27:22,922 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 8 ms
2021-05-17 00:27:22,925 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 1423.9 KB, free 6.8 GB)
2021-05-17 00:27:23,000 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
2021-05-17 00:27:23,000 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
2021-05-17 00:27:23,001 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@zeppelin:34305)
2021-05-17 00:27:23,000 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
2021-05-17 00:27:23,000 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
2021-05-17 00:27:23,053 INFO spark.MapOutputTrackerWorker: Got the output locations
2021-05-17 00:27:23,156 INFO storage.ShuffleBlockFetcherIterator: Getting 5720 non-empty blocks including 1952 local blocks and 3768 remote blocks
2021-05-17 00:27:23,158 INFO storage.ShuffleBlockFetcherIterator: Getting 5720 non-empty blocks including 1952 local blocks and 3768 remote blocks
2021-05-17 00:27:23,161 INFO storage.ShuffleBlockFetcherIterator: Getting 5720 non-empty blocks including 1952 local blocks and 3768 remote blocks
2021-05-17 00:27:23,162 INFO storage.ShuffleBlockFetcherIterator: Getting 5720 non-empty blocks including 1952 local blocks and 3768 remote blocks
2021-05-17 00:27:23,171 INFO client.TransportClientFactory: Successfully created connection to worker04/10.10.3.80:36857 after 1 ms (0 ms spent in bootstraps)
2021-05-17 00:27:23,198 INFO storage.ShuffleBlockFetcherIterator: Started 6 remote fetches in 96 ms
2021-05-17 00:27:23,198 INFO storage.ShuffleBlockFetcherIterator: Started 4 remote fetches in 97 ms
2021-05-17 00:27:23,200 INFO storage.ShuffleBlockFetcherIterator: Started 6 remote fetches in 101 ms
2021-05-17 00:27:23,200 INFO storage.ShuffleBlockFetcherIterator: Started 6 remote fetches in 99 ms
2021-05-17 00:28:09,422 INFO client.TransportClientFactory: Found inactive connection to zeppelin/10.10.2.227:34305, creating a new one.
2021-05-17 00:28:09,424 INFO client.TransportClientFactory: Successfully created connection to zeppelin/10.10.2.227:34305 after 1 ms (0 ms spent in bootstraps)
2021-05-17 00:28:09,446 INFO codegen.CodeGenerator: Code generated in 28.49208 ms
2021-05-17 00:28:09,480 INFO codegen.CodeGenerator: Code generated in 16.955928 ms

master
sys 0.01
----
2021-05-17T00:28:34+00:00
a82b2818cde07363dece8673c0f54004  part-00157-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
real 2.18
user 0.07
sys 0.01
----
2021-05-17T00:28:56+00:00
5ee421f705d7b716e5a4d0871b0e4985  part-00158-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
real 2.98
user 0.07
sys 0.01
----
2021-05-17T00:29:19+00:00
3b71377bbfed231b8d78d7b75a2a6893  part-00159-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
real 2.53
user 0.08
sys 0.01


worker
2021-05-17T00:28:35+00:00
bcb7111faa645f873d337e69bddfd760  part-01465-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
real 2.43
user 0.07
sys 0.01
----
2021-05-17T00:28:58+00:00
5863cc6ef8ae0d28a064073decd2c24e  part-01466-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
real 2.70
user 0.07
sys 0.01
----
2021-05-17T00:29:21+00:00
477da987955733da97614c7ba21dc1f5  part-01467-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
real 3.40
user 0.06
sys 0.02
----
2021-05-17T00:29:44+00:00
43b4057340a2fc3865729c993667edbd  part-01468-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
real 2.70
user 0.06
sys 0.01

HDFS
----
2021-05-17T00:29:00+00:00
hdfs://master01:9000/partitioned/GEDR3-4096/part-00216-dc9b579c-2247-48f9-8f92-6ffbc0384845_00216.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000f07ef0b592067764bf1abd25975c3c49
real 1.70
user 2.01
sys 0.19
----
2021-05-17T00:29:21+00:00
hdfs://master01:9000/partitioned/GEDR3-4096/part-00217-dc9b579c-2247-48f9-8f92-6ffbc0384845_00217.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C00000200000000000004000047189141a0fbb60b713efc096949a1b4
real 2.36
user 2.45
sys 0.21
----
2021-05-17T00:29:44+00:00
hdfs://master01:9000/partitioned/GEDR3-4096/part-00218-dc9b579c-2247-48f9-8f92-6ffbc0384845_00218.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C0000020000000000000400000c97d084ec413e0b5e37afe88254c1bf
real 1.80
user 2.24
sys 0.22
----
2021-05-17T00:30:06+00:00
hdfs://master01:9000/partitioned/GEDR3-4096/part-00219-dc9b579c-2247-48f9-8f92-6ffbc0384845_00219.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	000002000000000000040000b2885773d37d4bbd9234f225633f5111
real 1.87
user 2.24
sys 0.21







    worker

    >   ....
    >   2021-05-17 00:30:25,771 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 127124762
    >   2021-05-17 00:30:26,977 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210516231315_0003_m_000038_15759' to hdfs://master01:9000/partitioned/GEDR3-8192/_temporary/0/task_20210516231315_0003_m_000038
    >   2021-05-17 00:30:26,977 INFO mapred.SparkHadoopMapRedUtil: attempt_20210516231315_0003_m_000038_15759: Committed
    >   2021-05-17 00:30:26,977 INFO executor.Executor: Finished task 38.0 in stage 3.0 (TID 15759). 3073 bytes result sent to driver
    >   2021-05-17 00:30:26,979 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15771
    >   2021-05-17 00:30:26,979 INFO executor.Executor: Running task 50.0 in stage 3.0 (TID 15771)
    >   2021-05-17 00:30:26,999 INFO storage.ShuffleBlockFetcherIterator: Getting 5720 non-empty blocks including 1952 local blocks and 3768 remote blocks
    >   2021-05-17 00:30:27,005 INFO storage.ShuffleBlockFetcherIterator: Started 5 remote fetches in 12 ms
    >   2021-05-17 00:30:27,450 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 126977144
    >   2021-05-17 00:30:28,671 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210516231315_0003_m_000040_15761' to hdfs://master01:9000/partitioned/GEDR3-8192/_temporary/0/task_20210516231315_0003_m_000040
    >   2021-05-17 00:30:28,671 INFO mapred.SparkHadoopMapRedUtil: attempt_20210516231315_0003_m_000040_15761: Committed
    >   2021-05-17 00:30:28,672 INFO executor.Executor: Finished task 40.0 in stage 3.0 (TID 15761). 3116 bytes result sent to driver
    >   2021-05-17 00:30:28,673 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15773
    >   2021-05-17 00:30:28,673 INFO executor.Executor: Running task 52.0 in stage 3.0 (TID 15773)
    >   2021-05-17 00:30:28,691 INFO storage.ShuffleBlockFetcherIterator: Getting 5720 non-empty blocks including 1952 local blocks and 3768 remote blocks
    >   2021-05-17 00:30:28,695 INFO storage.ShuffleBlockFetcherIterator: Started 6 remote fetches in 9 ms
    >   2021-05-17 00:30:32,102 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 127065754
    >   2021-05-17 00:30:33,248 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210516231315_0003_m_000043_15764' to hdfs://master01:9000/partitioned/GEDR3-8192/_temporary/0/task_20210516231315_0003_m_000043
    >   2021-05-17 00:30:33,248 INFO mapred.SparkHadoopMapRedUtil: attempt_20210516231315_0003_m_000043_15764: Committed
    >   2021-05-17 00:30:33,248 INFO executor.Executor: Finished task 43.0 in stage 3.0 (TID 15764). 3073 bytes result sent to driver
    >   2021-05-17 00:30:33,250 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15775
    >   2021-05-17 00:30:33,250 INFO executor.Executor: Running task 54.0 in stage 3.0 (TID 15775)
    >   2021-05-17 00:30:33,268 INFO storage.ShuffleBlockFetcherIterator: Getting 5720 non-empty blocks including 1952 local blocks and 3768 remote blocks
    >   2021-05-17 00:30:33,272 INFO storage.ShuffleBlockFetcherIterator: Started 6 remote fetches in 9 ms
    >   2021-05-17 00:30:35,438 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
    >   2021-05-17 00:30:35,438 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
    >   2021-05-17 00:30:35,438 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
    >   2021-05-17 00:30:35,438 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
    >   2021-05-17 00:30:35,443 INFO codec.CodecConfig: Compression: SNAPPY
    >   2021-05-17 00:30:35,443 INFO codec.CodecConfig: Compression: SNAPPY
    >   2021-05-17 00:30:35,443 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
    >   2021-05-17 00:30:35,443 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
    >   2021-05-17 00:30:35,443 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
    >   2021-05-17 00:30:35,443 INFO hadoop.ParquetOutputFormat: Dictionary is on
    >   2021-05-17 00:30:35,443 INFO hadoop.ParquetOutputFormat: Validation is off
    >   2021-05-17 00:30:35,443 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
    >   2021-05-17 00:30:35,443 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
    >   2021-05-17 00:30:35,443 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
    >   2021-05-17 00:30:35,443 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
    >   2021-05-17 00:30:35,443 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
    >   2021-05-17 00:30:35,444 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
    >   ....

    HDFS

    >   ....
    >   2021-05-17T00:29:21+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00217-dc9b579c-2247-48f9-8f92-6ffbc0384845_00217.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C00000200000000000004000047189141a0fbb60b713efc096949a1b4
    >   real 2.36
    >   user 2.45
    >   sys 0.21
    >   ----
    >   2021-05-17T00:29:44+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00218-dc9b579c-2247-48f9-8f92-6ffbc0384845_00218.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C0000020000000000000400000c97d084ec413e0b5e37afe88254c1bf
    >   real 1.80
    >   user 2.24
    >   sys 0.22
    >   ----
    >   2021-05-17T00:30:06+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00219-dc9b579c-2247-48f9-8f92-6ffbc0384845_00219.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	000002000000000000040000b2885773d37d4bbd9234f225633f5111
    >   real 1.87
    >   user 2.24
    >   sys 0.21
    >   ----
    >   2021-05-17T00:30:27+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00220-dc9b579c-2247-48f9-8f92-6ffbc0384845_00220.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000d10f9491ca2a361ca1cd00feb96b359c
    >   real 2.24
    >   user 2.33
    >   sys 0.21
    >   ----
    >   2021-05-17T00:30:50+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00221-dc9b579c-2247-48f9-8f92-6ffbc0384845_00221.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C00000200000000000004000055e0ade90e8411af98c11cf2fc57d150
    >   real 2.32
    >   user 2.47
    >   sys 0.26
    >   ----
    >   2021-05-17T00:31:12+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00222-dc9b579c-2247-48f9-8f92-6ffbc0384845_00222.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000bc547268d295863d930168f71b70f7f1
    >   real 2.76
    >   user 2.26
    >   sys 0.22
    >   ----
    >   2021-05-17T00:31:35+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00223-dc9b579c-2247-48f9-8f92-6ffbc0384845_00223.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000ed7f1144af97bdf9d659547084edc867
    >   real 1.81
    >   user 2.36
    >   sys 0.20
    >   ----
    >   2021-05-17T00:31:57+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00224-dc9b579c-2247-48f9-8f92-6ffbc0384845_00224.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	000002000000000000040000cead07e93f62bfeeb8ddefcfad9840fb
    >   real 2.39
    >   user 2.15
    >   sys 0.23
    >   ....

    worker

    >   ....
    >   2021-05-17T00:31:16+00:00
    >   4dbb84c8b1f7f1676ea9bee54cae9a2f  part-01472-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 4.18
    >   user 0.06
    >   sys 0.02
    >   ----
    >   2021-05-17T00:31:40+00:00
    >   ee1dbbc73887e24c7d36d6cd2aab45a3  part-01473-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 3.80
    >   user 0.07
    >   sys 0.02
    >   ----
    >   2021-05-17T00:32:04+00:00
    >   80356cfeebc2bdda7658f27ab64f14d0  part-01474-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 1.84
    >   user 0.07
    >   sys 0.02
    >   ----
    >   2021-05-17T00:32:25+00:00
    >   ea68a6b96ea3c912bfcc281191d6db94  part-01475-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 2.96
    >   user 0.07
    >   sys 0.01
    >   ....

    master

    >   ....
    >   2021-05-17T00:32:01+00:00
    >   8d0b672af587c65f3234dc1fd2810136  part-00166-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 1.91
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T00:32:23+00:00
    >   0ad5a1e8eed89363103152849fe0068a  part-00167-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 2.59
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T00:32:46+00:00
    >   400046d16cac484e59973a8ee9d04dff  part-00168-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 3.64
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T00:33:09+00:00
    >   84980351c4cb27f424287c2f0b62665f  part-00169-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 3.10
    >   user 0.07
    >   sys 0.01
    >   ....



Worker


    >   ....
    >   
    >   2021-05-17 00:32:25,511 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 129964475
    >   2021-05-17 00:32:26,686 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210516231315_0003_m_000115_15836' to hdfs://master01:9000/partitioned/GEDR3-8192/_temporary/0/task_20210516231315_0003_m_000115
    >   2021-05-17 00:32:26,686 INFO mapred.SparkHadoopMapRedUtil: attempt_20210516231315_0003_m_000115_15836: Committed
    >   2021-05-17 00:32:26,686 INFO executor.Executor: Finished task 115.0 in stage 3.0 (TID 15836). 3073 bytes result sent to driver
    >   2021-05-17 00:32:26,688 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15845
    >   2021-05-17 00:32:26,688 INFO executor.Executor: Running task 124.0 in stage 3.0 (TID 15845)
    >   2021-05-17 00:32:26,705 INFO storage.ShuffleBlockFetcherIterator: Getting 5720 non-empty blocks including 1952 local blocks and 3768 remote blocks
    >   2021-05-17 00:32:26,709 INFO storage.ShuffleBlockFetcherIterator: Started 6 remote fetches in 8 ms
    >   2021-05-17 00:32:28,474 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
    >   2021-05-17 00:32:28,474 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
    >   2021-05-17 00:32:28,474 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
    >   2021-05-17 00:32:28,474 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
    >   2021-05-17 00:32:28,479 INFO codec.CodecConfig: Compression: SNAPPY
    >   2021-05-17 00:32:28,479 INFO codec.CodecConfig: Compression: SNAPPY
    >   2021-05-17 00:32:28,479 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
    >   2021-05-17 00:32:28,479 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
    >   2021-05-17 00:32:28,479 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
    >   2021-05-17 00:32:28,479 INFO hadoop.ParquetOutputFormat: Dictionary is on
    >   2021-05-17 00:32:28,479 INFO hadoop.ParquetOutputFormat: Validation is off
    >   2021-05-17 00:32:28,479 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
    >   2021-05-17 00:32:28,479 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
    >   2021-05-17 00:32:28,479 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
    >   2021-05-17 00:32:28,479 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
    >   2021-05-17 00:32:28,479 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
    >   2021-05-17 00:32:28,480 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
    >   ....
    >   
    >   2021-05-17 00:32:28,902 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
    >   2021-05-17 00:32:28,902 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
    >   2021-05-17 00:32:28,902 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
    >   2021-05-17 00:32:28,902 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
    >   2021-05-17 00:32:28,908 INFO codec.CodecConfig: Compression: SNAPPY
    >   2021-05-17 00:32:28,908 INFO codec.CodecConfig: Compression: SNAPPY
    >   2021-05-17 00:32:28,908 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
    >   2021-05-17 00:32:28,908 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
    >   2021-05-17 00:32:28,908 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
    >   2021-05-17 00:32:28,908 INFO hadoop.ParquetOutputFormat: Dictionary is on
    >   2021-05-17 00:32:28,908 INFO hadoop.ParquetOutputFormat: Validation is off
    >   2021-05-17 00:32:28,908 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
    >   2021-05-17 00:32:28,908 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
    >   2021-05-17 00:32:28,908 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
    >   2021-05-17 00:32:28,908 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
    >   2021-05-17 00:32:28,908 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
    >   2021-05-17 00:32:28,909 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
    >   ....
    >   2021-05-17 00:32:35,054 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 129241208


    Then the wheels fall off ...

Zeppelin

 INFO [2021-05-17 00:36:26,209] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Starting task 248.0 in stage 2.1 (TID 16100, worker02, executor 3, partition 767, PROCESS_LOCAL, 8450 bytes)
 INFO [2021-05-17 00:36:26,210] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 244.0 in stage 2.1 (TID 16096) in 3799 ms on worker02 (executor 3) (241/1952)
 INFO [2021-05-17 00:36:26,811] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Starting task 249.0 in stage 2.1 (TID 16101, worker02, executor 3, partition 768, PROCESS_LOCAL, 8450 bytes)
 INFO [2021-05-17 00:36:26,811] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 245.0 in stage 2.1 (TID 16097) in 3788 ms on worker02 (executor 3) (242/1952)
 INFO [2021-05-17 00:36:39,272] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Starting task 250.0 in stage 2.1 (TID 16102, worker04, executor 2, partition 773, PROCESS_LOCAL, 8450 bytes)
 INFO [2021-05-17 00:36:39,272] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 235.0 in stage 2.1 (TID 16087) in 21857 ms on worker04 (executor 2) (243/1952)
 INFO [2021-05-17 00:36:39,273] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Starting task 251.0 in stage 2.1 (TID 16103, worker04, executor 2, partition 774, PROCESS_LOCAL, 8450 bytes)
 INFO [2021-05-17 00:36:39,273] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 239.0 in stage 2.1 (TID 16091) in 19357 ms on worker04 (executor 2) (244/1952)
 WARN [2021-05-17 00:36:39,534] ({dispatcher-event-loop-7} Logging.scala[logWarning]:66) - Requesting driver to remove executor 3 for reason Container marked as failed: container_1619571756695_0027_01_000005 on host: worker02. Exit status: -100. Diagnostics: Container released on a *lost* node.
 WARN [2021-05-17 00:36:39,534] ({dispatcher-event-loop-7} Logging.scala[logWarning]:66) - Requesting driver to remove executor 2 for reason Container marked as failed: container_1619571756695_0027_01_000004 on host: worker04. Exit status: -100. Diagnostics: Container released on a *lost* node.
ERROR [2021-05-17 00:36:39,534] ({dispatcher-event-loop-7} Logging.scala[logError]:70) - Lost executor 3 on worker02: Container marked as failed: container_1619571756695_0027_01_000005 on host: worker02. Exit status: -100. Diagnostics: Container released on a *lost* node.
 INFO [2021-05-17 00:36:39,538] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Task 16098 failed because while it was being computed, its executor exited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task.
 INFO [2021-05-17 00:36:39,538] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Task 16101 failed because while it was being computed, its executor exited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task.
 INFO [2021-05-17 00:36:39,539] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Task 16100 failed because while it was being computed, its executor exited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task.
 INFO [2021-05-17 00:36:39,539] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 528), so marking it as still running.
 INFO [2021-05-17 00:36:39,539] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Task 16099 failed because while it was being computed, its executor exited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task.
 INFO [2021-05-17 00:36:39,539] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 704), so marking it as still running.
ERROR [2021-05-17 00:36:39,539] ({dispatcher-event-loop-7} Logging.scala[logError]:70) - Lost executor 2 on worker04: Container marked as failed: container_1619571756695_0027_01_000004 on host: worker04. Exit status: -100. Diagnostics: Container released on a *lost* node.
ERROR [2021-05-17 00:36:39,539] ({spark-listener-group-eventLog} Logging.scala[logError]:91) - Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.10.1.255:9866,DS-91baca01-0653-4178-9663-93761c26c553,DISK], DatanodeInfoWithStorage[10.10.3.14:9866,DS-585c3c40-042b-4247-816f-6ee06b9f60c9,DISK]], original=[DatanodeInfoWithStorage[10.10.1.255:9866,DS-91baca01-0653-4178-9663-93761c26c553,DISK], DatanodeInfoWithStorage[10.10.3.14:9866,DS-585c3c40-042b-4247-816f-6ee06b9f60c9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
 INFO [2021-05-17 00:36:39,540] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Task 16092 failed because while it was being computed, its executor exited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task.
 INFO [2021-05-17 00:36:39,540] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Task 16094 failed because while it was being computed, its executor exited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task.
 INFO [2021-05-17 00:36:39,540] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Task 16103 failed because while it was being computed, its executor exited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task.
 INFO [2021-05-17 00:36:39,540] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Task 16102 failed because while it was being computed, its executor exited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task.
ERROR [2021-05-17 00:36:39,540] ({spark-listener-group-eventLog} Logging.scala[logError]:91) - Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.10.1.255:9866,DS-91baca01-0653-4178-9663-93761c26c553,DISK], DatanodeInfoWithStorage[10.10.3.14:9866,DS-585c3c40-042b-4247-816f-6ee06b9f60c9,DISK]], original=[DatanodeInfoWithStorage[10.10.1.255:9866,DS-91baca01-0653-4178-9663-93761c26c553,DISK], DatanodeInfoWithStorage[10.10.3.14:9866,DS-585c3c40-042b-4247-816f-6ee06b9f60c9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
 INFO [2021-05-17 00:36:39,539] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 722), so marking it as still running.
 INFO [2021-05-17 00:36:39,546] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 612), so marking it as still running.
 INFO [2021-05-17 00:36:39,546] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 493), so marking it as still running.
 INFO [2021-05-17 00:36:39,546] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 370), so marking it as still running.
 INFO [2021-05-17 00:36:39,546] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 587), so marking it as still running.
 INFO [2021-05-17 00:36:39,546] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 430), so marking it as still running.
 INFO [2021-05-17 00:36:39,546] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 453), so marking it as still running.
 INFO [2021-05-17 00:36:39,546] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 654), so marking it as still running.
 INFO [2021-05-17 00:36:39,546] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 689), so marking it as still running.
 INFO [2021-05-17 00:36:39,546] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 598), so marking it as still running.
 INFO [2021-05-17 00:36:39,546] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 619), so marking it as still running.
 INFO [2021-05-17 00:36:39,546] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 638), so marking it as still running.
 INFO [2021-05-17 00:36:39,546] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 567), so marking it as still running.
 INFO [2021-05-17 00:36:39,546] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 225), so marking it as still running.
 INFO [2021-05-17 00:36:39,546] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 436), so marking it as still running.
 INFO [2021-05-17 00:36:39,546] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 306), so marking it as still running.
 INFO [2021-05-17 00:36:39,546] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 539), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 115), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 465), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 362), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 310), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 230), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 334), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 211), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 189), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 447), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 11), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 241), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 271), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 197), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 74), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 152), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 299), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 2), so marking it as still running.
 INFO [2021-05-17 00:36:39,547] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 616), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 636), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 80), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 31), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 54), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 662), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 691), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 565), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 589), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 388), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 419), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 527), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 720), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 545), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 571), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 600), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 463), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 743), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 298), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 515), so marking it as still running.
 INFO [2021-05-17 00:36:39,548] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 369), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 398), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 611), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 709), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 651), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 344), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 266), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 216), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 451), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 675), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 246), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 377), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 53), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 77), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 30), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 221), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 104), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 263), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 8), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 229), so marking it as still running.
 INFO [2021-05-17 00:36:39,549] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 35), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 188), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 101), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 151), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 706), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 724), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 674), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 712), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 733), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 615), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 455), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 290), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 524), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 620), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 540), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 742), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 416), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 357), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 570), so marking it as still running.
 INFO [2021-05-17 00:36:39,550] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 190), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 609), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 532), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 215), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 510), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 92), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 343), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 423), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 314), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 296), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 550), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 272), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 374), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 245), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 76), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 352), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 176), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 49), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 407), so marking it as still running.
 INFO [2021-05-17 00:36:39,551] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 432), so marking it as still running.
 INFO [2021-05-17 00:36:39,552] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 103), so marking it as still running.
 INFO [2021-05-17 00:36:39,552] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 57), so marking it as still running.
 INFO [2021-05-17 00:36:39,552] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 5), so marking it as still running.
 INFO [2021-05-17 00:36:39,552] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 741), so marking it as still running.
 INFO [2021-05-17 00:36:39,552] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 663), so marking it as still running.
 INFO [2021-05-17 00:36:39,552] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 226), so marking it as still running.
 INFO [2021-05-17 00:36:39,552] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 208), so marking it as still running.
 INFO [2021-05-17 00:36:39,552] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 33), so marking it as still running.
 INFO [2021-05-17 00:36:39,552] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 3 (epoch 11)
 INFO [2021-05-17 00:36:39,552] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Trying to remove executor 3 from BlockManagerMaster.
 INFO [2021-05-17 00:36:39,552] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(3, worker02, 46645, None)
 INFO [2021-05-17 00:36:39,553] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 3 successfully in removeExecutor
 INFO [2021-05-17 00:36:39,553] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 3 (epoch 11)
ERROR [2021-05-17 00:36:39,558] ({spark-listener-group-eventLog} Logging.scala[logError]:91) - Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.10.1.255:9866,DS-91baca01-0653-4178-9663-93761c26c553,DISK], DatanodeInfoWithStorage[10.10.3.14:9866,DS-585c3c40-042b-4247-816f-6ee06b9f60c9,DISK]], original=[DatanodeInfoWithStorage[10.10.1.255:9866,DS-91baca01-0653-4178-9663-93761c26c553,DISK], DatanodeInfoWithStorage[10.10.3.14:9866,DS-585c3c40-042b-4247-816f-6ee06b9f60c9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
 INFO [2021-05-17 00:36:39,564] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 507), so marking it as still running.
 INFO [2021-05-17 00:36:39,564] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 20), so marking it as still running.
 INFO [2021-05-17 00:36:39,564] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 40), so marking it as still running.
 INFO [2021-05-17 00:36:39,564] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 649), so marking it as still running.
 INFO [2021-05-17 00:36:39,564] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 673), so marking it as still running.
 INFO [2021-05-17 00:36:39,564] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 602), so marking it as still running.
 INFO [2021-05-17 00:36:39,564] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 622), so marking it as still running.
 INFO [2021-05-17 00:36:39,565] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 549), so marking it as still running.
 INFO [2021-05-17 00:36:39,565] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 576), so marking it as still running.
 INFO [2021-05-17 00:36:39,565] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 318), so marking it as still running.
 INFO [2021-05-17 00:36:39,565] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 406), so marking it as still running.
 INFO [2021-05-17 00:36:39,565] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 536), so marking it as still running.
 INFO [2021-05-17 00:36:39,565] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 629), so marking it as still running.
 INFO [2021-05-17 00:36:39,565] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 556), so marking it as still running.
 INFO [2021-05-17 00:36:39,565] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 516), so marking it as still running.
 INFO [2021-05-17 00:36:39,565] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 350), so marking it as still running.
 INFO [2021-05-17 00:36:39,565] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 710), so marking it as still running.
 INFO [2021-05-17 00:36:39,566] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 415), so marking it as still running.
 INFO [2021-05-17 00:36:39,566] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 110), so marking it as still running.
 INFO [2021-05-17 00:36:39,566] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 355), so marking it as still running.
 INFO [2021-05-17 00:36:39,566] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 379), so marking it as still running.
 INFO [2021-05-17 00:36:39,566] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 252), so marking it as still running.
 INFO [2021-05-17 00:36:39,566] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 326), so marking it as still running.
 INFO [2021-05-17 00:36:39,566] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 274), so marking it as still running.
 INFO [2021-05-17 00:36:39,566] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 500), so marking it as still running.
 INFO [2021-05-17 00:36:39,566] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 523), so marking it as still running.
 INFO [2021-05-17 00:36:39,566] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 178), so marking it as still running.
 INFO [2021-05-17 00:36:39,566] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 201), so marking it as still running.
 INFO [2021-05-17 00:36:39,567] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 459), so marking it as still running.
 INFO [2021-05-17 00:36:39,567] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 39), so marking it as still running.
 INFO [2021-05-17 00:36:39,567] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 389), so marking it as still running.
 INFO [2021-05-17 00:36:39,567] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 264), so marking it as still running.
 INFO [2021-05-17 00:36:39,567] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 292), so marking it as still running.
 INFO [2021-05-17 00:36:39,567] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 64), so marking it as still running.
 INFO [2021-05-17 00:36:39,567] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 420), so marking it as still running.
 INFO [2021-05-17 00:36:39,567] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 91), so marking it as still running.
 INFO [2021-05-17 00:36:39,567] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 217), so marking it as still running.
 INFO [2021-05-17 00:36:39,570] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 102), so marking it as still running.
 INFO [2021-05-17 00:36:39,570] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 727), so marking it as still running.
 INFO [2021-05-17 00:36:39,571] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 22), so marking it as still running.
 INFO [2021-05-17 00:36:39,571] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 46), so marking it as still running.
 INFO [2021-05-17 00:36:39,571] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 713), so marking it as still running.
 INFO [2021-05-17 00:36:39,571] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 621), so marking it as still running.
 INFO [2021-05-17 00:36:39,571] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 644), so marking it as still running.
 INFO [2021-05-17 00:36:39,571] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 505), so marking it as still running.
 INFO [2021-05-17 00:36:39,571] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 443), so marking it as still running.
 INFO [2021-05-17 00:36:39,571] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 672), so marking it as still running.
 INFO [2021-05-17 00:36:39,571] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 701), so marking it as still running.
 INFO [2021-05-17 00:36:39,571] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 534), so marking it as still running.
 INFO [2021-05-17 00:36:39,572] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 485), so marking it as still running.
 INFO [2021-05-17 00:36:39,572] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 425), so marking it as still running.
 INFO [2021-05-17 00:36:39,572] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 628), so marking it as still running.
 INFO [2021-05-17 00:36:39,572] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 315), so marking it as still running.
 INFO [2021-05-17 00:36:39,572] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 581), so marking it as still running.
 INFO [2021-05-17 00:36:39,572] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 240), so marking it as still running.
 INFO [2021-05-17 00:36:39,572] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 554), so marking it as still running.
 INFO [2021-05-17 00:36:39,572] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 177), so marking it as still running.
 INFO [2021-05-17 00:36:39,572] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 408), so marking it as still running.
 INFO [2021-05-17 00:36:39,572] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 519), so marking it as still running.
 INFO [2021-05-17 00:36:39,572] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 435), so marking it as still running.
 INFO [2021-05-17 00:36:39,572] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 457), so marking it as still running.
 INFO [2021-05-17 00:36:39,573] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 495), so marking it as still running.
 INFO [2021-05-17 00:36:39,573] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 323), so marking it as still running.
 INFO [2021-05-17 00:36:39,573] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 354), so marking it as still running.
 INFO [2021-05-17 00:36:39,573] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 273), so marking it as still running.
 INFO [2021-05-17 00:36:39,573] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 303), so marking it as still running.
 INFO [2021-05-17 00:36:39,573] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 200), so marking it as still running.
 INFO [2021-05-17 00:36:39,573] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 538), so marking it as still running.
 INFO [2021-05-17 00:36:39,573] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 291), so marking it as still running.
 INFO [2021-05-17 00:36:39,573] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 360), so marking it as still running.
 INFO [2021-05-17 00:36:39,573] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 90), so marking it as still running.
 INFO [2021-05-17 00:36:39,573] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 112), so marking it as still running.
 INFO [2021-05-17 00:36:39,574] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 62), so marking it as still running.
 INFO [2021-05-17 00:36:39,574] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 209), so marking it as still running.
 INFO [2021-05-17 00:36:39,574] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 309), so marking it as still running.
 INFO [2021-05-17 00:36:39,574] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 333), so marking it as still running.
 INFO [2021-05-17 00:36:39,574] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 68), so marking it as still running.
 INFO [2021-05-17 00:36:39,574] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 21), so marking it as still running.
 INFO [2021-05-17 00:36:39,574] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 626), so marking it as still running.
 INFO [2021-05-17 00:36:39,574] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 650), so marking it as still running.
 INFO [2021-05-17 00:36:39,574] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 193), so marking it as still running.
 INFO [2021-05-17 00:36:39,574] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 42), so marking it as still running.
 INFO [2021-05-17 00:36:39,574] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 634), so marking it as still running.
 INFO [2021-05-17 00:36:39,575] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 661), so marking it as still running.
 INFO [2021-05-17 00:36:39,575] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 690), so marking it as still running.
 INFO [2021-05-17 00:36:39,575] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 588), so marking it as still running.
 INFO [2021-05-17 00:36:39,575] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 518), so marking it as still running.
 INFO [2021-05-17 00:36:39,575] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 537), so marking it as still running.
 INFO [2021-05-17 00:36:39,575] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 494), so marking it as still running.
 INFO [2021-05-17 00:36:39,575] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 558), so marking it as still running.
 INFO [2021-05-17 00:36:39,575] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 307), so marking it as still running.
 INFO [2021-05-17 00:36:39,575] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 380), so marking it as still running.
 INFO [2021-05-17 00:36:39,575] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 461), so marking it as still running.
 INFO [2021-05-17 00:36:39,575] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 503), so marking it as still running.
 INFO [2021-05-17 00:36:39,575] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 718), so marking it as still running.
 INFO [2021-05-17 00:36:39,576] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 330), so marking it as still running.
 INFO [2021-05-17 00:36:39,576] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 599), so marking it as still running.
 INFO [2021-05-17 00:36:39,576] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 700), so marking it as still running.
 INFO [2021-05-17 00:36:39,576] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 442), so marking it as still running.
 INFO [2021-05-17 00:36:39,576] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 639), so marking it as still running.
 INFO [2021-05-17 00:36:39,576] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 664), so marking it as still running.
 INFO [2021-05-17 00:36:39,576] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 390), so marking it as still running.
 INFO [2021-05-17 00:36:39,576] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 577), so marking it as still running.
 INFO [2021-05-17 00:36:39,576] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 234), so marking it as still running.
 INFO [2021-05-17 00:36:39,576] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 265), so marking it as still running.
 INFO [2021-05-17 00:36:39,576] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 450), so marking it as still running.
 INFO [2021-05-17 00:36:39,576] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 471), so marking it as still running.
 INFO [2021-05-17 00:36:39,577] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 65), so marking it as still running.
 INFO [2021-05-17 00:36:39,577] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 368), so marking it as still running.
 INFO [2021-05-17 00:36:39,577] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 150), so marking it as still running.
 INFO [2021-05-17 00:36:39,577] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 23), so marking it as still running.
 INFO [2021-05-17 00:36:39,577] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 300), so marking it as still running.
 INFO [2021-05-17 00:36:39,577] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 218), so marking it as still running.
 INFO [2021-05-17 00:36:39,577] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 319), so marking it as still running.
 INFO [2021-05-17 00:36:39,577] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 199), so marking it as still running.
 INFO [2021-05-17 00:36:39,577] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 254), so marking it as still running.
 INFO [2021-05-17 00:36:39,577] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 111), so marking it as still running.
 INFO [2021-05-17 00:36:39,577] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 184), so marking it as still running.
 INFO [2021-05-17 00:36:39,577] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 89), so marking it as still running.
 INFO [2021-05-17 00:36:39,578] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 716), so marking it as still running.
 INFO [2021-05-17 00:36:39,578] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(2, 696), so marking it as still running.
 INFO [2021-05-17 00:36:39,578] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 2 (epoch 12)
 INFO [2021-05-17 00:36:39,578] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Trying to remove executor 2 from BlockManagerMaster.
 INFO [2021-05-17 00:36:39,578] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(2, worker04, 36857, None)
 INFO [2021-05-17 00:36:39,578] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 2 successfully in removeExecutor
 INFO [2021-05-17 00:36:39,578] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 2 (epoch 12)
ERROR [2021-05-17 00:36:39,579] ({spark-listener-group-eventLog} Logging.scala[logError]:91) - Listener EventLoggingListener threw an exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.10.1.255:9866,DS-91baca01-0653-4178-9663-93761c26c553,DISK], DatanodeInfoWithStorage[10.10.3.14:9866,DS-585c3c40-042b-4247-816f-6ee06b9f60c9,DISK]], original=[DatanodeInfoWithStorage[10.10.1.255:9866,DS-91baca01-0653-4178-9663-93761c26c553,DISK], DatanodeInfoWithStorage[10.10.3.14:9866,DS-585c3c40-042b-4247-816f-6ee06b9f60c9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:925)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:988)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1156)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:871)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:401)
 WARN [2021-05-17 00:36:45,989] ({rpc-server-4-7} TransportChannelHandler.java[exceptionCaught]:78) - Exception in connection from /10.10.2.218:34002
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:377)
	at io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1133)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:148)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)


    Notebook stalled at [RUNNING 51%]




    HDFS

    >   ....
    >   2021-05-17T00:28:16+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00214-dc9b579c-2247-48f9-8f92-6ffbc0384845_00214.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C0000020000000000000400001873252b0725f21cd78320e360ef9cfa
    >   real 1.67
    >   user 2.16
    >   sys 0.21
    >   ----
    >   2021-05-17T00:28:38+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00215-dc9b579c-2247-48f9-8f92-6ffbc0384845_00215.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000c3ffec8f63814393c8544209116cd8d5
    >   real 1.90
    >   user 2.42
    >   sys 0.23
    >   ----
    >   2021-05-17T00:29:00+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00216-dc9b579c-2247-48f9-8f92-6ffbc0384845_00216.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000f07ef0b592067764bf1abd25975c3c49
    >   real 1.70
    >   user 2.01
    >   sys 0.19
    >   ----
    >   2021-05-17T00:29:21+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00217-dc9b579c-2247-48f9-8f92-6ffbc0384845_00217.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C00000200000000000004000047189141a0fbb60b713efc096949a1b4
    >   real 2.36
    >   user 2.45
    >   sys 0.21
    >   ----
    >   2021-05-17T00:29:44+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00218-dc9b579c-2247-48f9-8f92-6ffbc0384845_00218.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C0000020000000000000400000c97d084ec413e0b5e37afe88254c1bf
    >   real 1.80
    >   user 2.24
    >   sys 0.22
    >   ----
    >   2021-05-17T00:30:06+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00219-dc9b579c-2247-48f9-8f92-6ffbc0384845_00219.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	000002000000000000040000b2885773d37d4bbd9234f225633f5111
    >   real 1.87
    >   user 2.24
    >   sys 0.21
    >   ----
    >   2021-05-17T00:30:27+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00220-dc9b579c-2247-48f9-8f92-6ffbc0384845_00220.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000d10f9491ca2a361ca1cd00feb96b359c
    >   real 2.24
    >   user 2.33
    >   sys 0.21
    >   ----
    >   2021-05-17T00:30:50+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00221-dc9b579c-2247-48f9-8f92-6ffbc0384845_00221.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C00000200000000000004000055e0ade90e8411af98c11cf2fc57d150
    >   real 2.32
    >   user 2.47
    >   sys 0.26
    >   ----
    >   2021-05-17T00:31:12+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00222-dc9b579c-2247-48f9-8f92-6ffbc0384845_00222.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000bc547268d295863d930168f71b70f7f1
    >   real 2.76
    >   user 2.26
    >   sys 0.22
    >   ----
    >   2021-05-17T00:31:35+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00223-dc9b579c-2247-48f9-8f92-6ffbc0384845_00223.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C000002000000000000040000ed7f1144af97bdf9d659547084edc867
    >   real 1.81
    >   user 2.36
    >   sys 0.20
    >   ----
    >   2021-05-17T00:31:57+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00224-dc9b579c-2247-48f9-8f92-6ffbc0384845_00224.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	000002000000000000040000cead07e93f62bfeeb8ddefcfad9840fb
    >   real 2.39
    >   user 2.15
    >   sys 0.23
    >   ----
    >   ....

    master

    >   ....
    >   2021-05-17T00:29:19+00:00
    >   3b71377bbfed231b8d78d7b75a2a6893  part-00159-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 2.53
    >   user 0.08
    >   sys 0.01
    >   ----
    >   2021-05-17T00:29:42+00:00
    >   35aebf8b079c9f13d8dd84775c599493  part-00160-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 1.32
    >   user 0.08
    >   sys 0.00
    >   ----
    >   2021-05-17T00:30:03+00:00
    >   e4caf562ba8a62e712502654fc34af60  part-00161-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 4.21
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T00:30:27+00:00
    >   f960b47b961ee6b61be34b61caa3045b  part-00162-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 5.71
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T00:30:53+00:00
    >   964b0366fdf185346914c1ac6405d8ac  part-00163-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 2.05
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T00:31:15+00:00
    >   ea00cf6f86ab0dd5b86eadf701b89885  part-00164-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 2.21
    >   user 0.08
    >   sys 0.01
    >   ----
    >   2021-05-17T00:31:37+00:00
    >   50ba4522274b93d2c259d0014990fa18  part-00165-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 3.88
    >   user 0.08
    >   sys 0.01
    >   ....

    worker

    >   ....
    >   2021-05-17T00:31:16+00:00
    >   4dbb84c8b1f7f1676ea9bee54cae9a2f  part-01472-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 4.18
    >   user 0.06
    >   sys 0.02
    >   ----
    >   2021-05-17T00:31:40+00:00
    >   ee1dbbc73887e24c7d36d6cd2aab45a3  part-01473-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 3.80
    >   user 0.07
    >   sys 0.02
    >   ----
    >   2021-05-17T00:32:04+00:00
    >   80356cfeebc2bdda7658f27ab64f14d0  part-01474-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 1.84
    >   user 0.07
    >   sys 0.02
    >   ----
    >   2021-05-17T00:32:25+00:00
    >   ea68a6b96ea3c912bfcc281191d6db94  part-01475-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 2.96
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T00:32:48+00:00
    >   07feae5f7718139aa061fd05474ce7de  part-01476-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 1.70
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T00:33:10+00:00
    >   96cf0dc9dbfb438c5f25e4f084736fe1  part-01477-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 1.09
    >   user 0.06
    >   sys 0.02
    >   ----
    >   2021-05-17T00:33:31+00:00
    >   fead0b3d78a124572a26b6f7150540fd  part-01478-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 1.12
    >   user 0.07
    >   sys 0.02
    >   ....


    Afterwards ...

    master

    >   ....
    >   2021-05-17T00:43:46+00:00
    >   6f80627703654796c3943bbbd1f696ad  part-00199-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 0.85
    >   user 0.08
    >   sys 0.01
    >   ----
    >   2021-05-17T00:44:07+00:00
    >   12efbd9b0943f6656404b9a1c1117e7e  part-00200-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 0.69
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T00:44:28+00:00
    >   5d7132ea8acf4ec3f3765cea97bf5bc8  part-00201-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 0.83
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T00:44:49+00:00
    >   45fe49a139544a20bd7c87e35886c705  part-00202-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 0.86
    >   user 0.07
    >   sys 0.01
    >   ....

    worker

    >   ....
    >   2021-05-17T00:44:03+00:00
    >   8cfc602e1ea7d1c180ea69a14a8897b6  part-01508-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 0.81
    >   user 0.07
    >   sys 0.02
    >   ----
    >   2021-05-17T00:44:24+00:00
    >   1a03c3b660b878e3216bf5e6ae3e6173  part-01509-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 0.90
    >   user 0.06
    >   sys 0.02
    >   ----
    >   2021-05-17T00:44:45+00:00
    >   1fb17d918d078f98c90a4e6d060fe9f3  part-01510-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 0.77
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T00:45:06+00:00
    >   464bf681c43d48f473dd27a12e40f8a8  part-01511-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 0.91
    >   user 0.07
    >   sys 0.01
    >   ....

    HDFS

    >   ....
    >   2021-05-17T00:44:20+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00258-dc9b579c-2247-48f9-8f92-6ffbc0384845_00258.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	0000020000000000000400005a5150323e25be0323c72b96076c24f8
    >   real 1.66
    >   user 2.11
    >   sys 0.20
    >   ----
    >   2021-05-17T00:44:42+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00259-dc9b579c-2247-48f9-8f92-6ffbc0384845_00259.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	00000200000000000004000025b57b645600e7eef49112cd57bd68ac
    >   real 1.79
    >   user 2.18
    >   sys 0.26
    >   ----
    >   2021-05-17T00:45:03+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00260-dc9b579c-2247-48f9-8f92-6ffbc0384845_00260.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	000002000000000000040000c0e9533da34d1809d983cdfc48c733ad
    >   real 1.72
    >   user 2.14
    >   sys 0.25
    >   ----
    >   2021-05-17T00:45:25+00:00
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00261-dc9b579c-2247-48f9-8f92-6ffbc0384845_00261.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	0000020000000000000400002835f83b706bb287edb3462a8ccf0c98
    >   real 1.93
    >   user 2.26
    >   sys 0.20
    >   ----
    >   2021-05-17T00:45:47+00:00
    >   
    >   hdfs://master01:9000/partitioned/GEDR3-4096/part-00262-dc9b579c-2247-48f9-8f92-6ffbc0384845_00262.c000.snappy.parquet	MD5-of-262144MD5-of-512CRC32C	0000020000000000000400005f2df4348b7ed1ab899c71a7b0a763ed
    >   real 1.62
    >   user 1.98
    >   sys 0.18
    >   ....


    Zeppelin (late)

    df -h

    >   Filesystem      Size  Used Avail Use% Mounted on
    >   devtmpfs         23G     0   23G   0% /dev
    >   tmpfs            23G     0   23G   0% /dev/shm
    >   tmpfs            23G  656K   23G   1% /run
    >   tmpfs            23G     0   23G   0% /sys/fs/cgroup
    >   /dev/vda1        20G  7.0G   12G  38% /
    >   tmpfs           4.5G     0  4.5G   0% /run/user/1000
    >   /dev/vdb         59G   67M   56G   1% /mnt/local/vdb
    >   /dev/vdc        512G   17M  510G   1% /mnt/cinder/vdc
    >   ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
    >   ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
    >   ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
    >   ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
    >   ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
    >   ceph-fuse        10T  6.5T  3.6T  65% /user/nch
    >   ceph-fuse       1.0T  960G   65G  94% /user/zrq
    >   ceph-fuse       1.0T     0  1.0T   0% /user/stv
    >   ceph-fuse       1.0T     0  1.0T   0% /user/dcr

    Master

    df -h

    >   Filesystem      Size  Used Avail Use% Mounted on
    >   devtmpfs         11G     0   11G   0% /dev
    >   tmpfs            11G     0   11G   0% /dev/shm
    >   tmpfs            11G  552K   11G   1% /run
    >   tmpfs            11G     0   11G   0% /sys/fs/cgroup
    >   /dev/vda1        20G  6.4G   13G  35% /
    >   tmpfs           2.2G     0  2.2G   0% /run/user/1000
    >   ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
    >   ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
    >   ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
    >   ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
    >   ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
    >   ceph-fuse        10T  6.5T  3.6T  65% /user/nch
    >   ceph-fuse       1.0T  960G   65G  94% /user/zrq
    >   ceph-fuse       1.0T     0  1.0T   0% /user/stv
    >   ceph-fuse       1.0T     0  1.0T   0% /user/dcr

    Worker 01

    df -h

    >   Filesystem      Size  Used Avail Use% Mounted on
    >   devtmpfs         23G     0   23G   0% /dev
    >   tmpfs            23G     0   23G   0% /dev/shm
    >   tmpfs            23G  576K   23G   1% /run
    >   tmpfs            23G     0   23G   0% /sys/fs/cgroup
    >   /dev/vda1        20G  3.7G   16G  20% /
    >   /dev/vdb         59G   53M   56G   1% /mnt/local/vdb
    >   /dev/vdc        1.0T  599G  424G  59% /mnt/cinder/vdc
    >   tmpfs           4.5G     0  4.5G   0% /run/user/1000
    >   ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
    >   ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
    >   ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
    >   ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
    >   ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
    >   ceph-fuse        10T  6.5T  3.6T  65% /user/nch
    >   ceph-fuse       1.0T  960G   65G  94% /user/zrq
    >   ceph-fuse       1.0T     0  1.0T   0% /user/stv
    >   ceph-fuse       1.0T     0  1.0T   0% /user/dcr

    Worker 02

    df -h

    >   Filesystem      Size  Used Avail Use% Mounted on
    >   devtmpfs         23G     0   23G   0% /dev
    >   tmpfs            23G     0   23G   0% /dev/shm
    >   tmpfs            23G  576K   23G   1% /run
    >   tmpfs            23G     0   23G   0% /sys/fs/cgroup
    >   /dev/vda1        20G  3.2G   16G  17% /
    >   /dev/vdb         59G   53M   56G   1% /mnt/local/vdb
    >   /dev/vdc        1.0T  936G   88G  92% /mnt/cinder/vdc
    >   tmpfs           4.5G     0  4.5G   0% /run/user/1000
    >   ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
    >   ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
    >   ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
    >   ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
    >   ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
    >   ceph-fuse        10T  6.5T  3.6T  65% /user/nch
    >   ceph-fuse       1.0T  960G   65G  94% /user/zrq
    >   ceph-fuse       1.0T     0  1.0T   0% /user/stv
    >   ceph-fuse       1.0T     0  1.0T   0% /user/dcr

    Worker 03

    df -h

    >   Filesystem      Size  Used Avail Use% Mounted on
    >   devtmpfs         23G     0   23G   0% /dev
    >   tmpfs            23G     0   23G   0% /dev/shm
    >   tmpfs            23G  600K   23G   1% /run
    >   tmpfs            23G     0   23G   0% /sys/fs/cgroup
    >   /dev/vda1        20G  3.7G   16G  20% /
    >   /dev/vdb         59G   53M   56G   1% /mnt/local/vdb
    >   /dev/vdc        1.0T  925G   98G  91% /mnt/cinder/vdc
    >   tmpfs           4.5G     0  4.5G   0% /run/user/1000
    >   ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
    >   ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
    >   ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
    >   ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
    >   ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
    >   ceph-fuse        10T  6.5T  3.6T  65% /user/nch
    >   ceph-fuse       1.0T  960G   65G  94% /user/zrq
    >   ceph-fuse       1.0T     0  1.0T   0% /user/stv
    >   ceph-fuse       1.0T     0  1.0T   0% /user/dcr

    Worker 04

    df -h

    >   Filesystem      Size  Used Avail Use% Mounted on
    >   devtmpfs         23G     0   23G   0% /dev
    >   tmpfs            23G     0   23G   0% /dev/shm
    >   tmpfs            23G  576K   23G   1% /run
    >   tmpfs            23G     0   23G   0% /sys/fs/cgroup
    >   /dev/vda1        20G  3.7G   16G  20% /
    >   /dev/vdb         59G   53M   56G   1% /mnt/local/vdb
    >   /dev/vdc        1.0T  932G   92G  92% /mnt/cinder/vdc
    >   tmpfs           4.5G     0  4.5G   0% /run/user/1000
    >   ceph-fuse       512G  473G   40G  93% /data/gaia/dr2
    >   ceph-fuse       540G  533G  7.9G  99% /data/gaia/edr3
    >   ceph-fuse       350G  341G  9.9G  98% /data/wise/allwise
    >   ceph-fuse       300G  270G   31G  90% /data/panstarrs/dr1
    >   ceph-fuse        40G   37G  3.5G  92% /data/twomass/allsky
    >   ceph-fuse        10T  6.5T  3.6T  65% /user/nch
    >   ceph-fuse       1.0T  960G   65G  94% /user/zrq
    >   ceph-fuse       1.0T     0  1.0T   0% /user/stv
    >   ceph-fuse       1.0T     0  1.0T   0% /user/dcr



    Might have found the culprit ..;

    worker03

    cat hadoop-fedora-nodemanager-gaia-prod-20210428-worker03.novalocal.log


    >   ....
    >   2021-05-16 23:11:55,789 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1619571756695_0027_01_000003 transitioned from LOCALIZING to SCHEDULED
    >   2021-05-16 23:11:55,789 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler: Starting container [container_1619571756695_0027_01_000003]
    >   2021-05-16 23:11:55,802 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1619571756695_0027_01_000003 transitioned from SCHEDULED to RUNNING
    >   2021-05-16 23:11:55,802 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1619571756695_0027_01_000003
    >   2021-05-16 23:11:55,804 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1619571756695_0027/container_1619571756695_0027_01_000003/default_container_executor.sh]
    >   2021-05-16 23:11:56,646 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: container_1619571756695_0027_01_000003's ip = 10.10.3.14, and hostname = worker03
    >   2021-05-16 23:11:56,656 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Skipping monitoring container container_1619571756695_0027_01_000003 since CPU usage is not yet available.
    >   2021-05-16 23:12:38,825 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 6609983366, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-05-16 23:22:35,711 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting path : /var/hadoop/logs/userlogs/application_1619571756695_0026
    >   2021-05-16 23:22:38,825 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 6609983366, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-05-16 23:32:38,826 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 6609983366, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-05-16 23:42:38,826 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 6609983366, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-05-16 23:52:38,826 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 6609983366, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-05-17 00:02:38,827 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 6609983366, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-05-17 00:12:38,827 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 6609983366, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-05-17 00:22:38,827 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 6609983366, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-05-17 00:32:37,750 WARN org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir error, used space above threshold of 90.0%, removing from list of valid directories
    >   2021-05-17 00:32:37,769 WARN org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/logs/userlogs error, used space above threshold of 90.0%, removing from list of valid directories
    >   2021-05-17 00:32:37,769 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) failed: 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ; 1/1 log-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/logs/userlogs : used space above threshold of 90.0% ]
    >   2021-05-17 00:32:37,769 ERROR org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Most of the disks failed. 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ; 1/1 log-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/logs/userlogs : used space above threshold of 90.0% ]
    >   2021-05-17 00:32:38,828 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 6609983366, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-05-17 00:32:38,903 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1619571756695_0027_01_000003 transitioned from RUNNING to KILLING
    >   2021-05-17 00:32:38,903 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1619571756695_0027_01_000003
    >   2021-05-17 00:32:38,981 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1619571756695_0027_01_000003 is : 143
    >   2021-05-17 00:32:38,992 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher: Got exception while cleaning container container_1619571756695_0027_01_000003. Ignoring.
    >   2021-05-17 00:32:38,992 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1619571756695_0027_01_000003 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
    >   2021-05-17 00:32:38,992 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1619571756695_0027/container_1619571756695_0027_01_000003
    >   2021-05-17 00:32:38,993 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=fedora	OPERATION=Container Finished - Killed	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1619571756695_0027	CONTAINERID=container_1619571756695_0027_01_000003
    >   2021-05-17 00:32:38,993 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1619571756695_0027_01_000003 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
    >   2021-05-17 00:32:38,993 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1619571756695_0027_01_000003 from application application_1619571756695_0027
    >   2021-05-17 00:32:38,994 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1619571756695_0027_01_000003
    >   2021-05-17 00:32:39,050 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1619571756695_0027
    >   2021-05-17 00:32:42,999 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1619571756695_0027_01_000003]
    >   2021-05-17 00:42:38,828 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 6609983366, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-05-17 00:52:38,829 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 6609983366, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0


    du -h /var/hadoop/logs/

    >   3.2M    /var/hadoop/logs/userlogs/application_1619571756695_0027/container_1619571756695_0027_01_000003
    >   3.2M    /var/hadoop/logs/userlogs/application_1619571756695_0027
    >   3.2M    /var/hadoop/logs/userlogs
    >   32M     /var/hadoop/logs/


    du -h -d 2 /var/hadoop/temp/

    >   0       /var/hadoop/temp/nm-local-dir/usercache_DEL_1620911858767
    >   268G    /var/hadoop/temp/nm-local-dir/usercache
    >   0       /var/hadoop/temp/nm-local-dir/nmPrivate
    >   0       /var/hadoop/temp/nm-local-dir/filecache
    >   268G    /var/hadoop/temp/nm-local-dir
    >   268G    /var/hadoop/temp/


    df -h /var/hadoop/temp/

    >   Filesystem  Size  Used Avail Use% Mounted on
    >   /dev/vdc    1.0T  925G   98G  91% /mnt/cinder/vdc


    du -h -d 2 /mnt/cinder/vdc

    >   0       /mnt/cinder/vdc/hadoop/data
    >   32M     /mnt/cinder/vdc/hadoop/logs
    >   268G    /mnt/cinder/vdc/hadoop/temp
    >   269G    /mnt/cinder/vdc/hadoop
    >   655G    /mnt/cinder/vdc/hdfs/data
    >   655G    /mnt/cinder/vdc/hdfs
    >   923G    /mnt/cinder/vdc



