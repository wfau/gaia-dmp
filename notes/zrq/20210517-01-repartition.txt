#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    Target:

        Run the GEDR3-8192 repartition process ..

    Result:

        Work in progress


# -----------------------------------------------------
# Check the free space on the HDFS filesystem
#[user@zeppelin]

    hadoop fs -ls hdfs://master01:9000/

    >   Found 3 items
    >   drwxr-xr-x   - fedora supergroup          0 2021-05-16 23:13 hdfs://master01:9000/partitioned
    >   drwxr-xr-x   - fedora supergroup          0 2021-05-16 23:11 hdfs://master01:9000/spark-log
    >   drwxr-xr-x   - fedora supergroup          0 2021-04-28 01:19 hdfs://master01:9000/user

    hadoop fs -du -h -v hdfs://master01:9000/

    >   SIZE     DISK_SPACE_CONSUMED_WITH_ALL_REPLICAS  FULL_PATH_NAME
    >   569.6 G  1.7 T                                  hdfs://master01:9000/partitioned
    >   10.6 G   31.9 G                                 hdfs://master01:9000/spark-log
    >   266.2 G  798.5 G                                hdfs://master01:9000/user


    hadoop fs -df -h hdfs://master01:9000/

    >   Filesystem            Size   Used  Available  Use%
    >   hdfs://master01:9000   4 T  2.5 T    699.4 G   62%


# -----------------------------------------------------
# Delete all the current data from HDFS
#[user@zeppelin]

    hadoop fs -rm -r hdfs://master01:9000/user

    >   Deleted hdfs://master01:9000/user


    hadoop fs -rm -r hdfs://master01:9000/spark-log

    >   Deleted hdfs://master01:9000/spark-log


    hadoop fs -rm -r hdfs://master01:9000/partitioned

    >   Deleted hdfs://master01:9000/partitioned


    hadoop fs -ls hdfs://master01:9000/

    >   -


    hadoop fs -du -h -v hdfs://master01:9000/

    >   -


    hadoop fs -df -h hdfs://master01:9000/

    >   Filesystem            Size     Used  Available  Use%
    >   hdfs://master01:9000   4 T  274.3 G      2.1 T    7%


# -----------------------------------------------------
# Check the free space on the Zeppelin node.
#[user@zeppelin]

    ls -al /mnt/cinder/vdc/

    >   drwxrwsr-x. 1 root root    8 Apr 28 00:56 spark


    df -h /mnt/cinder/vdc/

    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdc        512G   17M  510G   1% /mnt/cinder/vdc


    du -h -d 2 /mnt/cinder/vdc/

    >   208K    /mnt/cinder/vdc/spark/temp
    >   208K    /mnt/cinder/vdc/spark
    >   224K    /mnt/cinder/vdc/


# -----------------------------------------------------
# Check the free space on the Master node.
#[user@zeppelin]

    ssh master01

    ls -al /mnt/local/vda/hadoop/

    >   drwxrwsr-x. 2 fedora fedora 4096 Apr 28 00:51 data
    >   drwxrwsr-x. 2 fedora fedora 4096 May 15 23:45 logs
    >   drwxrwsr-x. 3 fedora fedora 4096 Apr 28 00:53 meta
    >   drwxrwsr-x. 3 fedora fedora 4096 Apr 28 01:02 temp


    df -h /mnt/local/vda

    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vda1        20G  6.5G   13G  35% /


    du -h -d 2 /mnt/local/vda

    >   1.4G    /mnt/local/vda/hadoop/logs
    >   38M     /mnt/local/vda/hadoop/meta
    >   4.0K    /mnt/local/vda/hadoop/data
    >   37M     /mnt/local/vda/hadoop/temp
    >   1.4G    /mnt/local/vda/hadoop
    >   1.4G    /mnt/local/vda


    rm -rf /mnt/local/vda/hadoop/logs/*

    rm -rf /mnt/local/vda/hadoop/temp/*

    du -h -d 2 /mnt/local/vda

    >   4.0K    /mnt/local/vda/hadoop/logs
    >   38M     /mnt/local/vda/hadoop/meta
    >   4.0K    /mnt/local/vda/hadoop/data
    >   4.0K    /mnt/local/vda/hadoop/temp
    >   38M     /mnt/local/vda/hadoop
    >   38M     /mnt/local/vda



# -----------------------------------------------------
# Check the free space on the worker nodes.
#[user@worker01]

    ls -al /mnt/cinder/vdc

    >   drwxrwsr-x. 1 root root   24 Apr 28 00:52 hadoop
    >   drwxrwsr-x. 1 root root    8 Apr 28 00:53 hdfs


    du -h -d 2 /mnt/cinder/vdc

    >   0       /mnt/cinder/vdc/hadoop/data
    >   34M     /mnt/cinder/vdc/hadoop/logs
    >   1.6G    /mnt/cinder/vdc/hadoop/temp
    >   1.7G    /mnt/cinder/vdc/hadoop
    >   36M     /mnt/cinder/vdc/hdfs/data
    >   36M     /mnt/cinder/vdc/hdfs
    >   1.7G    /mnt/cinder/vdc


    rm -rf /mnt/cinder/vdc/hadoop/logs/*

    rm -rf /mnt/cinder/vdc/hadoop/temp/*

    du -h -d 2 /mnt/cinder/vdc

    >   0       /mnt/cinder/vdc/hadoop/data
    >   0       /mnt/cinder/vdc/hadoop/logs
    >   0       /mnt/cinder/vdc/hadoop/temp
    >   0       /mnt/cinder/vdc/hadoop
    >   36M     /mnt/cinder/vdc/hdfs/data
    >   36M     /mnt/cinder/vdc/hdfs
    >   36M     /mnt/cinder/vdc


# -----------------------------------------------------
# Check the free space on the worker nodes.
#[user@worker02]

    ls -al /mnt/cinder/vdc

    >   drwxrwsr-x. 1 root root   24 Apr 28 00:52 hadoop
    >   drwxrwsr-x. 1 root root    8 Apr 28 00:53 hdfs


    du -h -d 2 /mnt/cinder/vdc

    >   0       /mnt/cinder/vdc/hadoop/data
    >   60M     /mnt/cinder/vdc/hadoop/logs
    >   347G    /mnt/cinder/vdc/hadoop/temp
    >   347G    /mnt/cinder/vdc/hadoop
    >   36M     /mnt/cinder/vdc/hdfs/data
    >   36M     /mnt/cinder/vdc/hdfs
    >   347G    /mnt/cinder/vdc


    rm -rf /mnt/cinder/vdc/hadoop/logs/*

    rm -rf /mnt/cinder/vdc/hadoop/temp/*

    du -h -d 2 /mnt/cinder/vdc

    >   0       /mnt/cinder/vdc/hadoop/data
    >   0       /mnt/cinder/vdc/hadoop/logs
    >   0       /mnt/cinder/vdc/hadoop/temp
    >   0       /mnt/cinder/vdc/hadoop
    >   36M     /mnt/cinder/vdc/hdfs/data
    >   36M     /mnt/cinder/vdc/hdfs
    >   36M     /mnt/cinder/vdc


# -----------------------------------------------------
# Check the free space on the worker nodes.
#[user@worker03]

    ls -al /mnt/cinder/vdc

    >   drwxrwsr-x. 1 root root   24 Apr 28 00:52 hadoop
    >   drwxrwsr-x. 1 root root    8 Apr 28 00:53 hdfs


    du -h -d 2 /mnt/cinder/vdc

    >   0       /mnt/cinder/vdc/hadoop/data
    >   39M     /mnt/cinder/vdc/hadoop/logs
    >   338G    /mnt/cinder/vdc/hadoop/temp
    >   338G    /mnt/cinder/vdc/hadoop
    >   36M     /mnt/cinder/vdc/hdfs/data
    >   36M     /mnt/cinder/vdc/hdfs
    >   338G    /mnt/cinder/vdc


    rm -rf /mnt/cinder/vdc/hadoop/logs/*

    rm -rf /mnt/cinder/vdc/hadoop/temp/*

    du -h -d 2 /mnt/cinder/vdc

    >   0       /mnt/cinder/vdc/hadoop/data
    >   0       /mnt/cinder/vdc/hadoop/logs
    >   0       /mnt/cinder/vdc/hadoop/temp
    >   0       /mnt/cinder/vdc/hadoop
    >   36M     /mnt/cinder/vdc/hdfs/data
    >   36M     /mnt/cinder/vdc/hdfs
    >   36M     /mnt/cinder/vdc


# -----------------------------------------------------
# Check the free space on the worker nodes.
#[user@worker04]

    ls -al /mnt/cinder/vdc

    >   drwxrwsr-x. 1 root root   24 Apr 28 00:52 hadoop
    >   drwxrwsr-x. 1 root root    8 Apr 28 00:53 hdfs


    du -h -d 2 /mnt/cinder/vdc

    >   0       /mnt/cinder/vdc/hadoop/data
    >   47M     /mnt/cinder/vdc/hadoop/logs
    >   351G    /mnt/cinder/vdc/hadoop/temp
    >   351G    /mnt/cinder/vdc/hadoop
    >   34M     /mnt/cinder/vdc/hdfs/data
    >   34M     /mnt/cinder/vdc/hdfs
    >   351G    /mnt/cinder/vdc


    rm -rf /mnt/cinder/vdc/hadoop/logs/*

    rm -rf /mnt/cinder/vdc/hadoop/temp/*

    du -h -d 2 /mnt/cinder/vdc

    >   0       /mnt/cinder/vdc/hadoop/data
    >   0       /mnt/cinder/vdc/hadoop/logs
    >   0       /mnt/cinder/vdc/hadoop/temp
    >   0       /mnt/cinder/vdc/hadoop
    >   34M     /mnt/cinder/vdc/hdfs/data
    >   34M     /mnt/cinder/vdc/hdfs
    >   34M     /mnt/cinder/vdc


# -----------------------------------------------------
# Restart the Yarn daemons
#[user@master01]

    stop-yarn.sh

    >   Stopping nodemanagers
    >   Stopping resourcemanager


    start-yarn.sh

    >   Starting resourcemanager
    >   Starting nodemanagers


# -----------------------------------------------------
# Create the target directory
#[user@master01]

    hadoop fs -mkdir hdfs://master01:9000/spark-log

    hadoop fs -mkdir hdfs://master01:9000/partitioned

    hadoop fs -mkdir hdfs://master01:9000/partitioned/GEDR3-8192


# -----------------------------------------------------
# -----------------------------------------------------

    Restart the interpreter
    Clear the output
    Run the notebook ...

    First cell : Took 0 sec. Last updated by zrq at May 17 2021, 2:46:35 AM.
    Partition cell : Took 4 hrs 26 min 34 sec. Last updated by zrq at May 17 2021, 7:13:52 AM.

    (*) The notebook itself fails in a later cell with a Python error message


#[user@zeppelin]

    >   ....
    >    INFO [2021-05-17 01:49:19,138] ({dispatcher-event-loop-8} Logging.scala[logInfo]:54) - Starting task 157.0 in stage 2.0 (TID 10158, worker01, executor 3, partition 157, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-05-17 01:49:19,139] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 145.0 in stage 2.0 (TID 10146) in 4365 ms on worker01 (executor 3) (146/5720)
    >    INFO [2021-05-17 01:49:20,852] ({dispatcher-event-loop-5} Logging.scala[logInfo]:54) - Starting task 158.0 in stage 2.0 (TID 10159, worker03, executor 1, partition 158, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-05-17 01:49:20,853] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 148.0 in stage 2.0 (TID 10149) in 4806 ms on worker03 (executor 1) (147/5720)
    >    INFO [2021-05-17 01:49:21,371] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 159.0 in stage 2.0 (TID 10160, worker01, executor 3, partition 159, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-05-17 01:49:21,372] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 151.0 in stage 2.0 (TID 10152) in 4119 ms on worker01 (executor 3) (148/5720)
    >    INFO [2021-05-17 01:49:24,931] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 160.0 in stage 2.0 (TID 10161, worker01, executor 3, partition 160, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-05-17 01:49:24,931] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 153.0 in stage 2.0 (TID 10154) in 6953 ms on worker01 (executor 3) (149/5720)
    >    INFO [2021-05-17 01:49:24,931] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 161.0 in stage 2.0 (TID 10162, worker01, executor 3, partition 161, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-05-17 01:49:24,932] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 154.0 in stage 2.0 (TID 10155) in 6104 ms on worker01 (executor 3) (150/5720)
    >    INFO [2021-05-17 01:49:24,932] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 162.0 in stage 2.0 (TID 10163, worker01, executor 3, partition 162, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-05-17 01:49:24,932] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 157.0 in stage 2.0 (TID 10158) in 5794 ms on worker01 (executor 3) (151/5720)
    >    INFO [2021-05-17 01:49:27,969] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Starting task 163.0 in stage 2.0 (TID 10164, worker01, executor 3, partition 163, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-05-17 01:49:27,969] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 159.0 in stage 2.0 (TID 10160) in 6598 ms on worker01 (executor 3) (152/5720)
    >    INFO [2021-05-17 01:49:28,099] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Starting task 164.0 in stage 2.0 (TID 10165, worker04, executor 2, partition 164, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-05-17 01:49:28,099] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 146.0 in stage 2.0 (TID 10147) in 13159 ms on worker04 (executor 2) (153/5720)
    >    INFO [2021-05-17 01:49:28,100] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Starting task 165.0 in stage 2.0 (TID 10166, worker04, executor 2, partition 165, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-05-17 01:49:28,100] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 149.0 in stage 2.0 (TID 10150) in 11238 ms on worker04 (executor 2) (154/5720)
    >    INFO [2021-05-17 01:49:29,542] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 166.0 in stage 2.0 (TID 10167, worker01, executor 3, partition 166, PROCESS_LOCAL, 8450 bytes)
    >    INFO [2021-05-17 01:49:29,542] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 161.0 in stage 2.0 (TID 10162) in 4611 ms on worker01 (executor 3) (155/5720)
    >   ....


#[user@worker03]

    tail -f  userlogs/application_1619571756695_0030/container_1619571756695_0030_01_000002/stderr

    >   ....
    >   2021-05-17 01:48:42,425 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10078
    >   2021-05-17 01:48:42,425 INFO executor.Executor: Running task 77.0 in stage 2.0 (TID 10078)
    >   2021-05-17 01:48:42,440 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-03439-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-50909908, partition values: [empty row]
    >   2021-05-17 01:48:42,447 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-05928-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-50914959, partition values: [empty row]
    >   2021-05-17 01:48:43,459 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-02340-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-50909294, partition values: [empty row]
    >   2021-05-17 01:48:44,665 INFO executor.Executor: Finished task 75.0 in stage 2.0 (TID 10076). 1619 bytes result sent to driver
    >   2021-05-17 01:48:44,667 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10085
    >   2021-05-17 01:48:44,667 INFO executor.Executor: Running task 84.0 in stage 2.0 (TID 10085)
    >   2021-05-17 01:48:44,679 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-05486-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-50889197, partition values: [empty row]
    >   2021-05-17 01:48:44,788 INFO executor.Executor: Finished task 71.0 in stage 2.0 (TID 10072). 1619 bytes result sent to driver
    >   2021-05-17 01:48:44,791 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10086
    >   2021-05-17 01:48:44,791 INFO executor.Executor: Running task 85.0 in stage 2.0 (TID 10086)
    >   2021-05-17 01:48:44,795 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-00581-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-50887917, partition values: [empty row]
    >   2021-05-17 01:48:44,902 INFO executor.Executor: Finished task 73.0 in stage 2.0 (TID 10074). 1619 bytes result sent to driver
    >   2021-05-17 01:48:44,905 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10087
    >   2021-05-17 01:48:44,905 INFO executor.Executor: Running task 86.0 in stage 2.0 (TID 10087)
    >   2021-05-17 01:48:44,909 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-09802-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-50884695, partition values: [empty row]
    >   2021-05-17 01:48:45,730 INFO executor.Executor: Finished task 77.0 in stage 2.0 (TID 10078). 1619 bytes result sent to driver
    >   2021-05-17 01:48:47,808 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-05528-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-50882326, partition values: [empty row]
    >   2021-05-17 01:48:46,763 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-07346-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-50887355, partition values: [empty row]
    >   2021-05-17 01:48:46,700 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-01969-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-50888877, partition values: [empty row]
    >   2021-05-17 01:48:51,235 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10097
    >   2021-05-17 01:48:51,236 INFO executor.Executor: Running task 96.0 in stage 2.0 (TID 10097)
    >   2021-05-17 01:48:51,243 INFO datasources.FileScanRDD: Reading File path: file:/user/nch/PARQUET/TESTS/GEDR3/part-04264-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet, range: 0-50861468, partition values: [empty row]
    >   2021-05-17 01:48:54,318 INFO executor.Executor: Finished task 86.0 in stage 2.0 (TID 10087). 1576 bytes result sent to driver
    >   ....


#[user@master01]

    >   ....
    >   2021-05-17T01:47:41+00:00
    >   7557c70d88f681b044cba5f709b57476  part-00380-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 0.85
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T01:48:02+00:00
    >   6f92c5128448f5068d414d704ba9233e  part-00381-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 0.76
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T01:48:22+00:00
    >   7633d35dea7f48a221b01dc394d2c4b0  part-00382-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 1.45
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T01:48:44+00:00
    >   6e893ab88855ba5d0765a00c6ccafbe9  part-00383-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 4.02
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T01:49:08+00:00
    >   27dde6ef4d1e60ca2be10541405233d6  part-00384-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 4.77
    >   user 0.06
    >   sys 0.01
    >   ----
    >   2021-05-17T01:49:33+00:00
    >   10066b4ec47034d3532380c1ab0c52f3  part-00385-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 3.66
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T01:49:56+00:00
    >   d8539b2da93f8d7f9a0557f8c8a4d7b1  part-00386-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 6.53
    >   user 0.07
    >   sys 0.01
    >   ....


#[user@worker03]

    >   ....
    >   2021-05-17T01:50:00+00:00
    >   162acff7055b5ef27c3c683965ac0abb  part-01694-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 5.26
    >   user 0.06
    >   sys 0.02
    >   ----
    >   2021-05-17T01:50:25+00:00
    >   96417cffe1051ac8e5dcb16b75e15d1d  part-01695-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 2.96
    >   user 0.06
    >   sys 0.01
    >   ----
    >   2021-05-17T01:50:48+00:00
    >   a36d20f31c96a150610c9f0ffdfdfb97  part-01696-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 5.19
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T01:51:13+00:00
    >   ca1382d119b7f3799fd4c8bcf26f3141  part-01697-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 3.60
    >   user 0.06
    >   sys 0.02
    >   ----
    >   2021-05-17T01:51:37+00:00
    >   9b0bb11bb26cd9c61b59c319a2d5d0fa  part-01698-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 2.86
    >   user 0.07
    >   sys 0.01
    >   ----
    >   2021-05-17T01:52:00+00:00
    >   bb8f187369b509ce32039cdce08caf70  part-01699-59b9273a-2ef1-4988-8778-e00f67e65264-c000.snappy.parquet
    >   real 3.15
    >   user 0.07
    >   sys 0.02
    >   ....


#[user@worker04]

    hostname ; date --iso-8601=s ; du -h -d 2 /mnt/cinder/vdc

    >   0       /mnt/cinder/vdc/hadoop/data
    >   1.4M    /mnt/cinder/vdc/hadoop/logs
    >   32G     /mnt/cinder/vdc/hadoop/temp
    >   32G     /mnt/cinder/vdc/hadoop
    >   37M     /mnt/cinder/vdc/hdfs/data
    >   37M     /mnt/cinder/vdc/hdfs
    >   32G     /mnt/cinder/vdc

    >   gaia-prod-20210428-worker04.novalocal
    >   2021-05-17T02:11:11+00:00
    >   0       /mnt/cinder/vdc/hadoop/data
    >   1.7M    /mnt/cinder/vdc/hadoop/logs
    >   96G     /mnt/cinder/vdc/hadoop/temp
    >   96G     /mnt/cinder/vdc/hadoop
    >   37M     /mnt/cinder/vdc/hdfs/data
    >   37M     /mnt/cinder/vdc/hdfs
    >   96G     /mnt/cinder/vdc


    >   gaia-prod-20210428-worker04.novalocal
    >   2021-05-17T02:20:25+00:00
    >   0       /mnt/cinder/vdc/hadoop/data
    >   1.9M    /mnt/cinder/vdc/hadoop/logs
    >   133G    /mnt/cinder/vdc/hadoop/temp
    >   133G    /mnt/cinder/vdc/hadoop
    >   37M     /mnt/cinder/vdc/hdfs/data
    >   37M     /mnt/cinder/vdc/hdfs
    >   133G    /mnt/cinder/vdc

    >   gaia-prod-20210428-worker04.novalocal
    >   2021-05-17T09:46:38+00:00
    >   0       /mnt/cinder/vdc/hadoop/data
    >   51M     /mnt/cinder/vdc/hadoop/logs
    >   519M    /mnt/cinder/vdc/hadoop/temp
    >   570M    /mnt/cinder/vdc/hadoop
    >   438G    /mnt/cinder/vdc/hdfs/data
    >   438G    /mnt/cinder/vdc/hdfs
    >   439G    /mnt/cinder/vdc



