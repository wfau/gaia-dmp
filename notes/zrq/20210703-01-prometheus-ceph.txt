#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#



#[user@monitor]

    curl http://zeppelin:9128/metrics
    
    >   # HELP ceph_active_pgs No. of active PGs in the cluster
    >   # TYPE ceph_active_pgs gauge
    >   ceph_active_pgs{cluster="ceph"} 2808
    >   # HELP ceph_backfill_wait_pgs No. of PGs in the cluster with backfill_wait state
    >   # TYPE ceph_backfill_wait_pgs gauge
    >   ceph_backfill_wait_pgs{cluster="ceph"} 0
    >   # HELP ceph_backfilling_pgs No. of backfilling PGs in the cluster
    >   # TYPE ceph_backfilling_pgs gauge
    >   ceph_backfilling_pgs{cluster="ceph"} 0
    >   # HELP ceph_cache_evict_io_bytes Rate of bytes being evicted from the cache pool per second
    >   # TYPE ceph_cache_evict_io_bytes gauge
    >   ceph_cache_evict_io_bytes{cluster="ceph"} 0
    >   # HELP ceph_cache_flush_io_bytes Rate of bytes being flushed from the cache pool per second
    >   # TYPE ceph_cache_flush_io_bytes gauge
    >   ceph_cache_flush_io_bytes{cluster="ceph"} 0
    >   # HELP ceph_cache_promote_io_ops Total cache promote operations measured per second
    >   # TYPE ceph_cache_promote_io_ops gauge
    >   ceph_cache_promote_io_ops{cluster="ceph"} 0
    >   # HELP ceph_client_io_ops Total client ops on the cluster measured per second
    >   # TYPE ceph_client_io_ops gauge
    >   ceph_client_io_ops{cluster="ceph"} 1779
    >   # HELP ceph_client_io_read_bytes Rate of bytes being read by all clients per second
    >   # TYPE ceph_client_io_read_bytes gauge
    >   ceph_client_io_read_bytes{cluster="ceph"} 9.12130309e+08
    >   # HELP ceph_client_io_read_ops Total client read I/O ops on the cluster measured per second
    >   # TYPE ceph_client_io_read_ops gauge
    >   ceph_client_io_read_ops{cluster="ceph"} 1773
    >   # HELP ceph_client_io_write_bytes Rate of bytes being written by all clients per second
    >   # TYPE ceph_client_io_write_bytes gauge
    >   ceph_client_io_write_bytes{cluster="ceph"} 330881
    >   # HELP ceph_client_io_write_ops Total client write I/O ops on the cluster measured per second
    >   # TYPE ceph_client_io_write_ops gauge
    >   ceph_client_io_write_ops{cluster="ceph"} 6
    >   # HELP ceph_cluster_available_bytes Available space within the cluster
    >   # TYPE ceph_cluster_available_bytes gauge
    >   ceph_cluster_available_bytes{cluster="ceph"} 6.9584396812288e+13
    >   # HELP ceph_cluster_capacity_bytes Total capacity of the cluster
    >   # TYPE ceph_cluster_capacity_bytes gauge
    >   ceph_cluster_capacity_bytes{cluster="ceph"} 1.28896934371328e+14
    >   # HELP ceph_cluster_objects No. of rados objects within the cluster
    >   # TYPE ceph_cluster_objects gauge
    >   ceph_cluster_objects{cluster="ceph"} 4.996021e+06
    >   # HELP ceph_cluster_used_bytes Capacity of the cluster currently in use
    >   # TYPE ceph_cluster_used_bytes gauge
    >   ceph_cluster_used_bytes{cluster="ceph"} 5.931253755904e+13
    >   # HELP ceph_deep_scrubbing_pgs No. of deep scrubbing PGs in the cluster
    >   # TYPE ceph_deep_scrubbing_pgs gauge
    >   ceph_deep_scrubbing_pgs{cluster="ceph"} 0
    >   # HELP ceph_degraded_objects No. of degraded objects across all PGs, includes replicas
    >   # TYPE ceph_degraded_objects gauge
    >   ceph_degraded_objects{cluster="ceph"} 0
    >   # HELP ceph_degraded_pgs No. of PGs in a degraded state
    >   # TYPE ceph_degraded_pgs gauge
    >   ceph_degraded_pgs{cluster="ceph"} 0
    >   # HELP ceph_down_pgs No. of PGs in the cluster in down state
    >   # TYPE ceph_down_pgs gauge
    >   ceph_down_pgs{cluster="ceph"} 0
    >   # HELP ceph_forced_backfill_pgs No. of PGs in the cluster with forced_backfill state
    >   # TYPE ceph_forced_backfill_pgs gauge
    >   ceph_forced_backfill_pgs{cluster="ceph"} 0
    >   # HELP ceph_forced_recovery_pgs No. of PGs in the cluster with forced_recovery state
    >   # TYPE ceph_forced_recovery_pgs gauge
    >   ceph_forced_recovery_pgs{cluster="ceph"} 0
    >   # HELP ceph_health_status Health status of Cluster, can vary only between 3 states (err:2, warn:1, ok:0)
    >   # TYPE ceph_health_status gauge
    >   ceph_health_status{cluster="ceph"} 1
    >   # HELP ceph_health_status_interp Health status of Cluster, can vary only between 4 states (err:3, critical_warn:2, soft_warn:1, ok:0)
    >   # TYPE ceph_health_status_interp gauge
    >   ceph_health_status_interp{cluster="ceph"} 1
    >   # HELP ceph_misplaced_objects No. of misplaced objects across all PGs, includes replicas
    >   # TYPE ceph_misplaced_objects gauge
    >   ceph_misplaced_objects{cluster="ceph"} 0
    >   # HELP ceph_monitor_clock_skew_seconds Clock skew the monitor node is incurring
    >   # TYPE ceph_monitor_clock_skew_seconds gauge
    >   ceph_monitor_clock_skew_seconds{cluster="ceph",monitor="ceph01"} 0
    >   ceph_monitor_clock_skew_seconds{cluster="ceph",monitor="ceph02"} -3.5e-05
    >   ceph_monitor_clock_skew_seconds{cluster="ceph",monitor="ceph03"} -6.9e-05
    >   # HELP ceph_monitor_latency_seconds Latency the monitor node is incurring
    >   # TYPE ceph_monitor_latency_seconds gauge
    >   ceph_monitor_latency_seconds{cluster="ceph",monitor="ceph01"} 0
    >   ceph_monitor_latency_seconds{cluster="ceph",monitor="ceph02"} 0.000429
    >   ceph_monitor_latency_seconds{cluster="ceph",monitor="ceph03"} 0.0004
    >   # HELP ceph_monitor_quorum_count The total size of the monitor quorum
    >   # TYPE ceph_monitor_quorum_count gauge
    >   ceph_monitor_quorum_count{cluster="ceph"} 3
    >   # HELP ceph_new_crash_reports Number of new crash reports available
    >   # TYPE ceph_new_crash_reports gauge
    >   ceph_new_crash_reports{cluster="ceph"} 0
    >   # HELP ceph_osd_avail_bytes OSD Available Storage in Bytes
    >   # TYPE ceph_osd_avail_bytes gauge
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.0",rack="",root="default"} 2.059001856e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.12",rack="",root="default"} 1.907787584e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.15",rack="",root="default"} 1.666744384e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.24",rack="",root="default"} 2.338995136e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.27",rack="",root="default"} 0
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.30",rack="",root="default"} 1.604770304e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.33",rack="",root="default"} 2.08141952e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.36",rack="",root="default"} 2.01862496e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.5",rack="",root="default"} 2.055887616e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.7",rack="",root="default"} 2.219838848e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.9",rack="",root="default"} 2.094337728e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.1",rack="",root="default"} 0
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.11",rack="",root="default"} 1.4851232e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.13",rack="",root="default"} 2.106715456e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.16",rack="",root="default"} 0
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.20",rack="",root="default"} 1.83999712e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.23",rack="",root="default"} 2.313881472e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.26",rack="",root="default"} 2.07080832e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.3",rack="",root="default"} 2.179662592e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.32",rack="",root="default"} 2.241694464e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.35",rack="",root="default"} 1.934603584e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.38",rack="",root="default"} 1.824044672e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.8",rack="",root="default"} 2.05093664e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.10",rack="",root="default"} 2.45487072e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.14",rack="",root="default"} 2.172613056e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.19",rack="",root="default"} 2.44369312e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.2",rack="",root="default"} 1.8640208e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.22",rack="",root="default"} 2.134313664e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.25",rack="",root="default"} 2.500445248e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.28",rack="",root="default"} 2.291012288e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.31",rack="",root="default"} 2.545393472e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.34",rack="",root="default"} 2.49405504e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.37",rack="",root="default"} 2.22787424e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.4",rack="",root="default"} 2.249456448e+12
    >   ceph_osd_avail_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.6",rack="",root="default"} 2.48088896e+12
    >   # HELP ceph_osd_average_utilization OSD Average Utilization
    >   # TYPE ceph_osd_average_utilization gauge
    >   ceph_osd_average_utilization{cluster="ceph"} 46.015476
    >   # HELP ceph_osd_backfill_full OSD Backfill Full Status
    >   # TYPE ceph_osd_backfill_full gauge
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.0",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.12",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.15",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.24",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.27",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.30",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.33",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.36",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.5",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.7",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.9",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.1",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.11",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.13",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.16",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.20",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.23",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.26",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.3",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.32",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.35",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.38",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.8",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.10",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.14",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.19",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.2",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.22",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.25",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.28",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.31",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.34",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.37",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.4",rack="",root="default"} 0
    >   ceph_osd_backfill_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.6",rack="",root="default"} 0
    >   # HELP ceph_osd_bytes OSD Total Bytes
    >   # TYPE ceph_osd_bytes gauge
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.0",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.12",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.15",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.24",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.27",rack="",root="default"} 0
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.30",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.33",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.36",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.5",rack="",root="default"} 3.907014656e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.7",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.9",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.1",rack="",root="default"} 0
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.11",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.13",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.16",rack="",root="default"} 0
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.20",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.23",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.26",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.3",rack="",root="default"} 3.907014656e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.32",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.35",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.38",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.8",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.10",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.14",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.19",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.2",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.22",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.25",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.28",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.31",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.34",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.37",rack="",root="default"} 3.936374776e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.4",rack="",root="default"} 3.907014656e+12
    >   ceph_osd_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.6",rack="",root="default"} 3.936374776e+12
    >   # HELP ceph_osd_crush_weight OSD Crush Weight
    >   # TYPE ceph_osd_crush_weight gauge
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.0",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.12",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.15",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.24",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.27",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.30",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.33",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.36",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.5",rack="",root="default"} 3.638687
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.7",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.9",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.1",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.11",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.13",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.16",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.20",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.23",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.26",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.3",rack="",root="default"} 3.638687
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.32",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.35",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.38",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.8",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.10",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.14",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.19",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.2",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.22",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.25",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.28",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.31",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.34",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.37",rack="",root="default"} 3.665985
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.4",rack="",root="default"} 3.638687
    >   ceph_osd_crush_weight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.6",rack="",root="default"} 3.665985
    >   # HELP ceph_osd_depth OSD Depth
    >   # TYPE ceph_osd_depth gauge
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.0",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.12",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.15",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.24",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.27",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.30",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.33",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.36",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.5",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.7",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.9",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.1",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.11",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.13",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.16",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.20",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.23",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.26",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.3",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.32",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.35",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.38",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.8",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.10",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.14",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.19",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.2",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.22",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.25",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.28",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.31",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.34",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.37",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.4",rack="",root="default"} 2
    >   ceph_osd_depth{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.6",rack="",root="default"} 2
    >   # HELP ceph_osd_down Number of OSDs down in the cluster
    >   # TYPE ceph_osd_down gauge
    >   ceph_osd_down{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.27",rack="default",root="",status="down"} 1
    >   ceph_osd_down{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.1",rack="default",root="",status="down"} 1
    >   ceph_osd_down{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.16",rack="default",root="",status="down"} 1
    >   # HELP ceph_osd_full OSD Full Status
    >   # TYPE ceph_osd_full gauge
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.0",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.12",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.15",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.24",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.27",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.30",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.33",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.36",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.5",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.7",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.9",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.1",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.11",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.13",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.16",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.20",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.23",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.26",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.3",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.32",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.35",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.38",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.8",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.10",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.14",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.19",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.2",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.22",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.25",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.28",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.31",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.34",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.37",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.4",rack="",root="default"} 0
    >   ceph_osd_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.6",rack="",root="default"} 0
    >   # HELP ceph_osd_in OSD In Status
    >   # TYPE ceph_osd_in gauge
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.0",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.12",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.15",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.24",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.27",rack="",root="default"} 0
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.30",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.33",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.36",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.5",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.7",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.9",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.1",rack="",root="default"} 0
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.11",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.13",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.16",rack="",root="default"} 0
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.20",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.23",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.26",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.3",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.32",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.35",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.38",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.8",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.10",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.14",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.19",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.2",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.22",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.25",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.28",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.31",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.34",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.37",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.4",rack="",root="default"} 1
    >   ceph_osd_in{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.6",rack="",root="default"} 1
    >   # HELP ceph_osd_near_full OSD Near Full Status
    >   # TYPE ceph_osd_near_full gauge
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.0",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.12",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.15",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.24",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.27",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.30",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.33",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.36",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.5",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.7",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.9",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.1",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.11",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.13",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.16",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.20",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.23",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.26",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.3",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.32",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.35",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.38",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.8",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.10",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.14",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.19",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.2",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.22",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.25",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.28",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.31",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.34",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.37",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.4",rack="",root="default"} 0
    >   ceph_osd_near_full{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.6",rack="",root="default"} 0
    >   # HELP ceph_osd_pgs OSD Placement Group Count
    >   # TYPE ceph_osd_pgs gauge
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.0",rack="",root="default"} 276
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.12",rack="",root="default"} 297
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.15",rack="",root="default"} 304
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.24",rack="",root="default"} 239
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.27",rack="",root="default"} 0
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.30",rack="",root="default"} 295
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.33",rack="",root="default"} 290
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.36",rack="",root="default"} 286
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.5",rack="",root="default"} 276
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.7",rack="",root="default"} 282
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.9",rack="",root="default"} 263
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.1",rack="",root="default"} 0
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.11",rack="",root="default"} 279
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.13",rack="",root="default"} 295
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.16",rack="",root="default"} 0
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.20",rack="",root="default"} 289
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.23",rack="",root="default"} 256
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.26",rack="",root="default"} 287
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.3",rack="",root="default"} 266
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.32",rack="",root="default"} 312
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.35",rack="",root="default"} 273
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.38",rack="",root="default"} 276
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.8",rack="",root="default"} 275
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.10",rack="",root="default"} 229
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.14",rack="",root="default"} 222
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.19",rack="",root="default"} 250
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.2",rack="",root="default"} 241
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.22",rack="",root="default"} 255
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.25",rack="",root="default"} 246
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.28",rack="",root="default"} 230
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.31",rack="",root="default"} 214
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.34",rack="",root="default"} 241
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.37",rack="",root="default"} 234
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.4",rack="",root="default"} 237
    >   ceph_osd_pgs{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.6",rack="",root="default"} 209
    >   # HELP ceph_osd_reweight OSD Reweight
    >   # TYPE ceph_osd_reweight gauge
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.0",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.12",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.15",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.24",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.27",rack="",root="default"} 0
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.30",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.33",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.36",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.5",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.7",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.9",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.1",rack="",root="default"} 0
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.11",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.13",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.16",rack="",root="default"} 0
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.20",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.23",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.26",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.3",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.32",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.35",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.38",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.8",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.10",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.14",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.19",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.2",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.22",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.25",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.28",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.31",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.34",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.37",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.4",rack="",root="default"} 1
    >   ceph_osd_reweight{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.6",rack="",root="default"} 1
    >   # HELP ceph_osd_total_avail_bytes OSD Total Available Storage Bytes 
    >   # TYPE ceph_osd_total_avail_bytes gauge
    >   ceph_osd_total_avail_bytes{cluster="ceph"} 6.7953512512e+13
    >   # HELP ceph_osd_total_bytes OSD Total Storage Bytes
    >   # TYPE ceph_osd_total_bytes gauge
    >   ceph_osd_total_bytes{cluster="ceph"} 1.25875912472e+14
    >   # HELP ceph_osd_total_used_bytes OSD Total Used Storage Bytes
    >   # TYPE ceph_osd_total_used_bytes gauge
    >   ceph_osd_total_used_bytes{cluster="ceph"} 5.792239996e+13
    >   # HELP ceph_osd_up OSD Up Status
    >   # TYPE ceph_osd_up gauge
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.0",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.12",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.15",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.24",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.27",rack="",root="default"} 0
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.30",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.33",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.36",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.5",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.7",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.9",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.1",rack="",root="default"} 0
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.11",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.13",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.16",rack="",root="default"} 0
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.20",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.23",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.26",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.3",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.32",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.35",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.38",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.8",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.10",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.14",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.19",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.2",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.22",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.25",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.28",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.31",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.34",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.37",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.4",rack="",root="default"} 1
    >   ceph_osd_up{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.6",rack="",root="default"} 1
    >   # HELP ceph_osd_used_bytes OSD Used Storage in Bytes
    >   # TYPE ceph_osd_used_bytes gauge
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.0",rack="",root="default"} 1.87737292e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.12",rack="",root="default"} 2.028587192e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.15",rack="",root="default"} 2.269630392e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.24",rack="",root="default"} 1.59737964e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.27",rack="",root="default"} 0
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.30",rack="",root="default"} 2.331604472e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.33",rack="",root="default"} 1.854955256e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.36",rack="",root="default"} 1.917749816e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.5",rack="",root="default"} 1.85112704e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.7",rack="",root="default"} 1.716535928e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.9",rack="",root="default"} 1.842037048e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.1",rack="",root="default"} 0
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.11",rack="",root="default"} 2.451251576e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.13",rack="",root="default"} 1.82965932e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.16",rack="",root="default"} 0
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.20",rack="",root="default"} 2.096377656e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.23",rack="",root="default"} 1.622493304e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.26",rack="",root="default"} 1.865566456e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.3",rack="",root="default"} 1.727352064e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.32",rack="",root="default"} 1.694680312e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.35",rack="",root="default"} 2.001771192e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.38",rack="",root="default"} 2.112330104e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.8",rack="",root="default"} 1.885438136e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.10",rack="",root="default"} 1.481504056e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.14",rack="",root="default"} 1.76376172e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.19",rack="",root="default"} 1.492681656e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.2",rack="",root="default"} 2.072353976e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.22",rack="",root="default"} 1.802061112e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.25",rack="",root="default"} 1.435929528e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.28",rack="",root="default"} 1.645362488e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.31",rack="",root="default"} 1.390981304e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.34",rack="",root="default"} 1.442319736e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.37",rack="",root="default"} 1.708500536e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.4",rack="",root="default"} 1.657558208e+12
    >   ceph_osd_used_bytes{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.6",rack="",root="default"} 1.455485816e+12
    >   # HELP ceph_osd_utilization OSD Utilization
    >   # TYPE ceph_osd_utilization gauge
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.0",rack="",root="default"} 47.692942
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.12",rack="",root="default"} 51.534402
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.15",rack="",root="default"} 57.657884
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.24",rack="",root="default"} 40.579968
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.27",rack="",root="default"} 0
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.30",rack="",root="default"} 59.232279
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.33",rack="",root="default"} 47.123441
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.36",rack="",root="default"} 48.71868
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.5",rack="",root="default"} 47.379578
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.7",rack="",root="default"} 43.607025
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.9",rack="",root="default"} 46.795266
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.1",rack="",root="default"} 0
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.11",rack="",root="default"} 62.271804
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.13",rack="",root="default"} 46.480821
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.16",rack="",root="default"} 0
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.20",rack="",root="default"} 53.256556
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.23",rack="",root="default"} 41.217958
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.26",rack="",root="default"} 47.393009
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.3",rack="",root="default"} 44.211558
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.32",rack="",root="default"} 43.051803
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.35",rack="",root="default"} 50.853166
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.38",rack="",root="default"} 53.661814
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.8",rack="",root="default"} 47.897831
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.10",rack="",root="default"} 37.636255
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.14",rack="",root="default"} 44.806753
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.19",rack="",root="default"} 37.920212
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.2",rack="",root="default"} 52.646257
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.22",rack="",root="default"} 45.779714
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.25",rack="",root="default"} 36.478476
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.28",rack="",root="default"} 41.798929
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.31",rack="",root="default"} 35.336607
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.34",rack="",root="default"} 36.640813
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.37",rack="",root="default"} 43.402893
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.4",rack="",root="default"} 42.425185
    >   ceph_osd_utilization{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.6",rack="",root="default"} 36.975286
    >   # HELP ceph_osd_variance OSD Variance
    >   # TYPE ceph_osd_variance gauge
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.0",rack="",root="default"} 1.036454
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.12",rack="",root="default"} 1.119936
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.15",rack="",root="default"} 1.253011
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.24",rack="",root="default"} 0.881877
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.27",rack="",root="default"} 0
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.30",rack="",root="default"} 1.287225
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.33",rack="",root="default"} 1.024078
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.36",rack="",root="default"} 1.058746
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.5",rack="",root="default"} 1.029644
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.7",rack="",root="default"} 0.94766
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph01",osd="osd.9",rack="",root="default"} 1.016946
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.1",rack="",root="default"} 0
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.11",rack="",root="default"} 1.35328
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.13",rack="",root="default"} 1.010113
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.16",rack="",root="default"} 0
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.20",rack="",root="default"} 1.157362
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.23",rack="",root="default"} 0.895741
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.26",rack="",root="default"} 1.029936
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.3",rack="",root="default"} 0.960798
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.32",rack="",root="default"} 0.935594
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.35",rack="",root="default"} 1.105132
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.38",rack="",root="default"} 1.166169
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph02",osd="osd.8",rack="",root="default"} 1.040907
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.10",rack="",root="default"} 0.817904
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.14",rack="",root="default"} 0.973732
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.19",rack="",root="default"} 0.824075
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.2",rack="",root="default"} 1.144099
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.22",rack="",root="default"} 0.994876
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.25",rack="",root="default"} 0.792744
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.28",rack="",root="default"} 0.908367
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.31",rack="",root="default"} 0.767929
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.34",rack="",root="default"} 0.796272
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.37",rack="",root="default"} 0.943224
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.4",rack="",root="default"} 0.921976
    >   ceph_osd_variance{cluster="ceph",device_class="hdd",host="ceph03",osd="osd.6",rack="",root="default"} 0.80354
    >   # HELP ceph_osdmap_flag_full The cluster is flagged as full and cannot service writes
    >   # TYPE ceph_osdmap_flag_full gauge
    >   ceph_osdmap_flag_full{cluster="ceph"} 0
    >   # HELP ceph_osdmap_flag_nobackfill OSDs will not be backfilled
    >   # TYPE ceph_osdmap_flag_nobackfill gauge
    >   ceph_osdmap_flag_nobackfill{cluster="ceph"} 0
    >   # HELP ceph_osdmap_flag_nodeep_scrub Deep scrubbing is disabled
    >   # TYPE ceph_osdmap_flag_nodeep_scrub gauge
    >   ceph_osdmap_flag_nodeep_scrub{cluster="ceph"} 0
    >   # HELP ceph_osdmap_flag_nodown OSD failure reports are ignored, OSDs will not be marked as down
    >   # TYPE ceph_osdmap_flag_nodown gauge
    >   ceph_osdmap_flag_nodown{cluster="ceph"} 0
    >   # HELP ceph_osdmap_flag_noin OSDs that are out will not be automatically marked in
    >   # TYPE ceph_osdmap_flag_noin gauge
    >   ceph_osdmap_flag_noin{cluster="ceph"} 0
    >   # HELP ceph_osdmap_flag_noout OSDs will not be automatically marked out after the configured interval
    >   # TYPE ceph_osdmap_flag_noout gauge
    >   ceph_osdmap_flag_noout{cluster="ceph"} 0
    >   # HELP ceph_osdmap_flag_norebalance Data rebalancing is suspended
    >   # TYPE ceph_osdmap_flag_norebalance gauge
    >   ceph_osdmap_flag_norebalance{cluster="ceph"} 0
    >   # HELP ceph_osdmap_flag_norecover Recovery is suspended
    >   # TYPE ceph_osdmap_flag_norecover gauge
    >   ceph_osdmap_flag_norecover{cluster="ceph"} 0
    >   # HELP ceph_osdmap_flag_noscrub Scrubbing is disabled
    >   # TYPE ceph_osdmap_flag_noscrub gauge
    >   ceph_osdmap_flag_noscrub{cluster="ceph"} 0
    >   # HELP ceph_osdmap_flag_notieragent Cache tiering activity is suspended
    >   # TYPE ceph_osdmap_flag_notieragent gauge
    >   ceph_osdmap_flag_notieragent{cluster="ceph"} 0
    >   # HELP ceph_osdmap_flag_noup OSDs are not allowed to start
    >   # TYPE ceph_osdmap_flag_noup gauge
    >   ceph_osdmap_flag_noup{cluster="ceph"} 0
    >   # HELP ceph_osdmap_flag_pauserd Reads are paused
    >   # TYPE ceph_osdmap_flag_pauserd gauge
    >   ceph_osdmap_flag_pauserd{cluster="ceph"} 0
    >   # HELP ceph_osdmap_flag_pausewr Writes are paused
    >   # TYPE ceph_osdmap_flag_pausewr gauge
    >   ceph_osdmap_flag_pausewr{cluster="ceph"} 0
    >   # HELP ceph_osds Count of total OSDs in the cluster
    >   # TYPE ceph_osds gauge
    >   ceph_osds{cluster="ceph"} 35
    >   # HELP ceph_osds_down Count of OSDs that are in DOWN state
    >   # TYPE ceph_osds_down gauge
    >   ceph_osds_down{cluster="ceph"} 3
    >   # HELP ceph_osds_in Count of OSDs that are in IN state and available to serve requests
    >   # TYPE ceph_osds_in gauge
    >   ceph_osds_in{cluster="ceph"} 32
    >   # HELP ceph_osds_up Count of OSDs that are in UP state
    >   # TYPE ceph_osds_up gauge
    >   ceph_osds_up{cluster="ceph"} 32
    >   # HELP ceph_peering_pgs No. of peering PGs in the cluster
    >   # TYPE ceph_peering_pgs gauge
    >   ceph_peering_pgs{cluster="ceph"} 0
    >   # HELP ceph_pg_state State of PGs in the cluster
    >   # TYPE ceph_pg_state gauge
    >   ceph_pg_state{cluster="ceph",state="active"} 2808
    >   ceph_pg_state{cluster="ceph",state="backfill_wait"} 0
    >   ceph_pg_state{cluster="ceph",state="backfilling"} 0
    >   ceph_pg_state{cluster="ceph",state="deep_scrubbing"} 0
    >   ceph_pg_state{cluster="ceph",state="degraded"} 0
    >   ceph_pg_state{cluster="ceph",state="down"} 0
    >   ceph_pg_state{cluster="ceph",state="forced_backfill"} 0
    >   ceph_pg_state{cluster="ceph",state="forced_recovery"} 0
    >   ceph_pg_state{cluster="ceph",state="peering"} 0
    >   ceph_pg_state{cluster="ceph",state="recovering"} 0
    >   ceph_pg_state{cluster="ceph",state="recovery_wait"} 0
    >   ceph_pg_state{cluster="ceph",state="scrubbing"} 2
    >   ceph_pg_state{cluster="ceph",state="stale"} 0
    >   ceph_pg_state{cluster="ceph",state="unclean"} 0
    >   ceph_pg_state{cluster="ceph",state="undersized"} 0
    >   # HELP ceph_pgs_remapped No. of PGs that are remapped and incurring cluster-wide movement
    >   # TYPE ceph_pgs_remapped gauge
    >   ceph_pgs_remapped{cluster="ceph"} 0
    >   # HELP ceph_pool_available_bytes Free space for this ceph pool
    >   # TYPE ceph_pool_available_bytes gauge
    >   ceph_pool_available_bytes{cluster="ceph",pool=".rgw.root"} 1.538112094208e+13
    >   ceph_pool_available_bytes{cluster="ceph",pool="backups"} 1.538112094208e+13
    >   ceph_pool_available_bytes{cluster="ceph",pool="cephfs_data"} 1.538112094208e+13
    >   ceph_pool_available_bytes{cluster="ceph",pool="cephfs_metadata"} 1.538112094208e+13
    >   ceph_pool_available_bytes{cluster="ceph",pool="default.rgw.buckets.data"} 1.538112094208e+13
    >   ceph_pool_available_bytes{cluster="ceph",pool="default.rgw.buckets.index"} 1.538112094208e+13
    >   ceph_pool_available_bytes{cluster="ceph",pool="default.rgw.buckets.non-ec"} 1.538112094208e+13
    >   ceph_pool_available_bytes{cluster="ceph",pool="default.rgw.control"} 1.538112094208e+13
    >   ceph_pool_available_bytes{cluster="ceph",pool="default.rgw.log"} 1.538112094208e+13
    >   ceph_pool_available_bytes{cluster="ceph",pool="default.rgw.meta"} 1.538112094208e+13
    >   ceph_pool_available_bytes{cluster="ceph",pool="images"} 1.538112094208e+13
    >   ceph_pool_available_bytes{cluster="ceph",pool="manila_data"} 1.538112094208e+13
    >   ceph_pool_available_bytes{cluster="ceph",pool="manila_metadata"} 1.538112094208e+13
    >   ceph_pool_available_bytes{cluster="ceph",pool="vms"} 1.538112094208e+13
    >   ceph_pool_available_bytes{cluster="ceph",pool="volumes"} 1.538112094208e+13
    >   # HELP ceph_pool_dirty_objects_total Total no. of dirty objects in a cache-tier pool
    >   # TYPE ceph_pool_dirty_objects_total gauge
    >   ceph_pool_dirty_objects_total{cluster="ceph",pool=".rgw.root"} 4
    >   ceph_pool_dirty_objects_total{cluster="ceph",pool="backups"} 2
    >   ceph_pool_dirty_objects_total{cluster="ceph",pool="cephfs_data"} 3.173553e+06
    >   ceph_pool_dirty_objects_total{cluster="ceph",pool="cephfs_metadata"} 2903
    >   ceph_pool_dirty_objects_total{cluster="ceph",pool="default.rgw.buckets.data"} 350710
    >   ceph_pool_dirty_objects_total{cluster="ceph",pool="default.rgw.buckets.index"} 19
    >   ceph_pool_dirty_objects_total{cluster="ceph",pool="default.rgw.buckets.non-ec"} 0
    >   ceph_pool_dirty_objects_total{cluster="ceph",pool="default.rgw.control"} 8
    >   ceph_pool_dirty_objects_total{cluster="ceph",pool="default.rgw.log"} 207
    >   ceph_pool_dirty_objects_total{cluster="ceph",pool="default.rgw.meta"} 51
    >   ceph_pool_dirty_objects_total{cluster="ceph",pool="images"} 0
    >   ceph_pool_dirty_objects_total{cluster="ceph",pool="manila_data"} 0
    >   ceph_pool_dirty_objects_total{cluster="ceph",pool="manila_metadata"} 0
    >   ceph_pool_dirty_objects_total{cluster="ceph",pool="vms"} 0
    >   ceph_pool_dirty_objects_total{cluster="ceph",pool="volumes"} 1.468564e+06
    >   # HELP ceph_pool_expansion_factor Data expansion multiplier for a pool
    >   # TYPE ceph_pool_expansion_factor gauge
    >   ceph_pool_expansion_factor{cluster="ceph",pool=".rgw.root",profile="replicated"} -1
    >   ceph_pool_expansion_factor{cluster="ceph",pool="backups",profile="replicated"} -1
    >   ceph_pool_expansion_factor{cluster="ceph",pool="cephfs_data",profile="replicated"} -1
    >   ceph_pool_expansion_factor{cluster="ceph",pool="cephfs_metadata",profile="replicated"} -1
    >   ceph_pool_expansion_factor{cluster="ceph",pool="default.rgw.buckets.data",profile="replicated"} -1
    >   ceph_pool_expansion_factor{cluster="ceph",pool="default.rgw.buckets.index",profile="replicated"} -1
    >   ceph_pool_expansion_factor{cluster="ceph",pool="default.rgw.buckets.non-ec",profile="replicated"} -1
    >   ceph_pool_expansion_factor{cluster="ceph",pool="default.rgw.control",profile="replicated"} -1
    >   ceph_pool_expansion_factor{cluster="ceph",pool="default.rgw.log",profile="replicated"} -1
    >   ceph_pool_expansion_factor{cluster="ceph",pool="default.rgw.meta",profile="replicated"} -1
    >   ceph_pool_expansion_factor{cluster="ceph",pool="images",profile="replicated"} -1
    >   ceph_pool_expansion_factor{cluster="ceph",pool="manila_data",profile="replicated"} -1
    >   ceph_pool_expansion_factor{cluster="ceph",pool="manila_metadata",profile="replicated"} -1
    >   ceph_pool_expansion_factor{cluster="ceph",pool="vms",profile="replicated"} -1
    >   ceph_pool_expansion_factor{cluster="ceph",pool="volumes",profile="replicated"} -1
    >   # HELP ceph_pool_min_size Minimum number of copies or chunks of an object that need to be present for active I/O
    >   # TYPE ceph_pool_min_size gauge
    >   ceph_pool_min_size{cluster="ceph",pool=".rgw.root",profile="replicated"} 2
    >   ceph_pool_min_size{cluster="ceph",pool="backups",profile="replicated"} 2
    >   ceph_pool_min_size{cluster="ceph",pool="cephfs_data",profile="replicated"} 2
    >   ceph_pool_min_size{cluster="ceph",pool="cephfs_metadata",profile="replicated"} 2
    >   ceph_pool_min_size{cluster="ceph",pool="default.rgw.buckets.data",profile="replicated"} 2
    >   ceph_pool_min_size{cluster="ceph",pool="default.rgw.buckets.index",profile="replicated"} 2
    >   ceph_pool_min_size{cluster="ceph",pool="default.rgw.buckets.non-ec",profile="replicated"} 2
    >   ceph_pool_min_size{cluster="ceph",pool="default.rgw.control",profile="replicated"} 2
    >   ceph_pool_min_size{cluster="ceph",pool="default.rgw.log",profile="replicated"} 2
    >   ceph_pool_min_size{cluster="ceph",pool="default.rgw.meta",profile="replicated"} 2
    >   ceph_pool_min_size{cluster="ceph",pool="images",profile="replicated"} 2
    >   ceph_pool_min_size{cluster="ceph",pool="manila_data",profile="replicated"} 2
    >   ceph_pool_min_size{cluster="ceph",pool="manila_metadata",profile="replicated"} 2
    >   ceph_pool_min_size{cluster="ceph",pool="vms",profile="replicated"} 2
    >   ceph_pool_min_size{cluster="ceph",pool="volumes",profile="replicated"} 2
    >   # HELP ceph_pool_objects_total Total no. of objects allocated within the pool
    >   # TYPE ceph_pool_objects_total gauge
    >   ceph_pool_objects_total{cluster="ceph",pool=".rgw.root"} 4
    >   ceph_pool_objects_total{cluster="ceph",pool="backups"} 2
    >   ceph_pool_objects_total{cluster="ceph",pool="cephfs_data"} 3.173553e+06
    >   ceph_pool_objects_total{cluster="ceph",pool="cephfs_metadata"} 2903
    >   ceph_pool_objects_total{cluster="ceph",pool="default.rgw.buckets.data"} 350710
    >   ceph_pool_objects_total{cluster="ceph",pool="default.rgw.buckets.index"} 19
    >   ceph_pool_objects_total{cluster="ceph",pool="default.rgw.buckets.non-ec"} 0
    >   ceph_pool_objects_total{cluster="ceph",pool="default.rgw.control"} 8
    >   ceph_pool_objects_total{cluster="ceph",pool="default.rgw.log"} 207
    >   ceph_pool_objects_total{cluster="ceph",pool="default.rgw.meta"} 51
    >   ceph_pool_objects_total{cluster="ceph",pool="images"} 0
    >   ceph_pool_objects_total{cluster="ceph",pool="manila_data"} 0
    >   ceph_pool_objects_total{cluster="ceph",pool="manila_metadata"} 0
    >   ceph_pool_objects_total{cluster="ceph",pool="vms"} 0
    >   ceph_pool_objects_total{cluster="ceph",pool="volumes"} 1.468564e+06
    >   # HELP ceph_pool_pg_num The total count of PGs alotted to a pool
    >   # TYPE ceph_pool_pg_num gauge
    >   ceph_pool_pg_num{cluster="ceph",pool=".rgw.root",profile="replicated"} 32
    >   ceph_pool_pg_num{cluster="ceph",pool="backups",profile="replicated"} 512
    >   ceph_pool_pg_num{cluster="ceph",pool="cephfs_data",profile="replicated"} 256
    >   ceph_pool_pg_num{cluster="ceph",pool="cephfs_metadata",profile="replicated"} 128
    >   ceph_pool_pg_num{cluster="ceph",pool="default.rgw.buckets.data",profile="replicated"} 512
    >   ceph_pool_pg_num{cluster="ceph",pool="default.rgw.buckets.index",profile="replicated"} 64
    >   ceph_pool_pg_num{cluster="ceph",pool="default.rgw.buckets.non-ec",profile="replicated"} 8
    >   ceph_pool_pg_num{cluster="ceph",pool="default.rgw.control",profile="replicated"} 32
    >   ceph_pool_pg_num{cluster="ceph",pool="default.rgw.log",profile="replicated"} 32
    >   ceph_pool_pg_num{cluster="ceph",pool="default.rgw.meta",profile="replicated"} 32
    >   ceph_pool_pg_num{cluster="ceph",pool="images",profile="replicated"} 32
    >   ceph_pool_pg_num{cluster="ceph",pool="manila_data",profile="replicated"} 8
    >   ceph_pool_pg_num{cluster="ceph",pool="manila_metadata",profile="replicated"} 8
    >   ceph_pool_pg_num{cluster="ceph",pool="vms",profile="replicated"} 128
    >   ceph_pool_pg_num{cluster="ceph",pool="volumes",profile="replicated"} 1024
    >   # HELP ceph_pool_pgp_num The total count of PGs alotted to a pool and used for placements
    >   # TYPE ceph_pool_pgp_num gauge
    >   ceph_pool_pgp_num{cluster="ceph",pool=".rgw.root",profile="replicated"} 32
    >   ceph_pool_pgp_num{cluster="ceph",pool="backups",profile="replicated"} 512
    >   ceph_pool_pgp_num{cluster="ceph",pool="cephfs_data",profile="replicated"} 256
    >   ceph_pool_pgp_num{cluster="ceph",pool="cephfs_metadata",profile="replicated"} 128
    >   ceph_pool_pgp_num{cluster="ceph",pool="default.rgw.buckets.data",profile="replicated"} 512
    >   ceph_pool_pgp_num{cluster="ceph",pool="default.rgw.buckets.index",profile="replicated"} 64
    >   ceph_pool_pgp_num{cluster="ceph",pool="default.rgw.buckets.non-ec",profile="replicated"} 8
    >   ceph_pool_pgp_num{cluster="ceph",pool="default.rgw.control",profile="replicated"} 32
    >   ceph_pool_pgp_num{cluster="ceph",pool="default.rgw.log",profile="replicated"} 32
    >   ceph_pool_pgp_num{cluster="ceph",pool="default.rgw.meta",profile="replicated"} 32
    >   ceph_pool_pgp_num{cluster="ceph",pool="images",profile="replicated"} 32
    >   ceph_pool_pgp_num{cluster="ceph",pool="manila_data",profile="replicated"} 8
    >   ceph_pool_pgp_num{cluster="ceph",pool="manila_metadata",profile="replicated"} 8
    >   ceph_pool_pgp_num{cluster="ceph",pool="vms",profile="replicated"} 128
    >   ceph_pool_pgp_num{cluster="ceph",pool="volumes",profile="replicated"} 1024
    >   # HELP ceph_pool_quota_max_bytes Maximum amount of bytes of data allowed in a pool
    >   # TYPE ceph_pool_quota_max_bytes gauge
    >   ceph_pool_quota_max_bytes{cluster="ceph",pool=".rgw.root",profile="replicated"} 0
    >   ceph_pool_quota_max_bytes{cluster="ceph",pool="backups",profile="replicated"} 0
    >   ceph_pool_quota_max_bytes{cluster="ceph",pool="cephfs_data",profile="replicated"} 0
    >   ceph_pool_quota_max_bytes{cluster="ceph",pool="cephfs_metadata",profile="replicated"} 0
    >   ceph_pool_quota_max_bytes{cluster="ceph",pool="default.rgw.buckets.data",profile="replicated"} 0
    >   ceph_pool_quota_max_bytes{cluster="ceph",pool="default.rgw.buckets.index",profile="replicated"} 0
    >   ceph_pool_quota_max_bytes{cluster="ceph",pool="default.rgw.buckets.non-ec",profile="replicated"} 0
    >   ceph_pool_quota_max_bytes{cluster="ceph",pool="default.rgw.control",profile="replicated"} 0
    >   ceph_pool_quota_max_bytes{cluster="ceph",pool="default.rgw.log",profile="replicated"} 0
    >   ceph_pool_quota_max_bytes{cluster="ceph",pool="default.rgw.meta",profile="replicated"} 0
    >   ceph_pool_quota_max_bytes{cluster="ceph",pool="images",profile="replicated"} 0
    >   ceph_pool_quota_max_bytes{cluster="ceph",pool="manila_data",profile="replicated"} 0
    >   ceph_pool_quota_max_bytes{cluster="ceph",pool="manila_metadata",profile="replicated"} 0
    >   ceph_pool_quota_max_bytes{cluster="ceph",pool="vms",profile="replicated"} 0
    >   ceph_pool_quota_max_bytes{cluster="ceph",pool="volumes",profile="replicated"} 0
    >   # HELP ceph_pool_quota_max_objects Maximum amount of RADOS objects allowed in a pool
    >   # TYPE ceph_pool_quota_max_objects gauge
    >   ceph_pool_quota_max_objects{cluster="ceph",pool=".rgw.root",profile="replicated"} 0
    >   ceph_pool_quota_max_objects{cluster="ceph",pool="backups",profile="replicated"} 0
    >   ceph_pool_quota_max_objects{cluster="ceph",pool="cephfs_data",profile="replicated"} 0
    >   ceph_pool_quota_max_objects{cluster="ceph",pool="cephfs_metadata",profile="replicated"} 0
    >   ceph_pool_quota_max_objects{cluster="ceph",pool="default.rgw.buckets.data",profile="replicated"} 0
    >   ceph_pool_quota_max_objects{cluster="ceph",pool="default.rgw.buckets.index",profile="replicated"} 0
    >   ceph_pool_quota_max_objects{cluster="ceph",pool="default.rgw.buckets.non-ec",profile="replicated"} 0
    >   ceph_pool_quota_max_objects{cluster="ceph",pool="default.rgw.control",profile="replicated"} 0
    >   ceph_pool_quota_max_objects{cluster="ceph",pool="default.rgw.log",profile="replicated"} 0
    >   ceph_pool_quota_max_objects{cluster="ceph",pool="default.rgw.meta",profile="replicated"} 0
    >   ceph_pool_quota_max_objects{cluster="ceph",pool="images",profile="replicated"} 0
    >   ceph_pool_quota_max_objects{cluster="ceph",pool="manila_data",profile="replicated"} 0
    >   ceph_pool_quota_max_objects{cluster="ceph",pool="manila_metadata",profile="replicated"} 0
    >   ceph_pool_quota_max_objects{cluster="ceph",pool="vms",profile="replicated"} 0
    >   ceph_pool_quota_max_objects{cluster="ceph",pool="volumes",profile="replicated"} 0
    >   # HELP ceph_pool_raw_used_bytes Raw capacity of the pool that is currently under use, this factors in the size
    >   # TYPE ceph_pool_raw_used_bytes gauge
    >   ceph_pool_raw_used_bytes{cluster="ceph",pool=".rgw.root"} 0
    >   ceph_pool_raw_used_bytes{cluster="ceph",pool="backups"} 0
    >   ceph_pool_raw_used_bytes{cluster="ceph",pool="cephfs_data"} 0
    >   ceph_pool_raw_used_bytes{cluster="ceph",pool="cephfs_metadata"} 0
    >   ceph_pool_raw_used_bytes{cluster="ceph",pool="default.rgw.buckets.data"} 0
    >   ceph_pool_raw_used_bytes{cluster="ceph",pool="default.rgw.buckets.index"} 0
    >   ceph_pool_raw_used_bytes{cluster="ceph",pool="default.rgw.buckets.non-ec"} 0
    >   ceph_pool_raw_used_bytes{cluster="ceph",pool="default.rgw.control"} 0
    >   ceph_pool_raw_used_bytes{cluster="ceph",pool="default.rgw.log"} 0
    >   ceph_pool_raw_used_bytes{cluster="ceph",pool="default.rgw.meta"} 0
    >   ceph_pool_raw_used_bytes{cluster="ceph",pool="images"} 0
    >   ceph_pool_raw_used_bytes{cluster="ceph",pool="manila_data"} 0
    >   ceph_pool_raw_used_bytes{cluster="ceph",pool="manila_metadata"} 0
    >   ceph_pool_raw_used_bytes{cluster="ceph",pool="vms"} 0
    >   ceph_pool_raw_used_bytes{cluster="ceph",pool="volumes"} 0
    >   # HELP ceph_pool_read_bytes_total Total read throughput for the pool
    >   # TYPE ceph_pool_read_bytes_total gauge
    >   ceph_pool_read_bytes_total{cluster="ceph",pool=".rgw.root"} 32768
    >   ceph_pool_read_bytes_total{cluster="ceph",pool="backups"} 0
    >   ceph_pool_read_bytes_total{cluster="ceph",pool="cephfs_data"} 2.95433874325504e+14
    >   ceph_pool_read_bytes_total{cluster="ceph",pool="cephfs_metadata"} 1.13943552e+09
    >   ceph_pool_read_bytes_total{cluster="ceph",pool="default.rgw.buckets.data"} 3.932592861184e+12
    >   ceph_pool_read_bytes_total{cluster="ceph",pool="default.rgw.buckets.index"} 2.26243296256e+11
    >   ceph_pool_read_bytes_total{cluster="ceph",pool="default.rgw.buckets.non-ec"} 1.5340544e+08
    >   ceph_pool_read_bytes_total{cluster="ceph",pool="default.rgw.control"} 0
    >   ceph_pool_read_bytes_total{cluster="ceph",pool="default.rgw.log"} 1.05165513728e+11
    >   ceph_pool_read_bytes_total{cluster="ceph",pool="default.rgw.meta"} 5.718528e+07
    >   ceph_pool_read_bytes_total{cluster="ceph",pool="images"} 0
    >   ceph_pool_read_bytes_total{cluster="ceph",pool="manila_data"} 0
    >   ceph_pool_read_bytes_total{cluster="ceph",pool="manila_metadata"} 0
    >   ceph_pool_read_bytes_total{cluster="ceph",pool="vms"} 0
    >   ceph_pool_read_bytes_total{cluster="ceph",pool="volumes"} 1.47524386542592e+14
    >   # HELP ceph_pool_read_total Total read i/o calls for the pool
    >   # TYPE ceph_pool_read_total gauge
    >   ceph_pool_read_total{cluster="ceph",pool=".rgw.root"} 48
    >   ceph_pool_read_total{cluster="ceph",pool="backups"} 0
    >   ceph_pool_read_total{cluster="ceph",pool="cephfs_data"} 5.64618904e+08
    >   ceph_pool_read_total{cluster="ceph",pool="cephfs_metadata"} 52100
    >   ceph_pool_read_total{cluster="ceph",pool="default.rgw.buckets.data"} 3.213604e+06
    >   ceph_pool_read_total{cluster="ceph",pool="default.rgw.buckets.index"} 2.329783e+06
    >   ceph_pool_read_total{cluster="ceph",pool="default.rgw.buckets.non-ec"} 285811
    >   ceph_pool_read_total{cluster="ceph",pool="default.rgw.control"} 0
    >   ceph_pool_read_total{cluster="ceph",pool="default.rgw.log"} 1.02674928e+08
    >   ceph_pool_read_total{cluster="ceph",pool="default.rgw.meta"} 67853
    >   ceph_pool_read_total{cluster="ceph",pool="images"} 0
    >   ceph_pool_read_total{cluster="ceph",pool="manila_data"} 0
    >   ceph_pool_read_total{cluster="ceph",pool="manila_metadata"} 0
    >   ceph_pool_read_total{cluster="ceph",pool="vms"} 0
    >   ceph_pool_read_total{cluster="ceph",pool="volumes"} 2.905084233e+09
    >   # HELP ceph_pool_size Total copies or chunks of an object that need to be present for a healthy cluster
    >   # TYPE ceph_pool_size gauge
    >   ceph_pool_size{cluster="ceph",pool=".rgw.root",profile="replicated"} 3
    >   ceph_pool_size{cluster="ceph",pool="backups",profile="replicated"} 3
    >   ceph_pool_size{cluster="ceph",pool="cephfs_data",profile="replicated"} 3
    >   ceph_pool_size{cluster="ceph",pool="cephfs_metadata",profile="replicated"} 3
    >   ceph_pool_size{cluster="ceph",pool="default.rgw.buckets.data",profile="replicated"} 3
    >   ceph_pool_size{cluster="ceph",pool="default.rgw.buckets.index",profile="replicated"} 3
    >   ceph_pool_size{cluster="ceph",pool="default.rgw.buckets.non-ec",profile="replicated"} 3
    >   ceph_pool_size{cluster="ceph",pool="default.rgw.control",profile="replicated"} 3
    >   ceph_pool_size{cluster="ceph",pool="default.rgw.log",profile="replicated"} 3
    >   ceph_pool_size{cluster="ceph",pool="default.rgw.meta",profile="replicated"} 3
    >   ceph_pool_size{cluster="ceph",pool="images",profile="replicated"} 3
    >   ceph_pool_size{cluster="ceph",pool="manila_data",profile="replicated"} 3
    >   ceph_pool_size{cluster="ceph",pool="manila_metadata",profile="replicated"} 3
    >   ceph_pool_size{cluster="ceph",pool="vms",profile="replicated"} 3
    >   ceph_pool_size{cluster="ceph",pool="volumes",profile="replicated"} 3
    >   # HELP ceph_pool_stripe_width Stripe width of a RADOS object in a pool
    >   # TYPE ceph_pool_stripe_width gauge
    >   ceph_pool_stripe_width{cluster="ceph",pool=".rgw.root",profile="replicated"} 0
    >   ceph_pool_stripe_width{cluster="ceph",pool="backups",profile="replicated"} 0
    >   ceph_pool_stripe_width{cluster="ceph",pool="cephfs_data",profile="replicated"} 0
    >   ceph_pool_stripe_width{cluster="ceph",pool="cephfs_metadata",profile="replicated"} 0
    >   ceph_pool_stripe_width{cluster="ceph",pool="default.rgw.buckets.data",profile="replicated"} 0
    >   ceph_pool_stripe_width{cluster="ceph",pool="default.rgw.buckets.index",profile="replicated"} 0
    >   ceph_pool_stripe_width{cluster="ceph",pool="default.rgw.buckets.non-ec",profile="replicated"} 0
    >   ceph_pool_stripe_width{cluster="ceph",pool="default.rgw.control",profile="replicated"} 0
    >   ceph_pool_stripe_width{cluster="ceph",pool="default.rgw.log",profile="replicated"} 0
    >   ceph_pool_stripe_width{cluster="ceph",pool="default.rgw.meta",profile="replicated"} 0
    >   ceph_pool_stripe_width{cluster="ceph",pool="images",profile="replicated"} 0
    >   ceph_pool_stripe_width{cluster="ceph",pool="manila_data",profile="replicated"} 0
    >   ceph_pool_stripe_width{cluster="ceph",pool="manila_metadata",profile="replicated"} 0
    >   ceph_pool_stripe_width{cluster="ceph",pool="vms",profile="replicated"} 0
    >   ceph_pool_stripe_width{cluster="ceph",pool="volumes",profile="replicated"} 0
    >   # HELP ceph_pool_used_bytes Capacity of the pool that is currently under use
    >   # TYPE ceph_pool_used_bytes gauge
    >   ceph_pool_used_bytes{cluster="ceph",pool=".rgw.root"} 1165
    >   ceph_pool_used_bytes{cluster="ceph",pool="backups"} 19
    >   ceph_pool_used_bytes{cluster="ceph",pool="cephfs_data"} 1.214347942385e+13
    >   ceph_pool_used_bytes{cluster="ceph",pool="cephfs_metadata"} 1.550753892e+09
    >   ceph_pool_used_bytes{cluster="ceph",pool="default.rgw.buckets.data"} 1.161473266644e+12
    >   ceph_pool_used_bytes{cluster="ceph",pool="default.rgw.buckets.index"} 0
    >   ceph_pool_used_bytes{cluster="ceph",pool="default.rgw.buckets.non-ec"} 0
    >   ceph_pool_used_bytes{cluster="ceph",pool="default.rgw.control"} 0
    >   ceph_pool_used_bytes{cluster="ceph",pool="default.rgw.log"} 0
    >   ceph_pool_used_bytes{cluster="ceph",pool="default.rgw.meta"} 12233
    >   ceph_pool_used_bytes{cluster="ceph",pool="images"} 0
    >   ceph_pool_used_bytes{cluster="ceph",pool="manila_data"} 0
    >   ceph_pool_used_bytes{cluster="ceph",pool="manila_metadata"} 0
    >   ceph_pool_used_bytes{cluster="ceph",pool="vms"} 0
    >   ceph_pool_used_bytes{cluster="ceph",pool="volumes"} 6.151059880412e+12
    >   # HELP ceph_pool_write_bytes_total Total write throughput for the pool
    >   # TYPE ceph_pool_write_bytes_total gauge
    >   ceph_pool_write_bytes_total{cluster="ceph",pool=".rgw.root"} 0
    >   ceph_pool_write_bytes_total{cluster="ceph",pool="backups"} 0
    >   ceph_pool_write_bytes_total{cluster="ceph",pool="cephfs_data"} 1.6488658957312e+13
    >   ceph_pool_write_bytes_total{cluster="ceph",pool="cephfs_metadata"} 3.0050791424e+10
    >   ceph_pool_write_bytes_total{cluster="ceph",pool="default.rgw.buckets.data"} 1.187636965376e+12
    >   ceph_pool_write_bytes_total{cluster="ceph",pool="default.rgw.buckets.index"} 3.62549248e+08
    >   ceph_pool_write_bytes_total{cluster="ceph",pool="default.rgw.buckets.non-ec"} 5.7629696e+07
    >   ceph_pool_write_bytes_total{cluster="ceph",pool="default.rgw.control"} 0
    >   ceph_pool_write_bytes_total{cluster="ceph",pool="default.rgw.log"} 6.524928e+06
    >   ceph_pool_write_bytes_total{cluster="ceph",pool="default.rgw.meta"} 3.908608e+06
    >   ceph_pool_write_bytes_total{cluster="ceph",pool="images"} 0
    >   ceph_pool_write_bytes_total{cluster="ceph",pool="manila_data"} 0
    >   ceph_pool_write_bytes_total{cluster="ceph",pool="manila_metadata"} 0
    >   ceph_pool_write_bytes_total{cluster="ceph",pool="vms"} 0
    >   ceph_pool_write_bytes_total{cluster="ceph",pool="volumes"} 1.01855169995776e+14
    >   # HELP ceph_pool_write_total Total write i/o calls for the pool
    >   # TYPE ceph_pool_write_total gauge
    >   ceph_pool_write_total{cluster="ceph",pool=".rgw.root"} 0
    >   ceph_pool_write_total{cluster="ceph",pool="backups"} 0
    >   ceph_pool_write_total{cluster="ceph",pool="cephfs_data"} 7.427688e+06
    >   ceph_pool_write_total{cluster="ceph",pool="cephfs_metadata"} 4.492419e+06
    >   ceph_pool_write_total{cluster="ceph",pool="default.rgw.buckets.data"} 2.124456e+06
    >   ceph_pool_write_total{cluster="ceph",pool="default.rgw.buckets.index"} 612947
    >   ceph_pool_write_total{cluster="ceph",pool="default.rgw.buckets.non-ec"} 240100
    >   ceph_pool_write_total{cluster="ceph",pool="default.rgw.control"} 0
    >   ceph_pool_write_total{cluster="ceph",pool="default.rgw.log"} 6.8416624e+07
    >   ceph_pool_write_total{cluster="ceph",pool="default.rgw.meta"} 7821
    >   ceph_pool_write_total{cluster="ceph",pool="images"} 0
    >   ceph_pool_write_total{cluster="ceph",pool="manila_data"} 0
    >   ceph_pool_write_total{cluster="ceph",pool="manila_metadata"} 0
    >   ceph_pool_write_total{cluster="ceph",pool="vms"} 0
    >   ceph_pool_write_total{cluster="ceph",pool="volumes"} 3.98959274e+08
    >   # HELP ceph_recovering_pgs No. of recovering PGs in the cluster
    >   # TYPE ceph_recovering_pgs gauge
    >   ceph_recovering_pgs{cluster="ceph"} 0
    >   # HELP ceph_recovery_io_bytes Rate of bytes being recovered in cluster per second
    >   # TYPE ceph_recovery_io_bytes gauge
    >   ceph_recovery_io_bytes{cluster="ceph"} 0
    >   # HELP ceph_recovery_io_keys Rate of keys being recovered in cluster per second
    >   # TYPE ceph_recovery_io_keys gauge
    >   ceph_recovery_io_keys{cluster="ceph"} 0
    >   # HELP ceph_recovery_io_objects Rate of objects being recovered in cluster per second
    >   # TYPE ceph_recovery_io_objects gauge
    >   ceph_recovery_io_objects{cluster="ceph"} 0
    >   # HELP ceph_recovery_wait_pgs No. of PGs in the cluster with recovery_wait state
    >   # TYPE ceph_recovery_wait_pgs gauge
    >   ceph_recovery_wait_pgs{cluster="ceph"} 0
    >   # HELP ceph_scrubbing_pgs No. of scrubbing PGs in the cluster
    >   # TYPE ceph_scrubbing_pgs gauge
    >   ceph_scrubbing_pgs{cluster="ceph"} 2
    >   # HELP ceph_slow_requests No. of slow requests/slow ops
    >   # TYPE ceph_slow_requests gauge
    >   ceph_slow_requests{cluster="ceph"} 0
    >   # HELP ceph_stale_pgs No. of stale PGs in the cluster
    >   # TYPE ceph_stale_pgs gauge
    >   ceph_stale_pgs{cluster="ceph"} 0
    >   # HELP ceph_stuck_degraded_pgs No. of PGs stuck in a degraded state
    >   # TYPE ceph_stuck_degraded_pgs gauge
    >   ceph_stuck_degraded_pgs{cluster="ceph"} 0
    >   # HELP ceph_stuck_stale_pgs No. of stuck stale PGs in the cluster
    >   # TYPE ceph_stuck_stale_pgs gauge
    >   ceph_stuck_stale_pgs{cluster="ceph"} 0
    >   # HELP ceph_stuck_unclean_pgs No. of PGs stuck in an unclean state
    >   # TYPE ceph_stuck_unclean_pgs gauge
    >   ceph_stuck_unclean_pgs{cluster="ceph"} 0
    >   # HELP ceph_stuck_undersized_pgs No. of stuck undersized PGs in the cluster
    >   # TYPE ceph_stuck_undersized_pgs gauge
    >   ceph_stuck_undersized_pgs{cluster="ceph"} 0
    >   # HELP ceph_total_pgs Total no. of PGs in the cluster
    >   # TYPE ceph_total_pgs gauge
    >   ceph_total_pgs{cluster="ceph"} 2808
    >   # HELP ceph_unclean_pgs No. of PGs in an unclean state
    >   # TYPE ceph_unclean_pgs gauge
    >   ceph_unclean_pgs{cluster="ceph"} 0
    >   # HELP ceph_undersized_pgs No. of undersized PGs in the cluster
    >   # TYPE ceph_undersized_pgs gauge
    >   ceph_undersized_pgs{cluster="ceph"} 0
    >   # HELP go_gc_duration_seconds A summary of the GC invocation durations.
    >   # TYPE go_gc_duration_seconds summary
    >   go_gc_duration_seconds{quantile="0"} 8.482e-06
    >   go_gc_duration_seconds{quantile="0.25"} 1.1796e-05
    >   go_gc_duration_seconds{quantile="0.5"} 1.3048e-05
    >   go_gc_duration_seconds{quantile="0.75"} 1.6637e-05
    >   go_gc_duration_seconds{quantile="1"} 0.00022216
    >   go_gc_duration_seconds_sum 0.136177947
    >   go_gc_duration_seconds_count 5662
    >   # HELP go_goroutines Number of goroutines that currently exist.
    >   # TYPE go_goroutines gauge
    >   go_goroutines 8
    >   # HELP go_memstats_alloc_bytes Number of bytes allocated and still in use.
    >   # TYPE go_memstats_alloc_bytes gauge
    >   go_memstats_alloc_bytes 2.196048e+06
    >   # HELP go_memstats_alloc_bytes_total Total number of bytes allocated, even if freed.
    >   # TYPE go_memstats_alloc_bytes_total counter
    >   go_memstats_alloc_bytes_total 1.2936046784e+10
    >   # HELP go_memstats_buck_hash_sys_bytes Number of bytes used by the profiling bucket hash table.
    >   # TYPE go_memstats_buck_hash_sys_bytes gauge
    >   go_memstats_buck_hash_sys_bytes 1.651494e+06
    >   # HELP go_memstats_frees_total Total number of frees.
    >   # TYPE go_memstats_frees_total counter
    >   go_memstats_frees_total 1.80598945e+08
    >   # HELP go_memstats_gc_cpu_fraction The fraction of this program's available CPU time used by the GC since the program started.
    >   # TYPE go_memstats_gc_cpu_fraction gauge
    >   go_memstats_gc_cpu_fraction 4.5890631512404445e-05
    >   # HELP go_memstats_gc_sys_bytes Number of bytes used for garbage collection system metadata.
    >   # TYPE go_memstats_gc_sys_bytes gauge
    >   go_memstats_gc_sys_bytes 2.377728e+06
    >   # HELP go_memstats_heap_alloc_bytes Number of heap bytes allocated and still in use.
    >   # TYPE go_memstats_heap_alloc_bytes gauge
    >   go_memstats_heap_alloc_bytes 2.196048e+06
    >   # HELP go_memstats_heap_idle_bytes Number of heap bytes waiting to be used.
    >   # TYPE go_memstats_heap_idle_bytes gauge
    >   go_memstats_heap_idle_bytes 6.2849024e+07
    >   # HELP go_memstats_heap_inuse_bytes Number of heap bytes that are in use.
    >   # TYPE go_memstats_heap_inuse_bytes gauge
    >   go_memstats_heap_inuse_bytes 3.60448e+06
    >   # HELP go_memstats_heap_objects Number of allocated objects.
    >   # TYPE go_memstats_heap_objects gauge
    >   go_memstats_heap_objects 28146
    >   # HELP go_memstats_heap_released_bytes Number of heap bytes released to OS.
    >   # TYPE go_memstats_heap_released_bytes gauge
    >   go_memstats_heap_released_bytes 6.1874176e+07
    >   # HELP go_memstats_heap_sys_bytes Number of heap bytes obtained from system.
    >   # TYPE go_memstats_heap_sys_bytes gauge
    >   go_memstats_heap_sys_bytes 6.6453504e+07
    >   # HELP go_memstats_last_gc_time_seconds Number of seconds since 1970 of last garbage collection.
    >   # TYPE go_memstats_last_gc_time_seconds gauge
    >   go_memstats_last_gc_time_seconds 1.6253138311418407e+09
    >   # HELP go_memstats_lookups_total Total number of pointer lookups.
    >   # TYPE go_memstats_lookups_total counter
    >   go_memstats_lookups_total 0
    >   # HELP go_memstats_mallocs_total Total number of mallocs.
    >   # TYPE go_memstats_mallocs_total counter
    >   go_memstats_mallocs_total 1.80627091e+08
    >   # HELP go_memstats_mcache_inuse_bytes Number of bytes in use by mcache structures.
    >   # TYPE go_memstats_mcache_inuse_bytes gauge
    >   go_memstats_mcache_inuse_bytes 10416
    >   # HELP go_memstats_mcache_sys_bytes Number of bytes used for mcache structures obtained from system.
    >   # TYPE go_memstats_mcache_sys_bytes gauge
    >   go_memstats_mcache_sys_bytes 16384
    >   # HELP go_memstats_mspan_inuse_bytes Number of bytes in use by mspan structures.
    >   # TYPE go_memstats_mspan_inuse_bytes gauge
    >   go_memstats_mspan_inuse_bytes 53176
    >   # HELP go_memstats_mspan_sys_bytes Number of bytes used for mspan structures obtained from system.
    >   # TYPE go_memstats_mspan_sys_bytes gauge
    >   go_memstats_mspan_sys_bytes 65536
    >   # HELP go_memstats_next_gc_bytes Number of heap bytes when next garbage collection will take place.
    >   # TYPE go_memstats_next_gc_bytes gauge
    >   go_memstats_next_gc_bytes 4.194304e+06
    >   # HELP go_memstats_other_sys_bytes Number of bytes used for other system allocations.
    >   # TYPE go_memstats_other_sys_bytes gauge
    >   go_memstats_other_sys_bytes 1.590738e+06
    >   # HELP go_memstats_stack_inuse_bytes Number of bytes in use by the stack allocator.
    >   # TYPE go_memstats_stack_inuse_bytes gauge
    >   go_memstats_stack_inuse_bytes 655360
    >   # HELP go_memstats_stack_sys_bytes Number of bytes obtained from system for stack allocator.
    >   # TYPE go_memstats_stack_sys_bytes gauge
    >   go_memstats_stack_sys_bytes 655360
    >   # HELP go_memstats_sys_bytes Number of bytes obtained from system.
    >   # TYPE go_memstats_sys_bytes gauge
    >   go_memstats_sys_bytes 7.2810744e+07
    >   # HELP go_threads Number of OS threads created
    >   # TYPE go_threads gauge
    >   go_threads 11
    >   # HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.
    >   # TYPE process_cpu_seconds_total counter
    >   process_cpu_seconds_total 139.85
    >   # HELP process_max_fds Maximum number of open file descriptors.
    >   # TYPE process_max_fds gauge
    >   process_max_fds 1024
    >   # HELP process_open_fds Number of open file descriptors.
    >   # TYPE process_open_fds gauge
    >   process_open_fds 19
    >   # HELP process_resident_memory_bytes Resident memory size in bytes.
    >   # TYPE process_resident_memory_bytes gauge
    >   process_resident_memory_bytes 5.287936e+07
    >   # HELP process_start_time_seconds Start time of the process since unix epoch in seconds.
    >   # TYPE process_start_time_seconds gauge
    >   process_start_time_seconds 1.62528418333e+09
    >   # HELP process_virtual_memory_bytes Virtual memory size in bytes.
    >   # TYPE process_virtual_memory_bytes gauge
    >   process_virtual_memory_bytes 1.546428416e+09
    
    


