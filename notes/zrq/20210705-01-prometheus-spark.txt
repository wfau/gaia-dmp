#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#


    Results of watching Spark ML example
    
    On the second pass it is going back to the source data, over and over.
    The impact of slow reads on CephFS shares causes the delay.
    .. but why does the 


    Grafana - dual axis graphs
    https://grafana.com/blog/2020/03/10/learn-grafana-how-to-use-dual-axis-graphs/

    


    Spark 3.x has native Prometheus matrics.
    https://databricks.com/session_na20/native-support-of-prometheus-monitoring-in-apache-spark-3-0
    
    Spark 3.x Prometheus matrics.
    https://dzlab.github.io/bigdata/2020/07/03/spark3-monitoring-1/

    Benchmarking random forests
    http://datascience.la/benchmarking-random-forest-implementations/
    
        No mention of issues with multiple passes

    Random forests on big data
    https://stats.stackexchange.com/questions/458140/advice-on-running-random-forests-on-a-large-dataset        
    
        The only thing that I'd add, is that you rather should not use Spark.
        You can check those benchmarks, Spark "is slower and has a larger memory
        footprint" and for some versions of Spark "random forests having low prediction
        accuracy vs the other methods", so basically, the Spark implementation of
        random forest is poor.
        https://stats.stackexchange.com/a/458148    
    
    
    RandomForest source code.
    https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/tree/impl/RandomForest.scala
    
    RandomForest example
    https://medium.com/rahasak/random-forest-classifier-with-apache-spark-c63b4a23a7cc
    
    Spark MLib tutorial
    https://towardsdatascience.com/apache-spark-mllib-tutorial-part-3-complete-classification-workflow-a1eb430ad069
    
    Weakness of the Apache Spark ML library
    https://zaleslaw.medium.com/weakness-of-the-apache-spark-ml-library-41e674103591
    
        No mention of the problems we are having.

    When to cache a DataFrame
    https://stackoverflow.com/questions/44156365/when-to-cache-a-dataframe
    
    PySpark dataframe persist       
    https://stackoverflow.com/questions/58396618/how-to-pyspark-dataframe-persist-usage-and-reading-back
    
    Spark persistence storage levels
    https://sparkbyexamples.com/spark/spark-persistence-storage-levels/

    PySpark StorageLevel
    https://data-flair.training/blogs/pyspark-storagelevel/

    PySpark StorageLevel
    https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.StorageLevel

    
    A performance comparison of Dask and Apache Spark
    https://arxiv.org/abs/1907.13030
    
    Distributed data processing with Spark
    https://medium.datadriveninvestor.com/distributed-data-processing-with-apache-spark-2a5e473b0cb1
    
        Where to store data? S3 or HDFS ?
        
        Spark and HDFS are designed to work well together. When Spark needs some data from HDFS,
        it grabs the closest copy which minimizes the time data spends traveling around the network.
        But there is a trade-off to HDFS. You have to maintain and fix the system yourself.
        For many companies, from small startups to big corporations, S3 is just easier, since
        you don’t have to maintain a separate cluster. Also, if you rent clusters from AWS,
        your data usually doesn’t have to go too far in the network since the cluster hardware
        and the S3 hardware are both on Amazon’s data centers. Finally, Spark is smart enough
        to download a small chunk of data and process that chunk while waiting for the rest to
        download.
        

    Spark - cache and checkpoint
    https://livebook.manning.com/book/spark-in-action-second-edition/16-cache-and-checkpoint-enhancing-spark-s-performances/v-14/138
        



    Where does Spark cache RDDs ?
    https://stackoverflow.com/questions/30057323/where-does-spark-actually-persist-rdds-on-disk

        spark.local.dir (by default /tmp)

        Directory to use for "scratch" space in Spark, including map output files and RDDs that get stored on disk.
        This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple
        directories on different disks. NOTE: In Spark 1.0 and later this will be overriden by SPARK_LOCAL_DIRS
        (Standalone, Mesos) or LOCAL_DIRS (YARN) environment variables set by the cluster manager.

