#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#


    How to get Spark to cache/persist/checkpoint on local disk.
    
    
    Checkpoint RDD ReliableCheckpointRDD has different number of partitions from original RDD
    https://stackoverflow.com/questions/33238882/checkpoint-rdd-reliablecheckpointrdd-has-different-number-of-partitions-from-ori 
    
        "I mounted a remote directory on all the workers as checkpoint and it worked perfectly."
        
        Implies it needs a shared file space to checkpoint.


    Spark RDD checkpoint on persisted/cached RDDs are performing the DAG twice
    https://stackoverflow.com/questions/31078350/spark-rdd-checkpoint-on-persisted-cached-rdds-are-performing-the-dag-twice
    
        The data you cached may be evicted due to the lack of memory, and you can open the Spark UI to check whether it's true.
        

    Where does Spark actually persist RDDs on disk?
    https://stackoverflow.com/questions/30057323/where-does-spark-actually-persist-rdds-on-disk

        I am using persist on different storage levels, but I found no difference on performance when I was using MEMORY_ONLY and DISK_ONLY. 

        SPARK_WORKER_DIR or SPARK_LOCAL_DIR are properties which tells where spark is caching on disk


        spark.local.dir (by default /tmp)

        Directory to use for "scratch" space in Spark, including map output files and RDDs that get stored on disk.
        This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories
        on different disks. NOTE: In Spark 1.0 and later this will be overriden by SPARK_LOCAL_DIRS (Standalone, Mesos)
        or LOCAL_DIRS (YARN) environment variables set by the cluster manager.
        

    Dataset Checkpointing
    https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-checkpointing.html
        
        Dataset Checkpointing is a feature of Spark SQL to truncate a logical query plan that could specifically be useful
        for highly iterative data algorithms (e.g. Spark MLlib that uses Spark SQLâ€™s Dataset API for data manipulation).        


    Caching and checkpointing can increase performance            
    https://livebook.manning.com/book/spark-in-action-second-edition/16-cache-and-checkpoint-enhancing-spark-s-performances/v-14/11
            
        n this first section, you will go through what caching and checkpointing are in the context of Apache Spark.
        You then run a lab on dummy data where you execute a process using no cache, using caching, and using both
        eager and non-eager checkpoints. During this process, you will also learn how to collect performance operation
        and finally see those collected data in a visual representation.
            
    PySpark - StorageLevel
    https://www.tutorialspoint.com/pyspark/pyspark_storagelevel.htm            
    
        StorageLevel decides how RDD should be stored. In Apache Spark, StorageLevel decides whether RDD
        should be stored in the memory or should it be stored over the disk, or both. It also decides
        whether to serialize RDD and whether to replicate RDD partitions.

    How can I explicitly free memory in Python?
    https://stackoverflow.com/questions/1316767/how-can-i-explicitly-free-memory-in-python
    
        del my_array
        del my_object
        gc.collect()
        
    
    SparkContext
    https://spark.apache.org/docs/2.0.0-preview/api/python/_modules/pyspark/context.html
    
        Main entry point for Spark functionality. A SparkContext represents the
        connection to a Spark cluster, and can be used to create L{RDD} and
        broadcast variables on that cluster.    
    
    
    RandomForest
    https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/tree/impl/RandomForest.scala
    
    
    Why is my query faster before caching my dataset in Spark?
    https://stackoverflow.com/questions/51423662/why-is-my-query-faster-before-caching-my-dataset-in-spark
    
    
    Best practices for caching in Spark SQL
    https://towardsdatascience.com/best-practices-for-caching-in-spark-sql-b22fb0f02d34
    
    How to force DataFrame evaluation in Spark
    https://stackoverflow.com/questions/42714291/how-to-force-dataframe-evaluation-in-spark
    
    Spark 1.6 Dataframe cache not working correctly
    https://stackoverflow.com/questions/46062742/spark-1-6-dataframe-cache-not-working-correctly
    
    https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.StorageLevel
    
        Flags for controlling the storage of an RDD. Each StorageLevel records whether to use
        memory, whether to drop the RDD to disk if it falls out of memory, whether to keep the
        data in memory in a JAVA-specific serialized format, and whether to replicate the RDD
        partitions on multiple nodes. Also contains static constants for some commonly used
        storage levels, MEMORY_ONLY.
        
        Since the data is always serialized on the Python side, all the constants use the
        serialized formats.
        
        
