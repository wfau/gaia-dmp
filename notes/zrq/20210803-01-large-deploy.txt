#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#


    Target:

        Deploy a large system.

        Apache Spark: Config Cheatsheet
        https://www.c2fo.io/c2fo/spark/aws/emr/2016/07/06/apache-spark-config-cheatsheet/
        https://www.c2fo.io/c2fo/spark/aws/emr/2016/09/01/apache-spark-config-cheatsheet-part2/

        https://github.com/AndresNamm/SparkDebugging

    Result:

        Going round in circles.
        Spark context fails at the first pyspark cell.
        Yarn containers created, but immediately sent a TERM signal.


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --publish 3000:3000 \
        --publish 8088:8088 \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        atolmis/ansible-client:2020.12.02 \
        bash


# -----------------------------------------------------
# Delete everything from the test and dev systems.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            'gaia-dev'

    >   real	3m6.583s
    >   user	1m5.399s
    >   sys	0m8.692s


    time \
        /deployments/openstack/bin/delete-all.sh \
            'gaia-test'

    >   real	3m7.701s
    >   user	1m5.218s
    >   sys	0m8.684s


# -----------------------------------------------------
# -----------------------------------------------------
# Create large deployment configuration.
#[user@desktop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        pushd deployments/hadoop-yarn/ansible/config

            cp cclake-medium-04.yml \
               cclake-large-06.yml

            gedit cclake-large-06.yml &

        popd
    popd

    >   ....
    >   ....


# -----------------------------------------------------
# -----------------------------------------------------
# Create everything, using the new config.
#[root@ansibler]

    cloudname=gaia-dev

    time \
        /deployments/hadoop-yarn/bin/create-all.sh \
            "${cloudname:?}" \
            'cclake-large-06'

    >   real    96m53.675s
    >   user    21m15.287s
    >   sys     6m56.112s


# -----------------------------------------------------
# -----------------------------------------------------
# Edit the deployment configuration.
#[user@desktop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        pushd deployments/hadoop-yarn/ansible/config

            gedit cclake-large-06.yml &

        popd
    popd

    # Increase the maximum to allow for a 10% overhead.
    # - yarn.scheduler.maximum-allocation-mb = 8G
    # + yarn.scheduler.maximum-allocation-mb = 9G


# -----------------------------------------------------
# -----------------------------------------------------
# Delete everything from the test and dev systems.
#[root@ansibler]

    cloudname=gaia-dev

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"

    >   real    3m18.959s
    >   user    1m12.129s
    >   sys     0m9.585s


# -----------------------------------------------------
# Create everything, using the new config.
#[root@ansibler]

    cloudname=gaia-dev

    time \
        /deployments/hadoop-yarn/bin/create-all.sh \
            "${cloudname:?}" \
            'cclake-large-06'

    >   real    67m19.093s
    >   user    17m37.604s
    >   sys     6m1.345s


# -----------------------------------------------------
# Check the deployment status.
#[root@ansibler]

    cat '/tmp/aglais-status.yml'

    >   aglais:
    >     status:
    >       deployment:
    >         type: hadoop-yarn
    >         conf: cclake-large-06
    >         name: gaia-dev-20210802
    >         date: 20210802T181543
    >     spec:
    >       openstack:
    >         cloud: gaia-dev


# -----------------------------------------------------
# Add the Zeppelin user accounts.
#[root@ansibler]

    ssh zeppelin

        pushd "${HOME}"
        ln -s "zeppelin-0.8.2-bin-all" "zeppelin"

            pushd "zeppelin"

                # Manual edit to add names and passwords
                vi conf/shiro.ini

                # Restart Zeppelin for the changes to take.
                bin/zeppelin-daemon.sh restart

            popd
        popd
    exit

    >   Zeppelin stop                                              [  OK  ]
    >   Zeppelin start                                             [  OK  ]


# -----------------------------------------------------
# Add the notebooks from github.
#[root@ansibler]

    ssh zeppelin

        pushd /home/fedora/zeppelin

            mv -b notebook \
               notebook-origin

	        git clone git@github.com:wfau/aglais-notebooks.git notebook

	        bin/zeppelin-daemon.sh restart

        popd
    exit

    >   Zeppelin stop                                              [  OK  ]
    >   Zeppelin start                                             [  OK  ]


# -----------------------------------------------------
# Get the public IP address of our Zeppelin node.
#[root@ansibler]

    deployname=$(
        yq read \
            '/tmp/aglais-status.yml' \
                'aglais.status.deployment.name'
        )

    zeppelinid=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            server list \
                --format json \
        | jq -r '.[] | select(.Name == "'${deployname:?}'-zeppelin") | .ID'
        )

    zeppelinip=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            server show \
                --format json \
                "${zeppelinid:?}" \
        | jq -r '.addresses' \
        | sed '
            s/[[:space:]]//
            s/.*=\(.*\)/\1/
            s/.*,\(.*\)/\1/
            '
        )

cat << EOF
Zeppelin ID [${zeppelinid:?}]
Zeppelin IP [${zeppelinip:?}]
EOF

    >   Zeppelin ID [566371f3-ceeb-4431-858b-b17fc29953da]
    >   Zeppelin IP [128.232.227.165]


# -----------------------------------------------------
# Update our DNS record.
#[root@ansibler]

    ssh root@infra-ops.aglais.uk

        vi /var/aglais/dnsmasq/hosts/gaia-dev.hosts

        ~   128.232.227.165  zeppelin.gaia-dev.aglais.uk


        podman kill --signal SIGHUP dnsmasq

        podman logs dnsmasq | tail

        exit

    >   dnsmasq[1]: read /etc/dnsmasq/hosts/gaia-prod.hosts - 1 addresses
    >   dnsmasq[1]: read /etc/dnsmasq/hosts/gaia-test.hosts - 1 addresses
    >   dnsmasq[1]: read /etc/dnsmasq/hosts/gaia-dev.hosts - 1 addresses


# -----------------------------------------------------
# Check the DNS record.
#[root@ansibler]

    # TODO Add bind-utils to our client container
    # https://github.com/wfau/aglais/issues/391

    dnf install bind-utils

    dig @infra-ops.aglais.uk zeppelin.${cloudname}.aglais.uk

    >   ;; ANSWER SECTION:
    >   zeppelin.gaia-dev.aglais.uk. 300 IN	A	128.232.227.165


# -----------------------------------------------------
# Check the data shares.
# TODO Move this to a bash script in the source tree.
#[root@ansibler]

    sharelist="/deployments/common/manila/datashares.yaml"

    for shareid in $(
        yq read "${sharelist}" 'datashares.[*].id'
        )
    do
        #echo ""
        #echo "Share [${shareid}]"

        checkbase=$(
            yq read "${sharelist}" "datashares.(id == ${shareid}).mountpath"
            )
        checknum=$(
            yq read "${sharelist}" --length "datashares.(id == ${shareid}).checksums"
            )

        for (( i=0; i<checknum; i++ ))
        do
            checkpath=$(
                yq read "${sharelist}" "datashares.(id == ${shareid}).checksums[${i}].path"
                )
            checkcount=$(
                yq read "${sharelist}" "datashares.(id == ${shareid}).checksums[${i}].count"
                )
            checkhash=$(
                yq read "${sharelist}" "datashares.(id == ${shareid}).checksums[${i}].md5sum"
                )

            echo ""
            #echo "Base  [${checkbase}]"
            echo "Share [${checkbase}/${checkpath}]"

            testcount=$(
                ssh zeppelin \
                    "
                    ls -1 ${checkbase}/${checkpath} | wc -l
                    "
                )

            if [ "${testcount}" == "${checkcount}" ]
            then
                echo "Count [PASS]"
            else
                echo "Count [FAIL][${checkcount}][${testcount}]"
            fi

            testhash=$(
                ssh zeppelin \
                    "
                    ls -1 -v ${checkbase}/${checkpath} | md5sum | cut -d ' ' -f 1
                    "
                )

            if [ "${testhash}" == "${checkhash}" ]
            then
                echo "Hash  [PASS]"
            else
                echo "Hash  [FAIL][${checkhash}][${testhash}]"
            fi
        done
    done

    >   Share [/data/gaia/GDR2_6514/GDR2_6514_GAIASOURCE]
    >   Count [PASS]
    >   Hash  [PASS]
    >   ....
    >   ....
    >   Share [/data/twomass/2MASSPSC/]
    >   Count [PASS]
    >   Hash  [PASS]


# -----------------------------------------------------
# -----------------------------------------------------
# Login to our Zeppelin node and generate a new interpreter.json file.
#[root@ansibler]

    ssh zeppelin

        # Create a new list of interpreter bindings.
        find /home/fedora/zeppelin/notebook \
            -mindepth 1 \
            -maxdepth 1 \
            -type d \
            ! -name '.git' \
            -printf '%f\n' \
        | sed '
            1 i \
"interpreterBindings": {
        s/^\(.*\)$/"\1": ["spark", "md", "sh"]/
        $ ! s/^\(.*\)$/\1,/
        $ a \
},
            ' \
            | tee /tmp/bindings.json

    >   "interpreterBindings": {
    >   "2C35YU814": ["spark", "md", "sh"],
    >   "2EZ3MQG4S": ["spark", "md", "sh"],
    >   ....
    >   ....
    >   "2G9BXYCKP": ["spark", "md", "sh"],
    >   "2FF2VTAAM": ["spark", "md", "sh"]
    >   },


        # Replace the existing interpreter bindings.
        jq '
            del(.interpreterBindings[])
            ' \
        /home/fedora/zeppelin/conf/interpreter.json \
        | sed '
            /interpreterBindings/ {
                r /tmp/bindings.json
                d
                }
            ' \
        | jq '.' \
        | tee /tmp/interpreter-new.json

    >   ....
    >   ....

    # Replace the original interpreter.json
    mv /home/fedora/zeppelin/conf/interpreter.json \
       /home/fedora/zeppelin/conf/interpreter.origin

    cp /tmp/interpreter-new.json \
       /home/fedora/zeppelin/conf/interpreter.json

    # Restart Zeppelin to take effect
    /home/fedora/zeppelin/bin/zeppelin-daemon.sh restart

    exit

    >   Zeppelin stop                                              [  OK  ]
    >   Zeppelin start                                             [  OK  ]


# -----------------------------------------------------
# -----------------------------------------------------
# Add our secret function to the ansibler container.
#[root@ansibler]

    # TODO Move this into the Ansible setup.
    # TODO Move our secrets onto our infra-ops server.

    if [ ! -e "${HOME}/bin" ]
    then
        mkdir "${HOME}/bin"
    fi

    cat > "${HOME}/bin/secret" << 'EOF'
ssh -n \
    'secretserver' \
    "bin/secret '${1}'"
EOF

    chmod u+x "${HOME}/bin/secret"


    if [ ! -e "${HOME}/.ssh" ]
    then
        mkdir "${HOME}/.ssh"
    fi

    # Fix for the out of date ssh server.
    # RSA key have been deprecated, so we need to explicitly allow them for now.
    # https://www.reddit.com/r/Fedora/comments/jh9iyi/f33_openssh_no_mutual_signature_algorithm/

    cat >> "${HOME}/.ssh/config" << 'EOF'
Host secretserver
  User     Zarquan
  Hostname data.metagrid.co.uk
  PubkeyAcceptedKeyTypes +ssh-rsa
EOF

    ssh-keyscan 'data.metagrid.co.uk' >> "${HOME}/.ssh/known_hosts"

    secret frog

    >   Green Frog


# -----------------------------------------------------
# -----------------------------------------------------
# Create shell script functions to wrap the REST API.
#[root@ansibler]

    # TODO Add this to the client container.
    # https://github.com/wfau/aglais/issues/542
    # https://github.com/wfau/aglais/issues/495
    dnf install -y dateutils

    zeppelinhost=zeppelin.${cloudname:?}.aglais.uk
    zeppelinport=8080
    zeppelinurl="http://${zeppelinhost:?}:${zeppelinport:?}"

    zeplogin()
        {
        local username=${1:?}
        local password=${2:?}
        zepcookies=/tmp/${username:?}.cookies
        curl \
            --silent \
            --request 'POST' \
            --cookie-jar "${zepcookies:?}" \
            --data "userName=${username:?}" \
            --data "password=${password:?}" \
            "${zeppelinurl:?}/api/login" \
        | jq '.'
        }

    zepnbjsonfile()
        {
        local nbident=${1:?}
        echo "/tmp/${nbident:?}.json"
        }

    zepnbjsonclr()
        {
        local nbident=${1:?}
        local jsonfile=$(zepnbjsonfile ${nbident})
        if [ -f "${jsonfile}" ]
        then
            rm -f "${jsonfile}"
        fi
        }

    zepnbclear()
        {
        local nbident=${1:?}
        zepnbjsonclr ${nbident}
        curl \
            --silent \
            --request PUT \
            --cookie "${zepcookies:?}" \
            "${zeppelinurl:?}/api/notebook/${nbident:?}/clear" \
        | jq '.'
        }

    zepnbstatus()
        {
        local nbident=${1:?}
        zepnbjsonclr ${nbident}
        curl \
            --silent \
            --request GET \
            --cookie "${zepcookies:?}" \
            "${zeppelinurl:?}/api/notebook/${nbident:?}" \
        | jq '.' | tee $(zepnbjsonfile ${nbident}) | jq 'del(.body.paragraphs[])'
        }

    zepnbexecute()
        {
        local nbident=${1:?}
        zepnbjsonclr ${nbident}
        curl \
            --silent \
            --request POST \
            --cookie "${zepcookies:?}" \
            "${zeppelinurl:?}/api/notebook/job/${nbident:?}" \
        | jq '.'
        }


# -----------------------------------------------------
# Execute a notebook paragraph at a time.
#[root@ansibler]

    zepnbexecstep()
        {
        local nbident=${1:?}
        zepnbjsonclr ${nbident}

        # Fetch the notbook details.
        curl \
            --silent \
            --request GET \
            --cookie "${zepcookies:?}" \
            "${zeppelinurl:?}/api/notebook/${nbident:?}" \
            > $(zepnbjsonfile ${nbident})


        # List the title, status and ident.
        paralist=$(mktemp --suffix '.json')
        jq '
            [.body.paragraphs[]? | {id, status, title}]
            ' "$(zepnbjsonfile ${nbident})" \
            > "${paralist}"


        # Execute each paragraph
        jq -r '.[] | @text' "${paralist}" \
        | while read line
            do
                title=$(jq -r '.title' <<< "${line}")
                paraid=$(jq -r '.id'   <<< "${line}")
                status=$(jq -r '.status' <<< "${line}")
                echo ""
                echo "Para [${paraid}][${title}]"

                curl \
                    --silent \
                    --request POST \
                    --cookie "${zepcookies:?}" \
                    "${zeppelinurl:?}/api/notebook/run/${nbident:?}/${paraid:?}" \
                | jq 'del(.body.msg[]? | select(.type != "TEXT") | .data)' \
                | tee "/tmp/para-${paraid}.json"


                result=$(
                    jq -r '.body.code' "/tmp/para-${paraid}.json"
                    )
                echo "Result [${result}]"

                if [ "${result}" != 'SUCCESS' ]
                then
                    break
                fi

            done
        }

# -----------------------------------------------------
# Calculate the elapsed time for each paragraph.
#[root@ansibler]

    zepnbparatime()
        {
        local nbident=${1:?}

        cat $(zepnbjsonfile ${nbident}) \
        | sed '
            /"dateStarted": null,/d
            /"dateStarted":/ {
                h
                s/\([[:space:]]*\)"dateStarted":[[:space:]]*\("[^"]*"\).*$/\1\2/
                x
                }
            /"dateFinished": null,/ d
            /"dateFinished":/ {
                H
                x
                s/[[:space:]]*"dateFinished":[[:space:]]*\("[^"]*"\).*$/ \1/
                s/\([[:space:]]*\)\(.*\)/\1echo "\1\\"elapsedTime\\": \\"$(datediff --format "%H:%M:%S" --input-format "%b %d, %Y %H:%M:%S %p" \2)\\","/e
                x
                G
                }
            ' \
        | jq '
            .body.paragraphs[] | select(.results.code != null) | {
                title,
                result: .results.code,
                time:   .elapsedTime,
                }
            '
        }

# -----------------------------------------------------
# Calculate the elapsed time for the whole notebook.
#[root@ansibler]

    zepnbtotaltime()
        {
        local nbident=${1:?}
        local jsonfile=$(zepnbjsonfile ${nbident})

        local first=$(
            jq -r '
                [.body.paragraphs[] | select(.dateStarted != null) | .dateStarted] | first
                ' \
                "${jsonfile}"
            )

        local last=$(
            jq -r '
                [.body.paragraphs[] | select(.dateFinished != null) | .dateFinished] | last
                ' \
                "${jsonfile}"
            )

        datediff --format "%H:%M:%S" --input-format "%b %d, %Y %H:%M:%S %p" "${first}" "${last}"
        }

# -----------------------------------------------------
# Login to Zeppelin as a normal user.
#[root@ansibler]

    gaiauser=$(secret aglais.zeppelin.gaiauser)
    gaiapass=$(secret aglais.zeppelin.gaiapass)

    zeplogin "${gaiauser:?}" "${gaiapass}"

    >   {
    >     "status": "OK",
    >     "message": "",
    >     "body": {
    >       "principal": "gaiauser",
    >       "ticket": "25b41e90-c56f-4587-b353-9cd2fa380d97",
    >       "roles": "[\"user\"]"
    >     }
    >   }


# -----------------------------------------------------
# Run the SetUp notebook.
#[root@ansibler]

    noteid=2G7GZKWUH

    zepnbclear ${noteid}

    zepnbexecute ${noteid}

    zepnbstatus ${noteid}

    zepnbparatime ${noteid}


    >   java.lang.IllegalStateException: Spark context stopped while waiting for backend
    >   	at org.apache.spark.scheduler.TaskSchedulerImpl.waitBackendReady(TaskSchedulerImpl.scala:834)

    #
    # No clues as to why.
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Edit the deployment configuration.
#[user@desktop]


    # Found a new resource
    # https://github.com/AndresNamm/SparkDebugging/tree/master/ExecutorSizing
    # https://www.c2fo.io/c2fo/spark/aws/emr/2016/07/06/apache-spark-config-cheatsheet/
    # https://www.c2fo.io/img/apache-spark-config-cheatsheet/C2FO-Spark-Config-Cheatsheet.xlsx


    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        pushd deployments/hadoop-yarn/ansible/config

            gedit cclake-large-06.yml &

        popd
    popd


# -----------------------------------------------------
# -----------------------------------------------------
# Run the Spark and Yarn configuration steps.
#[root@ansibler]

    deployconf=cclake-large-06

    pushd '/deployments/hadoop-yarn/ansible'

        ansible-playbook \
            --verbose \
            --inventory "config/${deployconf}.yml" \
            '16-config-yarn-masters.yml'

        ansible-playbook \
            --verbose \
            --inventory "config/${deployconf}.yml" \
            '17-config-yarn-workers.yml'

        ansible-playbook \
            --verbose \
            --inventory "config/${deployconf}.yml" \
            '22-config-spark-master.yml'

    popd


# -----------------------------------------------------
# Restart the services to recognise changes.
#[root@ansibler]

    ssh master01 \
        '
        /opt/hadoop/sbin/stop-all.sh

        echo ""
        echo "Pause ...."
        sleep 30
        echo ""

        /opt/hadoop/sbin/start-all.sh
        '


    >   WARNING: Stopping all Apache Hadoop daemons as fedora in 10 seconds.
    >   WARNING: Use CTRL-C to abort.
    >   Stopping namenodes on [master01]
    >   Stopping datanodes
    >   Stopping secondary namenodes [gaia-dev-20210802-master01.novalocal]
    >   Stopping nodemanagers
    >   worker05: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   worker04: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   worker02: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   worker06: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   worker03: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   worker01: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9
    >   Stopping resourcemanager

    >   WARNING: Attempting to start all Apache Hadoop daemons as fedora in 10 seconds.
    >   WARNING: This is not a recommended production deployment configuration.
    >   WARNING: Use CTRL-C to abort.
    >   Starting namenodes on [master01]
    >   Starting datanodes
    >   Starting secondary namenodes [gaia-dev-20210802-master01.novalocal]
    >   Starting resourcemanager
    >   Starting nodemanagers


    ssh zeppelin \
        '
        /home/fedora/zeppelin/bin/zeppelin-daemon.sh restart
        '

    >   Zeppelin stop                                              [  OK  ]
    >   Zeppelin start                                             [  OK  ]


# -----------------------------------------------------
# Login to Zeppelin as a normal user.
#[root@ansibler]

    gaiauser=$(secret aglais.zeppelin.gaiauser)
    gaiapass=$(secret aglais.zeppelin.gaiapass)

    zeplogin "${gaiauser:?}" "${gaiapass}"

    >   {
    >     "status": "OK",
    >     "message": "",
    >     "body": {
    >       "principal": "gaiauser",
    >       "ticket": "695d6805-b0e5-432b-bdc8-74ccf5acc371",
    >       "roles": "[\"user\"]"
    >     }
    >   }


# -----------------------------------------------------
# Run the SetUp notebook.
#[root@ansibler]

    noteid=2G7GZKWUH

    zepnbclear ${noteid}

    zepnbexecute ${noteid}

    zepnbstatus ${noteid}

    zepnbparatime ${noteid}


    >   {
    >     "title": null,
    >     "result": "SUCCESS",
    >     "time": "0:0:1"
    >   }
    >   {
    >     "title": "Catalogue structure definitions",
    >     "result": "ERROR",
    >     "time": "0:0:45"
    >   }


# -----------------------------------------------------
# -----------------------------------------------------
# Edit the deployment configuration.
#[user@desktop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        pushd deployments/hadoop-yarn/ansible/config

            gedit cclake-large-06.yml &

        popd
    popd


# -----------------------------------------------------
# -----------------------------------------------------
# Run the Spark and Yarn configuration steps.
#[root@ansibler]

    deployconf=cclake-large-06

    pushd '/deployments/hadoop-yarn/ansible'

        ansible-playbook \
            --verbose \
            --inventory "config/${deployconf}.yml" \
            '16-config-yarn-masters.yml'

        ansible-playbook \
            --verbose \
            --inventory "config/${deployconf}.yml" \
            '17-config-yarn-workers.yml'

        ansible-playbook \
            --verbose \
            --inventory "config/${deployconf}.yml" \
            '22-config-spark-master.yml'

    popd


# -----------------------------------------------------
# Restart the services to recognise changes.
#[root@ansibler]

    ssh master01 \
        '
        /opt/hadoop/sbin/stop-all.sh

        echo ""
        echo "Pause ...."
        sleep 30
        echo ""

        /opt/hadoop/sbin/start-all.sh
        '

    >   Stopping namenodes on [master01]
    >   Stopping datanodes
    >   Stopping secondary namenodes [gaia-dev-20210802-master01.novalocal]
    >   Stopping nodemanagers
    >   Stopping resourcemanager

    >   Starting namenodes on [master01]
    >   Starting datanodes
    >   Starting secondary namenodes [gaia-dev-20210802-master01.novalocal]
    >   Starting resourcemanager
    >   Starting nodemanagers


    ssh zeppelin \
        '
        /home/fedora/zeppelin/bin/zeppelin-daemon.sh restart
        '

    >   Zeppelin stop                                              [  OK  ]
    >   Zeppelin start                                             [  OK  ]



# -----------------------------------------------------
# Login to Zeppelin as a normal user.
#[root@ansibler]

    gaiauser=$(secret aglais.zeppelin.gaiauser)
    gaiapass=$(secret aglais.zeppelin.gaiapass)

    zeplogin "${gaiauser:?}" "${gaiapass}"

    >   {
    >     "status": "OK",
    >     "message": "",
    >     "body": {
    >       "principal": "gaiauser",
    >       "ticket": "7423a00f-f968-486d-b277-9cddeecb22a5",
    >       "roles": "[\"user\"]"
    >     }
    >   }


# -----------------------------------------------------
# Run the SetUp notebook.
#[root@ansibler]

    noteid=2G7GZKWUH

    zepnbclear ${noteid}

    zepnbexecstep ${noteid}

    >   Para [20210504-130917_57061499][null]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": [
    >         {
    >           "type": "HTML"
    >         }
    >       ]
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210504-131126_1544574772][Catalogue structure definitions]
    >   {
    >     "status": "INTERNAL_SERVER_ERROR",
    >     "body": {
    >       "code": "ERROR",
    >       "msg": [
    >         {
    >           "type": "TEXT",
    >           "data": "java.lang.IllegalStateException: ...."
    >         }
    >       ]
    >     }
    >   }
    >   Result [ERROR]

# -----------------------------------------------------
# Check the Zeppelin logs.
#[root@ansibler]

    ssh zeppelin \
        '
        cat /home/fedora/zeppelin/logs/zeppelin-interpreter-spark-$(id -un)-$(hostname).log
        '

    >   ....
    >   ....
    >   ERROR [2021-08-03 02:23:12,135] ({YARN application state monitor} Logging.scala[logError]:70) -
    >       Diagnostics message:
    >           Uncaught exception: org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException:
    >               Invalid resource request, requested resource type=[vcores] < 0 or greater than maximum allowed allocation.
    >                   Requested resource=<memory:8192, vCores:5>, maximum allowed allocation=<memory:9216, vCores:4>,
    >                       please note that maximum allowed allocation is calculated by scheduler based on maximum resource of registered NodeManagers,
    >                       which might be less than configured maximum allocation=<memory:9216, vCores:4>
    >   ....
    >   ....

    # THIS
    # Requested resource=<memory:8192, vCores:5>, maximum allowed allocation=<memory:9216, vCores:4>,


# -----------------------------------------------------
# -----------------------------------------------------
# Edit the deployment configuration.
#[user@desktop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        pushd deployments/hadoop-yarn/ansible/config

            gedit cclake-large-06.yml &

        popd
    popd


# -----------------------------------------------------
# -----------------------------------------------------
# Run the Spark and Yarn configuration steps.
#[root@ansibler]

    deployconf=cclake-large-06

    pushd '/deployments/hadoop-yarn/ansible'

        ansible-playbook \
            --verbose \
            --inventory "config/${deployconf}.yml" \
            '16-config-yarn-masters.yml'

        ansible-playbook \
            --verbose \
            --inventory "config/${deployconf}.yml" \
            '17-config-yarn-workers.yml'

        ansible-playbook \
            --verbose \
            --inventory "config/${deployconf}.yml" \
            '22-config-spark-master.yml'

    popd


# -----------------------------------------------------
# Restart the services to recognise changes.
#[root@ansibler]

    ssh master01 \
        '
        /opt/hadoop/sbin/stop-all.sh

        echo ""
        echo "Pause ...."
        sleep 30
        echo ""

        /opt/hadoop/sbin/start-all.sh
        '

    >   ....
    >   ....

    ssh zeppelin \
        '
        /home/fedora/zeppelin/bin/zeppelin-daemon.sh restart
        '

    >   ....
    >   ....


# -----------------------------------------------------
# Login to Zeppelin as a normal user.
#[root@ansibler]

    gaiauser=$(secret aglais.zeppelin.gaiauser)
    gaiapass=$(secret aglais.zeppelin.gaiapass)

    zeplogin "${gaiauser:?}" "${gaiapass}"

    >   ....
    >   ....


# -----------------------------------------------------
# Run the SetUp notebook.
#[root@ansibler]

    noteid=2G7GZKWUH

    zepnbclear ${noteid}

    zepnbexecstep ${noteid}


    >   Para [20210504-130917_57061499][null]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": [
    >         {
    >           "type": "HTML"
    >         }
    >       ]
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210504-131126_1544574772][Catalogue structure definitions]
    >   {
    >     "status": "INTERNAL_SERVER_ERROR",
    >     "body": {
    >       "code": "ERROR",
    >       "msg": [
    >         {
    >           "type": "TEXT",
    >           "data": "java.lang.IllegalStateException: ...."
    >         }
    >       ]
    >     }
    >   }
    >   Result [ERROR]


# -----------------------------------------------------
# Check the Zeppelin logs.
#[root@ansibler]

    ssh zeppelin \
        '
        cat /home/fedora/zeppelin/logs/zeppelin-interpreter-spark-$(id -un)-$(hostname).log
        '


    >   ....
    >   ERROR [2021-08-03 02:23:12,135] ({YARN application state monitor} Logging.scala[logError]:70) -
    >       Diagnostics message: Uncaught exception: org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException:
    >           Invalid resource request, requested resource type=[vcores] < 0 or greater than maximum allowed allocation.
    >               Requested resource=<memory:8192, vCores:5>, maximum allowed allocation=<memory:9216, vCores:4>,
    >               please note that maximum allowed allocation is calculated by scheduler based on maximum resource of registered NodeManagers,
    >               which might be less than configured maximum allocation=<memory:9216, vCores:4>
    >   ....


    >   ....
    >    WARN [2021-08-03 02:46:46,317] ({pool-2-thread-2} Logging.scala[logWarning]:66) - The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.
    >    WARN [2021-08-03 02:46:46,317] ({pool-2-thread-2} Logging.scala[logWarning]:66) - The configuration key 'spark.yarn.driver.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.driver.memoryOverhead' instead.
    >   ....


# -----------------------------------------------------
# Check the Zeppelin config.
#[root@ansibler]

    ssh zeppelin

        less /opt/hadoop/etc/hadoop/yarn-site.xml
        less /opt/hadoop/etc/hadoop/capacity-scheduler.xml
        less /opt/hadoop/etc/hadoop/yarn-env.sh

        grep -r '4' /opt/hadoop/etc

    ssh worker01

        less /opt/hadoop/etc/hadoop/yarn-site.xml
        less /opt/hadoop/etc/hadoop/capacity-scheduler.xml
        less /opt/hadoop/etc/hadoop/yarn-env.sh

        grep -r '4' /opt/hadoop/etc


    # Ahhhhh
    # https://community.cloudera.com/t5/Support-Questions/yarn-nodemanager-resource-cpu-vcores-and-yarn-scheduler/m-p/35503/highlight/true#M36177
    # Yes... it doesn't make sense... I bumped into the same question, and the default of "yarn.scheduler.maximum-allocation-vcores" is actually 4 in the code.
    # I fired a JIRA for this.
    # https://issues.apache.org/jira/browse/YARN-4499

    Fix mismatch in default values for yarn.scheduler.maximum-allocation-vcores property
    https://issues.apache.org/jira/browse/YARN-3823

    Bad config values of "yarn.scheduler.maximum-allocation-vcores"
    https://issues.apache.org/jira/browse/YARN-4499





    ssh zeppelin \
        '
        rm -f /home/fedora/zeppelin/logs/zeppelin-interpreter-spark-$(id -un)-$(hostname).log

        /home/fedora/zeppelin/bin/zeppelin-daemon.sh restart
        '

    # Login

    # Run notebook

    ssh zeppelin \
        '
        cat /home/fedora/zeppelin/logs/zeppelin-interpreter-spark-$(id -un)-$(hostname).log
        '

    >    ....
    >    INFO [2021-08-03 04:05:13,333] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 1 (epoch 0)
    >   ERROR [2021-08-03 04:05:13,335] ({rpc-server-4-1} TransportClient.java[operationComplete]:336) - Failed to send RPC RPC 7727111747369638429 to /10.10.2.143:42400: java.nio.channels.ClosedChannelException
    >   java.nio.channels.ClosedChannelException
    >       ....
    >       ....
    >    WARN [2021-08-03 04:05:13,339] ({dispatcher-event-loop-13} Logging.scala[logWarning]:87) - Attempted to get executor loss reason for executor id 1 at RPC address 10.10.1.141:48502, but got no response. Marking as slave lost.
    >   java.io.IOException: Failed to send RPC RPC 7727111747369638429 to /10.10.2.143:42400: java.nio.channels.ClosedChannelException
    >    ....


    ssh master01 \
        '
        cat /var/hadoop/logs/hadoop-fedora-resourcemanager-gaia-dev-20210802-master01.novalocal.log
        '

    >   ....
    >   2021-08-03 04:05:37,433 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=fedora   OPERATION=AM Released Container TARGET=SchedulerApp     RESULT=SUCCESS  APPID=application_1627962913917_0006    CONTAINERID=container_1627962913917_0006_02_000021      RESOURCE=<memory:8192, vCores:1>        QUEUENAME=default
    >   2021-08-03 04:05:37,433 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1627962913917_0006_02_000020 Container Transitioned from ACQUIRED to KILLED
    >   ....
    >   2021-08-03 04:05:38,100 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Unknown application application_1627962913917_0006 launched container container_1627962913917_0006_02_000020 on node: host: worker05:46731 #containers=2 available=<memory:28672, vCores:6> used=<memory:16384, vCores:2>
    >   2021-08-03 04:05:38,134 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Unknown application application_1627962913917_0006 launched container container_1627962913917_0006_02_000015 on node: host: worker06:39249 #containers=2 available=<memory:28672, vCores:6> used=<memory:16384, vCores:2>
    >   2021-08-03 04:05:38,134 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Unknown application application_1627962913917_0006 launched container container_1627962913917_0006_02_000021 on node: host: worker06:39249 #containers=2 available=<memory:28672, vCores:6> used=<memory:16384, vCores:2>
    >   ....
    >   2021-08-03 04:05:38,340 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Container container_1627962913917_0006_02_000003 completed with event FINISHED, but corresponding RMContainer doesn't exist.
    >   ....


    ssh worker01 \
        '
        cat /var/hadoop/logs/application_1627962913917_0006/container_1627962913917_0006_02_000019
        '


