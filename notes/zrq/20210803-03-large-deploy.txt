#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Deploy a large system.

    Result:

        Working, but not optimised.

        Jira issue
        https://issues.apache.org/jira/browse/YARN-4714

            "In our Hadoop 2 + Java8 effort , we found few jobs are being Killed by Hadoop due to excessive virtual memory allocation.
            The most common error message is "Container [pid=??,containerID=container_??] is running beyond virtual memory limits.
            Current usage: 365.1 MB of 1 GB physical memory used; 3.2 GB of 2.1 GB virtual memory used. Killing container."


        StackOverflow fix:
        https://stackoverflow.com/questions/38988941/running-yarn-with-spark-not-working-with-java-8
        https://stackoverflow.com/a/39456782

            <property>
                <name>yarn.nodemanager.pmem-check-enabled</name>
                <value>false</value>
            </property>

            <property>
                <name>yarn.nodemanager.vmem-check-enabled</name>
                <value>false</value>
            </property>


# -----------------------------------------------------
# Checkout this branch.
#[user@desktop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

        git checkout '20210702-zrq-prometheus'

    popd


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --publish 3000:3000 \
        --publish 8088:8088 \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        atolmis/ansible-client:2020.12.02 \
        bash


# -----------------------------------------------------
# Set the target cloud.
#[root@ansibler]

    cloudname=gaia-dev


# -----------------------------------------------------
# Create our Ansible vars file.
#[root@ansibler]

    configyml='/tmp/aglais-config.yml'
    statusyml='/tmp/aglais-status.yml'

    cat > "${statusyml:?}" << EOF
aglais:
  status:
    deployment:
      type: hadoop-yarn
      conf: cclake-large-xx
      name: gaia-dev-20210803
      date: 20210803T094519
  spec:
    openstack:
      cloud: gaia-dev
EOF

    ln -sf \
        "${statusyml:?}" \
        '/tmp/ansible-vars.yml'


# -----------------------------------------------------
# Read the config settings.
#[root@ansibler]

    deployconf=$(
        yq read \
            "${statusyml:?}" \
            'aglais.status.deployment.conf'
        )

    deployname=$(
        yq read \
            "${statusyml:?}" \
            'aglais.status.deployment.name'
        )

    deploydate=$(
        yq read \
            "${statusyml:?}" \
            'aglais.status.deployment.date'
        )

# -----------------------------------------------------
# Delete any existing known hosts file..
# Temp fix until we get a better solution.
# https://github.com/wfau/aglais/issues/401
#[root@ansibler]

    rm -f "${HOME}/.ssh/known_hosts"


# -----------------------------------------------------
# Run the Ansible ssh playbook.
#[root@ansibler]

    pushd '/deployments/hadoop-yarn/ansible'

        ansible-playbook \
            --verbose \
            --inventory "config/${deployconf}.yml" \
            '05-config-ssh.yml'

        ansible-playbook \
            --verbose \
            --inventory "config/${deployconf}.yml" \
            "08-ping-test.yml"

    popd


# -----------------------------------------------------
# Configure Hadoop, Spark and Zeppelin
#[root@ansibler]

    pushd '/deployments/hadoop-yarn/ansible'

        ansible-playbook \
            --verbose \
            --inventory "config/${deployconf}.yml" \
            '16-config-yarn-masters.yml'

        ansible-playbook \
            --verbose \
            --inventory "config/${deployconf}.yml" \
            '17-config-yarn-workers.yml'

        ansible-playbook \
            --verbose \
            --inventory "config/${deployconf}.yml" \
            '22-config-spark-master.yml'

    popd


# -----------------------------------------------------
# Restart the services to recognise changes.
#[root@ansibler]

    ssh master01 \
        '
        /opt/hadoop/sbin/stop-all.sh

        echo ""
        echo "Pause ...."
        sleep 30
        echo ""

        /opt/hadoop/sbin/start-all.sh
        '

    >   WARNING: Stopping all Apache Hadoop daemons as fedora in 10 seconds.
    >   WARNING: Use CTRL-C to abort.
    >   Stopping namenodes on [master01]
    >   Stopping datanodes
    >   Stopping secondary namenodes [gaia-dev-20210803-master01.novalocal]
    >   Stopping nodemanagers
    >   Stopping resourcemanager

    >   WARNING: Attempting to start all Apache Hadoop daemons as fedora in 10 seconds.
    >   WARNING: This is not a recommended production deployment configuration.
    >   WARNING: Use CTRL-C to abort.
    >   Starting namenodes on [master01]
    >   Starting datanodes
    >   Starting secondary namenodes [gaia-dev-20210803-master01.novalocal]
    >   Starting resourcemanager
    >   Starting nodemanagers


    ssh zeppelin \
        '
        /home/fedora/zeppelin/bin/zeppelin-daemon.sh restart
        '

    >   Zeppelin stop                                              [  OK  ]
    >   Zeppelin start                                             [  OK  ]


# -----------------------------------------------------
# -----------------------------------------------------
# Add our secret function to the ansibler container.
#[root@ansibler]

    # TODO Move this into the Ansible setup.
    # TODO Move our secrets onto our infra-ops server.

    if [ ! -e "${HOME}/bin" ]
    then
        mkdir "${HOME}/bin"
    fi

    cat > "${HOME}/bin/secret" << 'EOF'
ssh -n \
    'secretserver' \
    "bin/secret '${1}'"
EOF

    chmod u+x "${HOME}/bin/secret"


    if [ ! -e "${HOME}/.ssh" ]
    then
        mkdir "${HOME}/.ssh"
    fi

    # Fix for the out of date ssh server.
    # RSA key have been deprecated, so we need to explicitly allow them for now.
    # https://www.reddit.com/r/Fedora/comments/jh9iyi/f33_openssh_no_mutual_signature_algorithm/

    cat >> "${HOME}/.ssh/config" << 'EOF'
Host secretserver
  User     Zarquan
  Hostname data.metagrid.co.uk
  PubkeyAcceptedKeyTypes +ssh-rsa
EOF

    ssh-keyscan 'data.metagrid.co.uk' >> "${HOME}/.ssh/known_hosts"

    secret frog

    >   Green Frog


# -----------------------------------------------------
# Create shell script functions to wrap the REST API.
#[root@ansibler]

    # TODO Add this to the client container.
    # https://github.com/wfau/aglais/issues/542
    # https://github.com/wfau/aglais/issues/495
    dnf install -y dateutils

    zeppelinhost=zeppelin.${cloudname:?}.aglais.uk
    zeppelinport=8080
    zeppelinurl="http://${zeppelinhost:?}:${zeppelinport:?}"

    zeplogin()
        {
        local username=${1:?}
        local password=${2:?}
        zepcookies=/tmp/${username:?}.cookies
        curl \
            --silent \
            --request 'POST' \
            --cookie-jar "${zepcookies:?}" \
            --data "userName=${username:?}" \
            --data "password=${password:?}" \
            "${zeppelinurl:?}/api/login" \
        | jq '.'
        }

    zepnbjsonfile()
        {
        local nbident=${1:?}
        echo "/tmp/${nbident:?}.json"
        }

    zepnbjsonclr()
        {
        local nbident=${1:?}
        local jsonfile=$(zepnbjsonfile ${nbident})
        if [ -f "${jsonfile}" ]
        then
            rm -f "${jsonfile}"
        fi
        }

    zepnbclear()
        {
        local nbident=${1:?}
        zepnbjsonclr ${nbident}
        curl \
            --silent \
            --request PUT \
            --cookie "${zepcookies:?}" \
            "${zeppelinurl:?}/api/notebook/${nbident:?}/clear" \
        | jq '.'
        }

    zepnbstatus()
        {
        local nbident=${1:?}
        zepnbjsonclr ${nbident}
        curl \
            --silent \
            --request GET \
            --cookie "${zepcookies:?}" \
            "${zeppelinurl:?}/api/notebook/${nbident:?}" \
        | jq '.' | tee $(zepnbjsonfile ${nbident}) | jq 'del(.body.paragraphs[])'
        }

    zepnbexecute()
        {
        local nbident=${1:?}
        zepnbjsonclr ${nbident}
        curl \
            --silent \
            --request POST \
            --cookie "${zepcookies:?}" \
            "${zeppelinurl:?}/api/notebook/job/${nbident:?}" \
        | jq '.'
        }


# -----------------------------------------------------
# Execute a notebook paragraph at a time.
#[root@ansibler]

    zepnbexecstep()
        {
        local nbident=${1:?}
        zepnbjsonclr ${nbident}

        # Fetch the notbook details.
        curl \
            --silent \
            --request GET \
            --cookie "${zepcookies:?}" \
            "${zeppelinurl:?}/api/notebook/${nbident:?}" \
            > $(zepnbjsonfile ${nbident})


        # List the title, status and ident.
        paralist=$(mktemp --suffix '.json')
        jq '
            [.body.paragraphs[]? | {id, status, title}]
            ' "$(zepnbjsonfile ${nbident})" \
            > "${paralist}"


        # Execute each paragraph
        jq -r '.[] | @text' "${paralist}" \
        | while read line
            do
                title=$(jq -r '.title' <<< "${line}")
                paraid=$(jq -r '.id'   <<< "${line}")
                status=$(jq -r '.status' <<< "${line}")
                echo ""
                echo "Para [${paraid}][${title}]"

                curl \
                    --silent \
                    --request POST \
                    --cookie "${zepcookies:?}" \
                    "${zeppelinurl:?}/api/notebook/run/${nbident:?}/${paraid:?}" \
                | jq 'del(.body.msg[])' \
                | tee "/tmp/para-${paraid}.json"


                result=$(
                    jq -r '.body.code' "/tmp/para-${paraid}.json"
                    )
                echo "Result [${result}]"

                if [ "${result}" != 'SUCCESS' ]
                then
                    break
                fi

            done
        }

# -----------------------------------------------------
# Calculate the elapsed time for each paragraph.
#[root@ansibler]

    zepnbparatime()
        {
        local nbident=${1:?}

        cat $(zepnbjsonfile ${nbident}) \
        | sed '
            /"dateStarted": null,/d
            /"dateStarted":/ {
                h
                s/\([[:space:]]*\)"dateStarted":[[:space:]]*\("[^"]*"\).*$/\1\2/
                x
                }
            /"dateFinished": null,/ d
            /"dateFinished":/ {
                H
                x
                s/[[:space:]]*"dateFinished":[[:space:]]*\("[^"]*"\).*$/ \1/
                s/\([[:space:]]*\)\(.*\)/\1echo "\1\\"elapsedTime\\": \\"$(datediff --format "%H:%M:%S" --input-format "%b %d, %Y %H:%M:%S %p" \2)\\","/e
                x
                G
                }
            ' \
        | jq '
            .body.paragraphs[] | select(.results.code != null) | {
                title,
                result: .results.code,
                time:   .elapsedTime,
                }
            '
        }

# -----------------------------------------------------
# Calculate the elapsed time for the whole notebook.
#[root@ansibler]

    zepnbtotaltime()
        {
        local nbident=${1:?}
        local jsonfile=$(zepnbjsonfile ${nbident})

        local first=$(
            jq -r '
                [.body.paragraphs[] | select(.dateStarted != null) | .dateStarted] | first
                ' \
                "${jsonfile}"
            )

        local last=$(
            jq -r '
                [.body.paragraphs[] | select(.dateFinished != null) | .dateFinished] | last
                ' \
                "${jsonfile}"
            )

        datediff --format "%H:%M:%S" --input-format "%b %d, %Y %H:%M:%S %p" "${first}" "${last}"
        }

# -----------------------------------------------------
# Login to Zeppelin as a normal user.
#[root@ansibler]

    gaiauser=$(secret aglais.zeppelin.gaiauser)
    gaiapass=$(secret aglais.zeppelin.gaiapass)

    zeplogin "${gaiauser:?}" "${gaiapass}"

    >   {
    >     "status": "OK",
    >     "message": "",
    >     "body": {
    >       "principal": "gaiauser",
    >       "ticket": "06397ea5-4f3d-49b0-9e36-8fccb63a7a56",
    >       "roles": "[\"user\"]"
    >     }
    >   }


# -----------------------------------------------------
# Run the SetUp notebook.
#[root@ansibler]

    noteid=2G7GZKWUH

    zepnbclear ${noteid}

    zepnbexecstep ${noteid}

    >   Para [20210504-130917_57061499][null]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": [
    >         {
    >           "type": "HTML"
    >         }
    >       ]
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210504-131126_1544574772][Catalogue structure definitions]
    >   {
    >     "status": "INTERNAL_SERVER_ERROR",
    >     "body": {
    >       "code": "ERROR",
    >       "msg": [
    >         {
    >           "type": "TEXT",
    >           "data": "java.lang.IllegalStateException: Spark context stopped while waiting for backend ...."
    >         }
    >       ]
    >     }
    >   }
    >   Result [ERROR]

    >   java.lang.IllegalStateException: Spark context stopped while waiting for backend




    ssh worker02 \
        '
        less /var/hadoop/logs/hadoop-fedora-nodemanager-$(hostname).log
        '

    >   ....
    >   2021-08-03 15:59:20,435 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1628005923728_0003_01_000009 transitioned from SCHEDULED to RUNNING
    >   2021-08-03 15:59:20,435 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1628005923728_0003_01_000002 transitioned from SCHEDULED to RUNNING
    >   2021-08-03 15:59:20,436 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1628005923728_0003_01_000009
    >   2021-08-03 15:59:20,436 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1628005923728_0003_01_000002
    >   2021-08-03 15:59:20,439 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /var/hadoop/data/usercache/fedora/appcache/application_1628005923728_0003/container_1628005923728_0003_01_000002/default_container_executor.sh]
    >   2021-08-03 15:59:20,439 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /var/hadoop/data/usercache/fedora/appcache/application_1628005923728_0003/container_1628005923728_0003_01_000009/default_container_executor.sh]
    >   2021-08-03 15:59:21,241 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1628005923728_0003_01_000002 transitioned from RUNNING to KILLING
    >   2021-08-03 15:59:21,241 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1628005923728_0003_01_000009 transitioned from RUNNING to KILLING
    >   2021-08-03 15:59:21,241 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1628005923728_0003_01_000002
    >   ....

    ssh worker01 \
        '
        less /var/hadoop/logs/hadoop-fedora-nodemanager-$(hostname).log
        '

    >   ....
    >   2021-08-03 16:00:02,509 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:
    >       Container [pid=9605,containerID=container_1628005923728_0005_02_000001] is running 128969216B beyond the 'VIRTUAL' memory limit.
    >       Current usage: 376.6 MB of 1 GB physical memory used; 2.2 GB of 2.1 GB virtual memory used. Killing container.
    >   ....


    #
    # This looks like this:
    # https://issues.apache.org/jira/browse/YARN-4714

        "In our Hadoop 2 + Java8 effort , we found few jobs are being Killed by Hadoop due to excessive virtual memory allocation.
        Although the physical memory usage is low.
        The most common error message is "Container [pid=??,containerID=container_??] is running beyond virtual memory limits.
        Current usage: 365.1 MB of 1 GB physical memory used; 3.2 GB of 2.1 GB virtual memory used. Killing container."


    # StackOverflow fix:
    # https://stackoverflow.com/questions/38988941/running-yarn-with-spark-not-working-with-java-8
    # https://stackoverflow.com/a/39456782

        <property>
            <name>yarn.nodemanager.pmem-check-enabled</name>
            <value>false</value>
        </property>

        <property>
            <name>yarn.nodemanager.vmem-check-enabled</name>
            <value>false</value>
        </property>


    # Not added the second part yet.

        I have built another answer which depends whether you are using spark client or cluster mode.

            In cluster mode it failed when I specified Driver Memory --driver-memory to be 512m. (The default setting requested 2GB of am resources (This consists of driver memory + Overhead requested for Application Master) which was enough)
            In client mode the setting that mattered was spark.yarn.am.memory as by default this requested only 1024m for the AM which is too little as Java 8 requires a lot of virtual memory. > 1024m seemed to be working.

        Answer is described here
        https://github.com/AndresNamm/SparkConfAndDebugging/blob/master/Debug/SparkMemoryIssue.md

# -----------------------------------------------------
# Configure Hadoop, Spark and Zeppelin
#[root@ansibler]

    pushd '/deployments/hadoop-yarn/ansible'

        ansible-playbook \
            --verbose \
            --inventory "config/${deployconf}.yml" \
            '16-config-yarn-masters.yml'

        ansible-playbook \
            --verbose \
            --inventory "config/${deployconf}.yml" \
            '17-config-yarn-workers.yml'

        ansible-playbook \
            --verbose \
            --inventory "config/${deployconf}.yml" \
            '22-config-spark-master.yml'

    popd


# -----------------------------------------------------
# Restart the services to recognise changes.
#[root@ansibler]

    ssh master01 \
        '
        /opt/hadoop/sbin/stop-all.sh

        echo ""
        echo "Pause ...."
        sleep 30
        echo ""

        /opt/hadoop/sbin/start-all.sh
        '

    >   Stopping namenodes on [master01]
    >   Stopping datanodes
    >   Stopping secondary namenodes [gaia-dev-20210803-master01.novalocal]
    >   Stopping nodemanagers
    >   Stopping resourcemanager

    >   Starting namenodes on [master01]
    >   Starting datanodes
    >   Starting secondary namenodes [gaia-dev-20210803-master01.novalocal]
    >   Starting resourcemanager
    >   Starting nodemanagers


    ssh zeppelin \
        '
        /home/fedora/zeppelin/bin/zeppelin-daemon.sh restart
        '

    >   Zeppelin stop                                              [  OK  ]
    >   Zeppelin start                                             [  OK  ]


# -----------------------------------------------------
# Login to Zeppelin as a normal user.
#[root@ansibler]

    gaiauser=$(secret aglais.zeppelin.gaiauser)
    gaiapass=$(secret aglais.zeppelin.gaiapass)

    zeplogin "${gaiauser:?}" "${gaiapass}"

    >   {
    >     "status": "OK",
    >     "message": "",
    >     "body": {
    >       "principal": "gaiauser",
    >       "ticket": "7c1dad89-8253-4538-b017-3937b06f0357",
    >       "roles": "[\"user\"]"
    >     }
    >   }


# -----------------------------------------------------
# Run the SetUp notebook.
#[root@ansibler]

    noteid=2G7GZKWUH

    zepnbclear ${noteid}

    zepnbexecstep ${noteid}

    >   Para [20210504-130917_57061499][null]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": [
    >         {
    >           "type": "HTML"
    >         }
    >       ]
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210504-131126_1544574772][Catalogue structure definitions]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210504-131319_1186301617][Utility function definitions]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210504-131439_625331903][Set up the catalogues on the platform]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": [
    >         {
    >           "type": "TEXT",
    >           "data": "...."
    >         }
    >       ]
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210504-132955_1641890430][Show details of databases and tables]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": [
    >         {
    >           "type": "TEXT",
    >           "data": "...."
    >         }
    >       ]
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210504-141425_1480464936][Check location on disk for main catalogue table from metastore]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": [
    >         {
    >           "type": "TEXT",
    >           "data": "...."
    >         }
    >       ]
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210521-084938_875368697][null]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]

    zepnbstatus    ${noteid}
    zepnbtotaltime ${noteid}

    >   {
    >     "status": "OK",
    >     "message": "",
    >     "body": {
    >       "paragraphs": [],
    >       "name": "/AglaisPublicExamples/SetUp",
    >       "id": "2G7GZKWUH",
    >       "noteParams": {},
    >       "noteForms": {},
    >       "angularObjects": {
    >         "md:shared_process": [],
    >         "spark:gaiauser:": []
    >       },
    >       "config": {
    >         "isZeppelinNotebookCronEnable": false
    >       },
    >       "info": {}
    >     }
    >   }

    >   0:0:39


# -----------------------------------------------------
# Run the HealpixSourceCounts notebook
#[root@ansibler]

    noteid=2FKJ25GVF

    zepnbclear    ${noteid}
    zepnbexecstep ${noteid}

    >
    >   Para [20210507-084613_357121151][null]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": [
    >         {
    >           "type": "HTML"
    >         }
    >       ]
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20200826-105718_1698521515][Set the resolution level and define the query]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20200826-110030_2095441495][Plot up the results]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": [
    >         {
    >           "type": "TEXT",
    >           "data": "...."
    >         },
    >         {
    >           "type": "IMG"
    >         }
    >       ]
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210507-091244_670006530][Further reading and resources]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": [
    >         {
    >           "type": "HTML"
    >         }
    >       ]
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20200826-110146_414730471][null]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]

    zepnbstatus    ${noteid}
    zepnbtotaltime ${noteid}

    >   {
    >     "status": "OK",
    >     "message": "",
    >     "body": {
    >       "paragraphs": [],
    >       "name": "/AglaisPublicExamples/Source counts over the sky",
    >       "id": "2FKJ25GVF",
    >       "noteParams": {},
    >       "noteForms": {},
    >       "angularObjects": {
    >         "md:shared_process": [],
    >         "spark:nch:": [],
    >         "sh:shared_process": []
    >       },
    >       "config": {
    >         "isZeppelinNotebookCronEnable": false
    >       },
    >       "info": {}
    >     }
    >   }

    >   0:0:26


# -----------------------------------------------------
# Run the MeanProperMotions notebook
#[root@ansibler]

    noteid=2G748GZSW

    zepnbclear    ${noteid}
    zepnbexecstep ${noteid}

    >   Para [20210510-111756_391695716][Set HEALPix resolution]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210510-111538_106023214][Define a data frame by SQL query]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210510-111939_1386609632][Mean RA proper motion plot]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": [
    >         {
    >           "type": "TEXT"
    >         },
    >         {
    >           "type": "IMG"
    >         }
    >       ]
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210510-111943_814907111][Mean Dec proper motion plot]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": [
    >         {
    >           "type": "TEXT"
    >         },
    >         {
    >           "type": "IMG"
    >         }
    >       ]
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210510-111956_1822284967][Further reading and resources]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": [
    >         {
    >           "type": "HTML"
    >         }
    >       ]
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210510-132447_1514402898][null]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": [
    >         {
    >           "type": "HTML"
    >         }
    >       ]
    >     }
    >   }
    >   Result [SUCCESS]

    zepnbstatus    ${noteid}
    zepnbtotaltime ${noteid}

    >   {
    >     "status": "OK",
    >     "message": "",
    >     "body": {
    >       "paragraphs": [],
    >       "name": "AglaisPublicExamples/Mean proper motions over the sky",
    >       "id": "2G748GZSW",
    >       "noteParams": {},
    >       "noteForms": {},
    >       "angularObjects": {
    >         "md:shared_process": [],
    >         "sh:shared_process": [],
    >         "spark:gaiauser:": []
    >       },
    >       "config": {
    >         "isZeppelinNotebookCronEnable": false
    >       },
    >       "info": {}
    >     }
    >   }

    >   0:0:49


# -----------------------------------------------------
# Run the RandomForest notebook.
#[root@ansibler]

    noteid=2G5NU6HTK

    zepnbclear    ${noteid}
    zepnbexecstep ${noteid}

    >   Para [20201013-131059_546082898][null]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20201013-131649_1734629667][Basic catalogue query selections and predicates]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20201013-132418_278702125][Raw catalogue with selected columns]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20201120-094650_221463065][Visualisation (colour / absolute-magnitue diagram) of the raw catalogue]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20201120-110502_1704727157][null]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20201123-105445_95907042][Define the training samples]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20201015-161110_18118893][Assemble training and reserve test sets]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20201013-152110_1282917873][Train up the Random Forrest]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210504-153521_1591875670][Check feature set for nulls]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20201015-131823_1744793710][Classify the reserved test sets]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20201016-154755_24366630][Classification confusion matrix]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20201123-163421_1811049882][Relative importance of the selected features]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20201123-162249_1468741293][Apply the classification model and plot sample results]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20201124-100512_110153564][Histogram of classification probability]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20201125-103046_1353183691][Sky distribution of good source sample]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20201125-163312_728555601][Sky distribution of bad source sample]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210428-140519_1288739408][Further reading and resources]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]
    >
    >   Para [20210506-134212_1741520795][null]
    >   {
    >     "status": "OK",
    >     "body": {
    >       "code": "SUCCESS",
    >       "msg": []
    >     }
    >   }
    >   Result [SUCCESS]

    zepnbstatus    ${noteid}
    zepnbtotaltime ${noteid}

    >   {
    >     "status": "OK",
    >     "message": "",
    >     "body": {
    >       "paragraphs": [],
    >       "name": "/AglaisPublicExamples/Good astrometric solutions via ML Random Forrest classifier",
    >       "id": "2G5NU6HTK",
    >       "noteParams": {},
    >       "noteForms": {},
    >       "angularObjects": {
    >         "md:shared_process": [],
    >         "sh:shared_process": [],
    >         "spark:gaiauser:": []
    >       },
    >       "config": {
    >         "isZeppelinNotebookCronEnable": false
    >       },
    >       "info": {}
    >     }
    >   }

    >   0:9:41




