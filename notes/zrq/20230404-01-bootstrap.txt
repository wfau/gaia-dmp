#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2023, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Initial bootstrap K8s cluster from nothing.

    Result:

        Work in progress ...


# -----------------------------------------------------
# Check which platform is live.
#[user@desktop]

    ssh fedora@live.gaia-dmp.uk \
        '
        date
        hostname
        '

    >   Tue  4 Apr 15:41:25 UTC 2023
    >   iris-gaia-green-20230308-zeppelin


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    #
    # Live is green, selecting red for the deployment.
    #

    source "${HOME:?}/aglais.env"

    agcolour=red

    clientname=ansibler-${agcolour}
    cloudname=iris-gaia-${agcolour}

    podman run \
        --rm \
        --tty \
        --interactive \
        --name     "${clientname:?}" \
        --hostname "${clientname:?}" \
        --env "cloudname=${cloudname:?}" \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK:?}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        ghcr.io/wfau/atolmis/ansible-client:2022.07.25 \
        bash

    >   ....
    >   ....


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"

    >   real    2m4.760s
    >   user    0m56.294s
    >   sys     0m6.073s


# -----------------------------------------------------
# Add YAML editor role to our client container.
#[root@ansibler]

    ansible-galaxy install kwoodson.yedit

    >   Starting galaxy role install process
    >   - downloading role 'yedit', owned by kwoodson
    >   - downloading role from https://github.com/kwoodson/ansible-role-yedit/archive/master.tar.gz
    >   - extracting kwoodson.yedit to /root/.ansible/roles/kwoodson.yedit
    >   - kwoodson.yedit (master) was installed successfully


# -----------------------------------------------------
# Create our deployment settings.
#[root@ansibler]

    deployname=${cloudname:?}-$(date '+%Y%m%d')
    deploydate=$(date '+%Y%m%dT%H%M%S')

    statusyml='/opt/aglais/aglais-status.yml'
    if [ ! -e "$(dirname ${statusyml})" ]
    then
        mkdir "$(dirname ${statusyml})"
    fi
    rm -f "${statusyml}"
    touch "${statusyml}"

    yq eval \
        --inplace \
        "
        .aglais.deployment.type = \"cluster-api\"   |
        .aglais.deployment.name = \"${deployname}\" |
        .aglais.deployment.date = \"${deploydate}\" |
        .aglais.openstack.cloud.name = \"${cloudname}\"
        " "${statusyml}"


# -----------------------------------------------------
# Create our bootstrap node.
#[root@ansibler]

    inventory=/deployments/cluster-api/bootstrap/ansible/config/inventory.yml

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/01-create-keypair.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/02-create-network.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/03-create-bootstrap.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/04-local-config.yml'

    >   ....
    >   ....
    >   PLAY RECAP **********************************************************************************************************
    >   localhost                  : ok=2    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   ....
    >   ....
    >   PLAY RECAP **********************************************************************************************************
    >   localhost                  : ok=4    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   ....
    >   ....
    >   PLAY RECAP **********************************************************************************************************
    >   localhost                  : ok=7    changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
    >   ....
    >   ....
    >   PLAY RECAP **********************************************************************************************************
    >   localhost                  : ok=4    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0


# -----------------------------------------------------
# Check our local config.
#[root@ansibler]

    cat /opt/aglais/aglais-status.yml

    >   aglais:
    >     deployment:
    >       date: 20230404T155040
    >       name: iris-gaia-red-20230404
    >       type: cluster-api
    >     openstack:
    >       cloud:
    >         name: iris-gaia-red
    >       keypairs:
    >         team:
    >           fingerprint: 2e:84:98:98:df:70:06:0e:4c:ed:bd:d4:d6:6b:eb:16
    >           id: iris-gaia-red-20230404-keypair
    >           name: iris-gaia-red-20230404-keypair
    >       networks:
    >         internal:
    >           network:
    >             id: 05750b83-08c6-4ad4-838b-4a47fbcf29d8
    >             name: iris-gaia-red-20230404-internal-network
    >           router:
    >             id: 4ce12070-96ac-44a7-b60e-c38f0dfe68aa
    >             name: iris-gaia-red-20230404-internal-router
    >           subnet:
    >             cidr: 10.10.0.0/16
    >             id: 043d114c-86f5-4267-8ec8-97a3e028bafb
    >             name: iris-gaia-red-20230404-internal-subnet
    >       servers:
    >         bootstrap:
    >           float:
    >             external: 128.232.227.17
    >             id: 63903d51-6e08-4bf6-82c8-460483b3496d
    >             internal: 10.10.3.127
    >           server:
    >             address:
    >               ipv4: 10.10.3.127
    >             flavor:
    >               name: gaia.vm.cclake.2vcpu
    >             hostname: bootstrap
    >             id: bfbbb840-ba12-4ff6-93ff-1a5afa16c962
    >             image:
    >               id: e5c23082-cc34-4213-ad31-ff4684657691
    >               name: Fedora-34.1.2
    >             name: iris-gaia-red-20230404-bootstrap


# -----------------------------------------------------
# SSH test.
#[root@ansibler]

    ssh bootstrap \
        '
        date
        hostname
        '

    >   Tue Apr  4 15:55:48 UTC 2023
    >   iris-gaia-red-20230404-bootstrap


# -----------------------------------------------------
# -----------------------------------------------------
# Login to the bootstrap node.
#[root@ansibler]

    ssh bootstrap

    >   ....
    >   ....


# -----------------------------------------------------
# Install Docker.
# https://docs.docker.com/engine/install/fedora/#install-using-the-repository
#[fedora@bootstrap]

    sudo dnf -y install dnf-plugins-core

    sudo dnf config-manager \
        --add-repo \
        https://download.docker.com/linux/fedora/docker-ce.repo

    >   ....
    >   ....
    >   Adding repo from: https://download.docker.com/linux/fedora/docker-ce.repo


    sudo dnf install \
        -y \
        docker-ce \
        docker-ce-cli \
        containerd.io \
        docker-compose-plugin

    >   ....
    >   ....
    >   Installed:
    >     ....
    >     ....
    >     docker-ce-3:20.10.17-3.fc34.x86_64


# -----------------------------------------------------
# Start the Docker service.
#[fedora@bootstrap]

    sudo systemctl start docker

    >   ....
    >   ....


# -----------------------------------------------------
# Install kubectl.
# https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-using-native-package-management
#[fedora@bootstrap]

cat > '/tmp/kubernetes.repo' << EOF
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

    sudo chown 'root:root' '/tmp/kubernetes.repo'
    sudo mv '/tmp/kubernetes.repo' '/etc/yum.repos.d/kubernetes.repo'

    sudo dnf install -y 'kubectl'

    >   ....
    >   ....
    >   Installed:
    >     kubectl-1.26.3-0.x86_64

    #
    # This is is slightly more recent than installing the Fedora package.
    # Although, we could try just using the Fedora package.
    # dnf install -y kubernetes-client
    #

# -----------------------------------------------------
# Install kind on the bootstrap node.
# https://kind.sigs.k8s.io/docs/user/quick-start/#installing-from-release-binaries
#[fedora@bootstrap]

    kindversion=0.17.0
    kindbinary=kind-${kindversion:?}
    kindtemp=/tmp/${kindbinary:?}

    curl \
        --location \
        --no-progress-meter \
        --output "${kindtemp:?}" \
        "https://kind.sigs.k8s.io/dl/v${kindversion:?}/kind-linux-amd64"

    pushd /usr/local/bin
        sudo mv "${kindtemp:?}" .
        sudo chown 'root:root' "${kindbinary:?}"
        sudo chmod 'u=rwx,g=rx,o=rx' "${kindbinary:?}"
        sudo ln -s "${kindbinary:?}" 'kind'
    popd

    >   ....
    >   ....


# -----------------------------------------------------
# Create a cluster, with logs.
# https://github.com/kubernetes-sigs/kind/pull/2478#issuecomment-1214656908
#[fedora@bootstrap]

    sudo kind create cluster --retain

    >   Creating cluster "kind" ...
    >    âœ“ Ensuring node image (kindest/node:v1.25.3) ðŸ–¼
    >    âœ“ Preparing nodes ðŸ“¦
    >    âœ“ Writing configuration ðŸ“œ
    >    âœ“ Starting control-plane ðŸ•¹ï¸
    >    âœ“ Installing CNI ðŸ”Œ
    >    âœ“ Installing StorageClass ðŸ’¾
    >   Set kubectl context to "kind-kind"
    >   ....
    >   ....


# -----------------------------------------------------
# Start to explore ....
#[fedora@bootstrap]

    sudo kubectl cluster-info --context kind-kind

    >   Kubernetes control plane is running at https://127.0.0.1:46203
    >   CoreDNS is running at https://127.0.0.1:46203/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
    >   ....
    >   ....

    # Sucks that we need to be root for all of this.
    # TODO Replace Docker with Podman and run our containers rootless.


# -----------------------------------------------------
# ....
#[fedora@bootstrap]

    sudo cat /root/.kube/config

    >   apiVersion: v1
    >   clusters:
    >   - cluster:
    >       certificate-authority-data: LS0tLS1C....0tLS0tCg==
    >       server: https://127.0.0.1:46203
    >     name: kind-kind
    >   contexts:
    >   - context:
    >       cluster: kind-kind
    >       user: kind-kind
    >     name: kind-kind
    >   current-context: kind-kind
    >   kind: Config
    >   preferences: {}
    >   users:
    >   - name: kind-kind
    >     user:
    >       client-certificate-data: LS0tLS1C....0tLS0tCg==
    >       client-key-data: LS0tLS1C....0tLS0tCg==

    # Lots of 'kind' things in all that ...

    >   clusters:
    >   - name: kind-kind
    >     ....
    >   contexts:
    >   - name: kind-kind
    >     ....
    >   ...
    >   users:
    >   - name: kind-kind


    # References from one 'kind' thing to all the others.

    >   ....
    >   contexts:
    >   - context:
    >       cluster: kind-kind
    >       user: kind-kind
    >     name: kind-kind
    >   current-context: kind-kind
    >   ....


    # .. but this is a different 'kind' of thing

    >   ....
    >   kind: Config
    >   ....


# -----------------------------------------------------
# ....
#[fedora@bootstrap]

    sudo kubectl get pods --all-namespaces

    >   NAMESPACE            NAME                                         READY   STATUS    RESTARTS   AGE
    >   kube-system          coredns-565d847f94-lks6r                     1/1     Running   0          5m53s
    >   kube-system          coredns-565d847f94-wf86h                     1/1     Running   0          5m53s
    >   kube-system          etcd-kind-control-plane                      1/1     Running   0          6m6s
    >   kube-system          kindnet-lfmqb                                1/1     Running   0          5m53s
    >   kube-system          kube-apiserver-kind-control-plane            1/1     Running   0          6m6s
    >   kube-system          kube-controller-manager-kind-control-plane   1/1     Running   0          6m6s
    >   kube-system          kube-proxy-xt6zc                             1/1     Running   0          5m53s
    >   kube-system          kube-scheduler-kind-control-plane            1/1     Running   0          6m6s
    >   local-path-storage   local-path-provisioner-684f458cdd-4jzzq      1/1     Running   0          5m53s


# -----------------------------------------------------
# Install Helm on the bootstrap node.
# https://helm.sh/docs/intro/install/
# https://github.com/helm/helm/releases
#[fedora@bootstrap]

    helmarch=linux-amd64
    helmversion=3.11.2
    helmtarfile=helm-v${helmversion}-${helmarch}.tar.gz
    helmtmpfile=/tmp/${helmtarfile:?}
    helmbinary=helm-${helmversion:?}

    curl \
        --location \
        --no-progress-meter \
        --output "${helmtmpfile:?}" \
        "https://get.helm.sh/${helmtarfile:?}"

    tar \
        --gzip \
        --extract \
        --directory /tmp \
        --file "${helmtmpfile:?}"

    pushd /usr/local/bin
        sudo mv "/tmp/${helmarch:?}/helm" "${helmbinary:?}"
        sudo chown 'root:root' "${helmbinary:?}"
        sudo chmod 'u=rwx,g=rx,o=rx' "${helmbinary:?}"
        sudo ln -s "${helmbinary:?}" 'helm'
    popd


    helm env

    >   HELM_BIN="helm"
    >   HELM_BURST_LIMIT="100"
    >   HELM_CACHE_HOME="/home/fedora/.cache/helm"
    >   HELM_CONFIG_HOME="/home/fedora/.config/helm"
    >   HELM_DATA_HOME="/home/fedora/.local/share/helm"
    >   HELM_DEBUG="false"
    >   HELM_KUBEAPISERVER=""
    >   HELM_KUBEASGROUPS=""
    >   HELM_KUBEASUSER=""
    >   HELM_KUBECAFILE=""
    >   HELM_KUBECONTEXT=""
    >   HELM_KUBEINSECURE_SKIP_TLS_VERIFY="false"
    >   HELM_KUBETLS_SERVER_NAME=""
    >   HELM_KUBETOKEN=""
    >   HELM_MAX_HISTORY="10"
    >   HELM_NAMESPACE="default"
    >   HELM_PLUGINS="/home/fedora/.local/share/helm/plugins"
    >   HELM_REGISTRY_CONFIG="/home/fedora/.config/helm/registry/config.json"
    >   HELM_REPOSITORY_CACHE="/home/fedora/.cache/helm/repository"
    >   HELM_REPOSITORY_CONFIG="/home/fedora/.config/helm/repositories.yaml"


    sudo helm env

    >   HELM_BIN="helm"
    >   HELM_BURST_LIMIT="100"
    >   HELM_CACHE_HOME="/root/.cache/helm"
    >   HELM_CONFIG_HOME="/root/.config/helm"
    >   HELM_DATA_HOME="/root/.local/share/helm"
    >   HELM_DEBUG="false"
    >   HELM_KUBEAPISERVER=""
    >   HELM_KUBEASGROUPS=""
    >   HELM_KUBEASUSER=""
    >   HELM_KUBECAFILE=""
    >   HELM_KUBECONTEXT=""
    >   HELM_KUBEINSECURE_SKIP_TLS_VERIFY="false"
    >   HELM_KUBETLS_SERVER_NAME=""
    >   HELM_KUBETOKEN=""
    >   HELM_MAX_HISTORY="10"
    >   HELM_NAMESPACE="default"
    >   HELM_PLUGINS="/root/.local/share/helm/plugins"
    >   HELM_REGISTRY_CONFIG="/root/.config/helm/registry/config.json"
    >   HELM_REPOSITORY_CACHE="/root/.cache/helm/repository"
    >   HELM_REPOSITORY_CONFIG="/root/.config/helm/repositories.yaml"


# -----------------------------------------------------
# Install StackHPC's Helm charts
# https://github.com/stackhpc/capi-helm-charts
#[fedora@bootstrap]

    helm repo add capi https://stackhpc.github.io/capi-helm-charts

    >   "capi" has been added to your repositories


    helm search repo capi --versions

    >   NAME                  	CHART VERSION	APP VERSION	DESCRIPTION
    >   capi/cluster-addons   	0.1.0        	75676de    	Helm chart that deploys cluster addons for a CA...
    >   capi/openstack-cluster	0.1.0        	75676de    	Helm chart for deploying a cluster on an OpenSt...



    StackHPC's README has a warning about the version of the Openstack Cluster API Provider.

    WARNING
    This chart depends on features in cluster-api-provider-openstack that are not yet in a release.
    StackHPC maintain custom builds of cluster-api-provider-openstack for use with this chart. You can find these in the StackHPC fork of cluster-api-provider-openstack.

    Checking StackHPC's fork of the Openstack Cluster API Provider.
    https://github.com/stackhpc/cluster-api-provider-openstack

    The main branch is behind the upstream kubernetes repository.
    My guess is that the changes have been merged and this is no longer needed.

        Prerequisites

            First, you must set up a [Cluster API management cluster][1] with the [OpenStack Infrastructure Provider][3] installed.

            Addons are managed by the Cluster API Addon Provider, which must also be installed if you wish to use the addons functionality.

            In addition, Helm must be installed and configured to access your management cluster, and the chart repository containing this chart must be configured:
                helm repo add capi https://stackhpc.github.io/capi-helm-charts

    [1] Management cluster
    https://cluster-api.sigs.k8s.io/user/concepts.html#management-cluster

        A Kubernetes cluster that manages the lifecycle of Workload Clusters.
        A Management Cluster is also where one or more providers run, and where resources such as Machines are stored.

    [2] Infrastructure provider
    https://cluster-api.sigs.k8s.io/user/concepts.html#infrastructure-provider

        A component responsible for the provisioning of infrastructure/computational resources required by the Cluster
        or by Machines (e.g. VMs, networking, etc.). For example, cloud Infrastructure Providers include AWS, Azure,
        and Google, and bare metal Infrastructure Providers include VMware, MAAS, and metal3.io.

    [3] Kubernetes Cluster API Provider OpenStack
    https://github.com/kubernetes-sigs/cluster-api-provider-openstack

        Documentation
        Please see our book for in-depth documentation.
        https://cluster-api-openstack.sigs.k8s.io/


# -----------------------------------------------------
# Install clusterctl
# The clusterctl CLI tool handles the lifecycle of a Cluster API management cluster.
# https://cluster-api-openstack.sigs.k8s.io/getting-started.html#install-clusterctl
#[fedora@bootstrap]

    cctlversion=1.4.1
    cctlbinary=clusterctl-${cctlversion:?}

    curl \
        --location \
        --no-progress-meter \
        --output "/tmp/${cctlbinary:?}" \
        "https://github.com/kubernetes-sigs/cluster-api/releases/download/v${cctlversion:?}/clusterctl-linux-amd64"

    sudo install \
        --owner 'root' \
        --group 'root' \
        --mode  'u=,g=rwx,o=rx' \
        "/tmp/${cctlbinary:?}" \
        "/usr/local/bin/"

    pushd "/usr/local/bin"
        sudo ln -s "${cctlbinary:?}" "clusterctl"
    popd

    clusterctl version

    >   clusterctl version: &version.Info
    >       {
    >       Major:"1",
    >       Minor:"4",
    >       GitVersion:"v1.4.1",
    >       GitCommit:"39d87e91080088327c738c43f39e46a7f557d03b",
    >       GitTreeState:"clean",
    >       BuildDate:"2023-04-04T17:31:43Z",
    >       GoVersion:"go1.19.6",
    >       Compiler:"gc",
    >       Platform:"linux/amd64"
    >       }


# -----------------------------------------------------
# Initialize the Openstack management cluster
# https://cluster-api-openstack.sigs.k8s.io/getting-started.html#initialization-for-common-providers
#[fedora@bootstrap]

    sudo clusterctl init --infrastructure openstack

    >   Fetching providers
    >   Installing cert-manager Version="v1.11.0"
    >   Waiting for cert-manager to be available...
    >   Installing Provider="cluster-api" Version="v1.4.1" TargetNamespace="capi-system"
    >   Installing Provider="bootstrap-kubeadm" Version="v1.4.1" TargetNamespace="capi-kubeadm-bootstrap-system"
    >   Installing Provider="control-plane-kubeadm" Version="v1.4.1" TargetNamespace="capi-kubeadm-control-plane-system"
    >   Installing Provider="infrastructure-openstack" Version="v0.7.1" TargetNamespace="capo-system"
    >
    >   Your management cluster has been initialized successfully!
    >
    >   You can now create your first workload cluster by running the following:
    >
    >     clusterctl generate cluster [name] --kubernetes-version [version] | kubectl apply -f -


    sudo kubectl get pods --all-namespaces

    >   NAMESPACE                           NAME                                                             READY   STATUS    RESTARTS   AGE
    >   capi-kubeadm-bootstrap-system       capi-kubeadm-bootstrap-controller-manager-8654485994-m99zc       1/1     Running   0          69s
    >   capi-kubeadm-control-plane-system   capi-kubeadm-control-plane-controller-manager-5d9d9494d5-65cv9   1/1     Running   0          68s
    >   capi-system                         capi-controller-manager-746b4f5db4-p5r8g                         1/1     Running   0          71s
    >   capo-system                         capo-controller-manager-775d744795-r7wqz                         1/1     Running   0          65s
    >   cert-manager                        cert-manager-99bb69456-fdb77                                     1/1     Running   0          99s
    >   cert-manager                        cert-manager-cainjector-ffb4747bb-hd555                          1/1     Running   0          99s
    >   cert-manager                        cert-manager-webhook-545bd5d7d8-ktd9w                            1/1     Running   0          99s
    >   kube-system                         coredns-565d847f94-lks6r                                         1/1     Running   0          94m
    >   kube-system                         coredns-565d847f94-wf86h                                         1/1     Running   0          94m
    >   kube-system                         etcd-kind-control-plane                                          1/1     Running   0          94m
    >   kube-system                         kindnet-lfmqb                                                    1/1     Running   0          94m
    >   kube-system                         kube-apiserver-kind-control-plane                                1/1     Running   0          94m
    >   kube-system                         kube-controller-manager-kind-control-plane                       1/1     Running   0          94m
    >   kube-system                         kube-proxy-xt6zc                                                 1/1     Running   0          94m
    >   kube-system                         kube-scheduler-kind-control-plane                                1/1     Running   0          94m
    >   local-path-storage                  local-path-provisioner-684f458cdd-4jzzq                          1/1     Running   0          94m


# -----------------------------------------------------
# Create a workload cluster
# https://cluster-api-openstack.sigs.k8s.io/getting-started.html#required-configuration-for-common-providers
#[fedora@bootstrap]

    #
    # List the environment variables we would need to set.
    #

    clusterctl generate cluster --infrastructure openstack --list-variables capi-quickstart

    >   Required Variables:
    >     - KUBERNETES_VERSION
    >     - OPENSTACK_CLOUD
    >     - OPENSTACK_CLOUD_CACERT_B64
    >     - OPENSTACK_CLOUD_PROVIDER_CONF_B64
    >     - OPENSTACK_CLOUD_YAML_B64
    >     - OPENSTACK_CONTROL_PLANE_MACHINE_FLAVOR
    >     - OPENSTACK_DNS_NAMESERVERS
    >     - OPENSTACK_EXTERNAL_NETWORK_ID
    >     - OPENSTACK_FAILURE_DOMAIN
    >     - OPENSTACK_IMAGE_NAME
    >     - OPENSTACK_NODE_MACHINE_FLAVOR
    >     - OPENSTACK_SSH_KEY_NAME
    >
    >   Optional Variables:
    >     - CLUSTER_NAME                 (defaults to capi-quickstart)
    >     - CONTROL_PLANE_MACHINE_COUNT  (defaults to 1)
    >     - WORKER_MACHINE_COUNT         (defaults to 0)


# -----------------------------------------------------
# -----------------------------------------------------
# List the available VM flavors and images.
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
        flavor list

    >   +--------------------------------------+-----------------------------+--------+------+-----------+-------+-----------+
    >   | ID                                   | Name                        |    RAM | Disk | Ephemeral | VCPUs | Is Public |
    >   +--------------------------------------+-----------------------------+--------+------+-----------+-------+-----------+
    >   | 0997c60d-3460-432a-a7fc-78d2cd466b4c | gaia.vm.cclake.26vcpu       |  44032 |   20 |       180 |    26 | False     |
    >   | 0bba49a9-a11f-45cb-ad1b-09527bc0e991 | gaia.vm.cclake.himem.12vcpu |  43008 |   20 |        80 |    12 | False     |
    >   | 166497c3-a0bb-4276-bee3-e56932e6f3e4 | gaia.vm.cclake.1vcpu        |   1024 |    8 |         0 |     1 | False     |
    >   | 19754fec-4177-4468-99a0-554a0caed37f | gaia.vm.cclake.himem.1vcpu  |   2048 |    8 |         0 |     1 | False     |
    >   | 2e5dc624-1d3b-4da7-8107-cc2dd4cb5073 | vm.v1.large                 |  32768 |   60 |         0 |     8 | True      |
    >   | 56c420d5-abea-41da-9863-f5bc08b08430 | gaia.vm.cclake.54vcpu       |  88064 |   20 |       380 |    54 | False     |
    >   | 58c86aeb-be90-4958-8990-89709fee00b1 | gaia.vm.cclake.himem.2vcpu  |   6144 |   14 |         0 |     2 | False     |
    >   | 6793b213-5efa-4b51-96bf-1340ff066499 | vm.v1.xsmall                |   2048 |   20 |         0 |     1 | True      |
    >   | 698a8d46-eceb-4c55-91ff-38286bf9eabb | vm.v1.tiny                  |   1024 |   10 |         0 |     1 | True      |
    >   | 6b56d6e9-5397-4543-87fb-e0c0b6d47961 | vm.v1.small                 |  16384 |   20 |         0 |     4 | True      |
    >   | 80e0721d-db0f-407f-a2bf-fe6641312204 | gaia.vm.cclake.4vcpu        |   6144 |   22 |         0 |     4 | False     |
    >   | a1b2789c-761a-4843-8ea8-603a9209dec8 | gaia.vm.cclake.6vcpu        |   9216 |   20 |        24 |     6 | False     |
    >   | a61ccf32-a9cf-4c23-9f00-dff5ebacf0cd | gaia.vm.cclake.himem.54vcpu | 176128 |   20 |       380 |    54 | False     |
    >   | b091654c-428e-47c9-a7f3-b69900b98bea | gaia.vm.cclake.himem.26vcpu |  88064 |   20 |       180 |    26 | False     |
    >   | b80c05db-da78-4172-ade3-dd3f500c5076 | C2.vss.xlarge               |  12288 |  180 |         0 |    12 | True      |
    >   | bd2eb2e7-baf9-4a73-9bb1-a5559964c9be | gaia.vm.cclake.himem.4vcpu  |  12288 |   22 |         0 |     4 | False     |
    >   | df5133ea-1bfb-45fd-ba39-71fc820abcb1 | gaia.vm.cclake.2vcpu        |   3072 |   14 |         0 |     2 | False     |
    >   | ef01ce36-283f-4df3-a039-1b47504de078 | gaia.vm.cclake.12vcpu       |  21504 |   20 |        80 |    12 | False     |
    >   | fbbf4183-c727-4fd3-a3bf-7aa08cb45210 | gaia.vm.cclake.himem.6vcpu  |  18432 |   20 |        24 |     6 | False     |
    >   +--------------------------------------+-----------------------------+--------+------+-----------+-------+-----------+


    openstack \
        --os-cloud "${cloudname:?}" \
        image list

    >   +--------------------------------------+------------------------------------------------+--------+
    >   | ID                                   | Name                                           | Status |
    >   +--------------------------------------+------------------------------------------------+--------+
    >   | 0f242b58-0563-46c5-b357-64b46cf46030 | AlmaLinux-8.5-20211119                         | active |
    >   | 490eb74c-11ed-4dc3-bd94-68d7cb3a9220 | CentOS-Stream-GenericCloud-8-20220913.0.x86_64 | active |
    >   | cdd05545-8373-4af8-b31a-9d6bea5d9d79 | CentOS7-1907                                   | active |
    >   | 0757b325-9d08-41d7-a101-dddf4a055507 | CentOS7-2009                                   | active |
    >   | 15209bdf-fe99-4447-9879-7895492dd86e | CentOS7-2111                                   | active |
    >   | 913904b6-fff6-4644-bc7c-1061f4ec1988 | CentOS7.9-OFED-5.4-1.0.3.0                     | active |
    >   | 9ae4e489-fef6-44b4-aaf9-ba24b51ff545 | CentOS8-2011                                   | active |
    >   | 9ab38ae7-61ee-4de7-aa95-138c7b4b916f | CentOS8-2105                                   | active |
    >   | ec7c8e40-ef9e-43f3-a960-afd495117f40 | CentOS8.4-OFED-5.4-1.0.3.0                     | active |
    >   | c6ab1a0f-7d3c-4c3d-b05a-c820cd921f8b | Cirros-0.5.1                                   | active |
    >   | 65ba7d67-ccdb-4d32-bd64-7ceb6ae15805 | Debian-Bullseye-11.0.0                         | active |
    >   | 854e54a4-2d43-462e-b565-aef8b17ed94e | Debian-Buster-10.10.0                          | active |
    >   | d264ae9a-bef7-4738-b1d9-18eadc4cc244 | Debian-Buster-10.10.3                          | active |
    >   | 79ab3e50-6f97-4f15-955d-6b0517f1d562 | Debian-Buster-10.2.0                           | active |
    >   | e70a961c-e7ff-490d-87d4-89377c44b874 | Debian-Buster-10.7.4                           | active |
    >   | f73a0735-6a67-4407-8436-9ad54822e6f8 | Debian-Stretch-9.11.6                          | active |
    >   | 5c17f6d4-6201-4436-aa08-6b71c6ca20ef | Debian-Stretch-9.13.13                         | active |
    >   | 7ea9d508-c992-4cbb-a983-c098c51aeb0c | Debian-Stretch-9.13.24                         | active |
    >   | 700b6450-c1d1-42aa-a15f-61199e5c1bf6 | Debian-Stretch-9.13.27                         | active |
    >   | 1779f380-780d-40d8-8052-b3acb91ed530 | Fedora-31-1.9                                  | active |
    >   | e62a71df-4bd2-4498-9eae-058ff476b5ad | Fedora-33-1.2                                  | active |
    >   | e5c23082-cc34-4213-ad31-ff4684657691 | Fedora-34.1.2                                  | active |
    >   | dcb41a5f-868a-4880-9fd5-04b95ab97c47 | FedoraAtomic29-20191126                        | active |
    >   | a079781f-80b7-4d89-95ae-ef65bfb0834f | FedoraCoreOS33                                 | active |
    >   | 191d3d4d-60cc-4b87-b4a7-0a03cc48a51e | FedoraCoreOS34                                 | active |
    >   | 7f7153a4-48f2-4e1f-a8f6-89ca70f3e2b6 | Rocky-8-GenericCloud-LVM.20221130.0.x86_64     | active |
    >   | e7432a57-9bf1-43f2-8ea5-5f4e5436b30e | Rocky-9-GenericCloud-LVM-9.1-20221130.0.x86_64 | active |
    >   | aebeee6a-1435-42fc-91f7-edc861a2d8cf | RockyLinux-8.5-20211114.2                      | active |
    >   | c3f0319c-58b5-48a5-82be-0d71c84a9544 | Ubuntu-Bionic-18.04-20191218                   | active |
    >   | 9cd29964-d27b-44b6-9fa1-b3b8a9449858 | Ubuntu-Bionic-18.04-20210112                   | active |
    >   | 583aae83-50ec-41f3-8c84-d5c1d38bc622 | Ubuntu-Bionic-18.04-20210609                   | active |
    >   | 3c8331a4-2445-42eb-870a-23f240e09ef1 | Ubuntu-Bionic-18.04-20210922                   | active |
    >   | 8be8c170-cd91-4c05-b2eb-269bfd68316d | Ubuntu-Focal-20.04-20210114                    | active |
    >   | d9887730-d28e-497f-b676-6f2fb81c4a31 | Ubuntu-Focal-20.04-20210624                    | active |
    >   | b13c5939-927d-487b-b8e0-d72d071dd3e1 | Ubuntu-Focal-20.04-20210922                    | active |
    >   | a42aef9d-a910-4a5e-b2f4-9a3e9d8cfc7e | Ubuntu-Focal-20.04-20220124                    | active |
    >   | b662809e-5000-416c-8283-5ada993e2386 | kayobe-rocky-8.6-rc5                           | active |
    >   | e8b6f548-4a5f-49ac-8207-ce6f2e4adb9a | kayobe-rocky-8.6-uefi-v2                       | active |
    >   | 6de54d53-adda-470c-91c3-9fe41e78e1e7 | kayobe-rocky-8.6-uefi-v3                       | active |
    >   | 0b1414f8-8758-42ab-b5b0-070135305dd1 | kayobe-rocky-8.6-uefi-v4                       | active |
    >   | 796cc099-e302-4e9b-b5cf-d99a633da092 | os_migrate_conv                                | active |
    >   +--------------------------------------+------------------------------------------------+--------+

    #
    # StackHPC's example uses an Ubuntu image.
    # https://github.com/stackhpc/capi-helm-charts/tree/main/charts/openstack-cluster#managing-a-workload-cluster
    #

    >   ....
    >   # The target Kubernetes version
    >   kubernetesVersion: 1.22.1
    >
    >   # An image with the required software installed at the target version
    >   machineImage: ubuntu-2004-kube-v{{ .Values.kubernetesVersion }}
    >   ....

    #
    # When we are logged in via the Horizon GUI, we can see an 'ubuntu-2004-kube' image, and create a VM with it.
    #

    #
    # When we are logged in via the command line, we can't see the image.
    #

    openstack \
        --os-cloud "${cloudname:?}" \
        image show \
            'ubuntu-2004-kube-v1.25.4'

    >   No Image found for ubuntu-2004-kube-v1.25.4

    #
    # Because the visibility is set to 'community'.
    # https://wiki.openstack.org/wiki/Glance-v2-community-image-visibility-design
    #

    openstack \
        --os-cloud "${cloudname:?}" \
        image list \
            --community

    >   +--------------------------------------+----------------------------------------------+--------+
    >   | ID                                   | Name                                         | Status |
    >   +--------------------------------------+----------------------------------------------+--------+
    >   | 948b088f-f15a-4082-9fa2-8d6ec46d63fc | CentOS8-2004                                 | active |
    >   | f18c8fdd-b5b3-4c6f-a5bf-61fb35c061b1 | openhpc-220830-2042                          | active |
    >   | b3727075-e811-471b-b72e-c129dc18a221 | openhpc-221027-1557                          | active |
    >   | fb6bea43-b19b-4dd9-9334-f747060de018 | openhpc-230106-1107                          | active |
    >   | 86a46069-d395-4157-99ff-254f093edf21 | openhpc-230110-1629                          | active |
    >   | 5d7dca79-da94-4ce2-90f9-18e5e35d15d0 | openhpc-230217-1440                          | active |
    >   | 74f481a4-440f-466d-a1dd-7bc521c4418d | rocky-8-20220702                             | active |
    >   | aba8be34-d96e-4276-863f-af76ebab71d5 | ubuntu-2004-kube-v1.22.12                    | active |
    >   | 932a8846-ce6d-49a9-b926-f200c322d237 | ubuntu-2004-kube-v1.22.15                    | active |
    >   | d248843e-f1ea-49b2-b3c8-fca8e0ecb3fe | ubuntu-2004-kube-v1.23.14                    | active |
    >   | 6d2fe080-a9fe-4fde-a8d3-cd101c5310a3 | ubuntu-2004-kube-v1.23.9                     | active |
    >   | a83b5cfc-eb5e-4ca3-8d5c-cef95f41bcce | ubuntu-2004-kube-v1.24.2                     | active |
    >   | 0350c998-23c7-44c5-8782-e29ac9640527 | ubuntu-2004-kube-v1.24.8                     | active |
    >   | 75f1e989-d2e3-4f31-b660-8a8a9e5fa967 | ubuntu-2004-kube-v1.25.4                     | active |
    >   | 6c30d015-6193-4479-b962-653a1a18d622 | ubuntu-focal-desktop-220808-1248             | active |
    >   | 856f77c7-7e68-4d6f-be37-0d414844768b | ubuntu-focal-desktop-220810-0904             | active |
    >   | f7e6408b-a9b8-4ba2-a4c1-e19f61bebe7e | ubuntu-focal-desktop-221017-1036             | active |
    >   | 85d3cdde-2661-4e3f-ba14-5c9d061060a1 | ubuntu-focal-jupyter-repo2docker-220808-1351 | active |
    >   | af5b2e4a-c650-4a50-9aad-b1206e7867d2 | ubuntu-focal-jupyter-repo2docker-220810-1028 | active |
    >   | 5753f0ed-78de-4e8e-a5b6-c7f31fa34795 | ubuntu-focal-jupyter-repo2docker-221017-1147 | active |
    >   +--------------------------------------+----------------------------------------------+--------+

    #
    # 'ubuntu-focal' and 'ubuntu-2004' are the same thing.
    # https://wiki.ubuntu.com/Releases
    # Released April 2020.
    # Ubuntu have released 22.04, Jammy Jellyfish, released April 2022, but that isn't in the image list yet.
    #
    # If we are going to use a custom image, I'd go for Rocky or CoreOS.
    # Later ..
    #

    openstack \
        --os-cloud "${cloudname:?}" \
        availability zone list \
            --long

--START--
+-----------+-------------+---------------+-----------+--------------+----------------+
| Zone Name | Zone Status | Zone Resource | Host Name | Service Name | Service Status |
+-----------+-------------+---------------+-----------+--------------+----------------+
| nova      | available   |               |           |              |                |
| nova      | available   |               |           |              |                |
| nova      | available   | network       |           |              |                |
| nova      | available   | router        |           |              |                |
+-----------+-------------+---------------+-----------+--------------+----------------+
--END--


    openstack \
        --os-cloud "${cloudname:?}" \
        availability zone list \
            --compute \
            --long

--START--
+-----------+-------------+---------------+-----------+--------------+----------------+
| Zone Name | Zone Status | Zone Resource | Host Name | Service Name | Service Status |
+-----------+-------------+---------------+-----------+--------------+----------------+
| nova      | available   |               |           |              |                |
+-----------+-------------+---------------+-----------+--------------+----------------+
--END--


    openstack \
        --os-cloud "${cloudname:?}" \
        network list \
            --external

--START--
+--------------------------------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
| ID                                   | Name          | Subnets                                                                                                                                                |
+--------------------------------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
| 410920fb-5714-4447-b26a-e7b06092fc62 | cephfs        | 5699fb5d-8316-4b88-b889-b05c8a1ec975                                                                                                                   |
| 57add367-d205-4030-a929-d75617a7c63e | CUDN-Internet | 1847b14d-b974-4f78-959d-44d18d4485b8, 3fcaa5a5-ba8e-49a9-bf94-d87fbb0afc42, 5f1388b3-a0c7-463e-bb58-5532c38e4b40, a79eb610-eca3-4ee8-aaf1-88f4fef5a4e7 |
+--------------------------------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
--END--

    #
    # This detects more than one 'external' network, so we will need to specify which one is the real external network.
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Set the environment variables used by cloud-provider-openstack.
# https://cluster-api-openstack.sigs.k8s.io/clusteropenstack/configuration.html
#[fedora@bootstrap]

    #
    # The image name 'ubuntu-2004-kube-v1.25.4' gives us the OS image and the K8s version.
    #

    KUBERNETES_VERSION=1.25.4
    OPENSTACK_IMAGE_NAME=ubuntu-2004-kube-v1.25.4

    #
    # The Ubuntu images are quite large, and won't fit on a tiny image.
    # Error message from Horizon GUI :

--START--
Error: Failed to perform requested operation on instance "vsdfhc",
the instance has an error status: Please try again later
    [
    Error: Build of instance d701f3f9-1f98-4d01-bf8a-b2b4cde741b7 aborted:
    Flavor's disk is too small for requested image.
    Flavor disk is 10737418240 bytes, image is 21474836480 bytes.
    ].
--END--


    OPENSTACK_CONTROL_PLANE_MACHINE_FLAVOR=vm.v1.small
    OPENSTACK_NODE_MACHINE_FLAVOR=vm.v1.large

    OPENSTACK_CLOUD=iris-gaia-red
    OPENSTACK_SSH_KEY_NAME=iris-gaia-red-20230409-keypair
    OPENSTACK_EXTERNAL_NETWORK_ID=57add367-d205-4030-a929-d75617a7c63e

    OPENSTACK_FAILURE_DOMAIN=nova


# -----------------------------------------------------
# -----------------------------------------------------
# Transfer a copy of our clouds.yaml file.
#[root@ansibler]

    scp /etc/openstack/clouds.yaml bootstrap:/tmp/clouds.yaml

    ssh bootstrap 'sudo mkdir -p /etc/openstack'
    ssh bootstrap 'sudo install --target-directory '/etc/openstack' /tmp/clouds.yaml'

    #
    # If we can expose the cluster endpoint, we should be able to do more in the client container.
    # Look at this next round.
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Install yq on the bootstrap node.
#[fedora@bootstrap]

    linkedinstall()
        {
        local sourcepath=${1}
        local sourcename=$(basename "${sourcepath}")
        local shortname=${2}
        local destpath=/usr/local/bin/

        sudo install \
            --owner 'root' \
            --group 'root' \
            --mode  'u=rwx,g=rwx,o=rx' \
            "${sourcepath}" \
            "${destpath}"

        pushd "${destpath}"
            sudo ln -s "${sourcename}" "${shortname}"
        popd
        }

    curl \
        --location \
        --no-progress-meter \
        --output '/tmp/yq-4.26.1' \
        'https://github.com/mikefarah/yq/releases/download/v4.26.1/yq_linux_amd64'

    linkedinstall '/tmp/yq-4.26.1' 'yq'


# -----------------------------------------------------
# Use the script provided by cluster-api-provider-openstack to parse our clouds.yaml file.
# https://cluster-api-openstack.sigs.k8s.io/clusteropenstack/configuration.html#generate-credentials
# https://github.com/kubernetes-sigs/cluster-api-provider-openstack/blob/main/docs/book/src/clusteropenstack/configuration.md#generate-credentials
#[fedora@bootstrap]

    curl \
        --location \
        --no-progress-meter \
        --output '/tmp/env.rc' \
        'https://raw.githubusercontent.com/kubernetes-sigs/cluster-api-provider-openstack/master/templates/env.rc'

    source '/tmp/env.rc' '/etc/openstack/clouds.yaml' "${OPENSTACK_CLOUD:?}"

    echo "CLOUD_YAML_B64 [${OPENSTACK_CLOUD_YAML_B64}]"
    echo "CLOUD_CACERT_B64 [${OPENSTACK_CLOUD_CACERT_B64}]"
    echo "CLOUD_PROVIDER_CONF_B64 [${OPENSTACK_CLOUD_PROVIDER_CONF_B64}]"

--START--
CLOUD_YAML_B64 [Y2xvdWRz....aWFsIgo=]
CLOUD_CACERT_B64 [Cg==]
CLOUD_PROVIDER_CONF_B64 [W0dsb2Jh....5b28yIgo=]
--END--


    #
    # https://cluster-api-openstack.sigs.k8s.io/clusteropenstack/configuration.html
    # Note: Only the [external cloud provider][1] supports [Application Credentials][2].
    # [1] https://cluster-api-openstack.sigs.k8s.io/topics/external-cloud-provider.html
    # [2] https://docs.openstack.org/keystone/latest/user/application_credentials.html
    #

    #
    # https://cluster-api-openstack.sigs.k8s.io/topics/external-cloud-provider.html
    # To deploy a cluster using [external cloud provider][3], create a cluster configuration with the [external cloud provider template][4] or refer to [helm chart][5].
    # [3] https://github.com/kubernetes/cloud-provider-openstack
    # [4] https://github.com/kubernetes-sigs/cluster-api-provider-openstack/blob/main/templates/cluster-template-external-cloud-provider.yaml
    # [5] https://github.com/kubernetes/cloud-provider-openstack/tree/master/charts/openstack-cloud-controller-manager
    #

    #
    # https://cluster-api-openstack.sigs.k8s.io/clusteropenstack/configuration.html#required-configuration
    # Note: By default the command creates highly available control plane with internal OpenStack cloud provider.
    # If you wish to create highly available control plane with external OpenStack cloud provider
    # or single control plane without load balancer, use external-cloud-provider or without-lb flavor respectively.
    #

    #
    # cluster-api-provider-openstack configuration.
    # https://github.com/kubernetes-sigs/cluster-api-provider-openstack/blob/main/docs/book/src/clusteropenstack/configuration.md
    #


# -----------------------------------------------------
# Generate our cluster config.
# https://cluster-api-openstack.sigs.k8s.io/getting-started.html#generating-the-cluster-configuration
#[fedora@bootstrap]

    CLUSTER_BASE=green-frog

    #
    # Basic quick-start.
    CLUSTER_NAME=${CLUSTER_BASE}-basic
    sudo clusterctl generate cluster \
        "${CLUSTER_NAME:?}" \
        --kubernetes-version "${KUBERNETES_VERSION:?}" \
        --control-plane-machine-count 3 \
        --worker-machine-count 3 \
    | tee "/tmp/${CLUSTER_NAME:?}.yaml" \
    | yq -P


--START--
Error: value for variables [
    OPENSTACK_CLOUD,
    OPENSTACK_CLOUD_CACERT_B64,
    OPENSTACK_CLOUD_PROVIDER_CONF_B64,
    OPENSTACK_CLOUD_YAML_B64,
    OPENSTACK_CONTROL_PLANE_MACHINE_FLAVOR,
    OPENSTACK_DNS_NAMESERVERS,
    OPENSTACK_EXTERNAL_NETWORK_ID,
    OPENSTACK_FAILURE_DOMAIN,
    OPENSTACK_IMAGE_NAME,
    OPENSTACK_NODE_MACHINE_FLAVOR,
    OPENSTACK_SSH_KEY_NAME
    ] is not set.
    Please set the value using os environment variables or the clusterctl config file
--END--

    #
    # Bugger.
    # We need to run 'clusterctl' as root because (Docker),
    # but we have set all the environment variables in 'fedora's environment.
    #

    #
    # My guess is once we have started the control cluster on bootstrap VM,
    # we should be able to do everything else in the client container.
    #

    #
    # Perhaps ... we could run control cluster locally ?
    # Just a thought.
    # Does it _need_ to be inside the Openstack platform ?
    # Just somewhere safe.
    # So probably not on the client machine or we will run into the terraform state problem.
    #





    #
    # Using the 'external-cloud-provider' template
    CLUSTER_NAME=${CLUSTER_BASE}-extern
    clusterctl generate cluster \
        "${CLUSTER_NAME:?}" \
        --flavor external-cloud-provider \
        --kubernetes-version "${KUBERNETES_VERSION:?}" \
        --control-plane-machine-count 3 \
        --worker-machine-count 3 \
    | tee "/tmp/${CLUSTER_NAME:?}.yaml" \
    | yq -P







