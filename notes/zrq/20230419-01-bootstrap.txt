#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2023, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Delete everything and run the build again from the start ....
        This time set the cloud name to iris-gaia-red-admin

    Result:

        Work in progress ...


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    #
    # Live is green, selecting red for the deployment.
    #

    source "${HOME:?}/aglais.env"

    agcolour=red

    clientname=ansibler-${agcolour}
    cloudname=iris-gaia-${agcolour}-admin

    podman run \
        --rm \
        --tty \
        --interactive \
        --name     "${clientname:?}" \
        --hostname "${clientname:?}" \
        --env "cloudname=${cloudname:?}" \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK:?}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        ghcr.io/wfau/atolmis/ansible-client:2022.07.25 \
        bash

    >   ....
    >   ....


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"

    >   real    2m30.209s
    >   user    1m8.264s
    >   sys     0m7.731s


# -----------------------------------------------------
# Add YAML editor role to our client container.
# TODO Add this to the Ansible client.
# https://github.com/wfau/atolmis/issues/30
#[root@ansibler]

    ansible-galaxy install kwoodson.yedit


# -----------------------------------------------------
# Create our deployment settings.
#[root@ansibler]

    deployname=${cloudname:?}-$(date '+%Y%m%d')
    deploydate=$(date '+%Y%m%dT%H%M%S')

    statusyml='/opt/aglais/aglais-status.yml'
    if [ ! -e "$(dirname ${statusyml})" ]
    then
        mkdir "$(dirname ${statusyml})"
    fi
    rm -f "${statusyml}"
    touch "${statusyml}"

    yq eval \
        --inplace \
        "
        .aglais.deployment.type = \"cluster-api\"   |
        .aglais.deployment.name = \"${deployname}\" |
        .aglais.deployment.date = \"${deploydate}\" |
        .aglais.openstack.cloud.name = \"${cloudname}\"
        " "${statusyml}"


    cat /opt/aglais/aglais-status.yml

    >   aglais:
    >     deployment:
    >       type: cluster-api
    >       name: iris-gaia-red-admin-20230419
    >       date: 20230419T044842
    >     openstack:
    >       cloud:
    >         name: iris-gaia-red-admin


# -----------------------------------------------------
# Create our bootstrap components.
#[root@ansibler]

    inventory=/deployments/cluster-api/bootstrap/ansible/config/inventory.yml

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/01-create-keypair.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/02-create-network.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/03-create-bootstrap.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/04-local-config.yml'

    cat /opt/aglais/aglais-status.yml

    >   aglais:
    >     deployment:
    >       date: 20230419T044842
    >       name: iris-gaia-red-admin-20230419
    >       type: cluster-api
    >     openstack:
    >       cloud:
    >         name: iris-gaia-red-admin
    >       keypairs:
    >         team:
    >           fingerprint: 2e:84:98:98:df:70:06:0e:4c:ed:bd:d4:d6:6b:eb:16
    >           id: iris-gaia-red-admin-20230419-keypair
    >           name: iris-gaia-red-admin-20230419-keypair
    >       networks:
    >         internal:
    >           network:
    >             id: 8cf4eede-c293-446b-9145-e28e5dbc7483
    >             name: iris-gaia-red-admin-20230419-internal-network
    >           router:
    >             id: 05c2cfda-1916-4047-9dd9-d6931974c1a7
    >             name: iris-gaia-red-admin-20230419-internal-router
    >           subnet:
    >             cidr: 10.10.0.0/16
    >             id: 9635e673-6309-462d-8919-9a5b7d34eba3
    >             name: iris-gaia-red-admin-20230419-internal-subnet
    >       servers:
    >         bootstrap:
    >           float:
    >             external: 128.232.226.76
    >             id: 64b6af66-160b-46c7-bb96-3ce8fc1407d9
    >             internal: 10.10.0.134
    >           server:
    >             address:
    >               ipv4: 10.10.0.134
    >             flavor:
    >               name: gaia.vm.cclake.2vcpu
    >             hostname: bootstrap
    >             id: 8167b60d-afe9-4822-916f-63d5a5bf9a49
    >             image:
    >               id: e5c23082-cc34-4213-ad31-ff4684657691
    >               name: Fedora-34.1.2
    >             name: iris-gaia-red-admin-20230419-bootstrap


# -----------------------------------------------------
# SSH test.
#[root@ansibler]

    ssh bootstrap \
        '
        date
        hostname
        '

    >   Wed Apr 19 04:50:38 UTC 2023
    >   iris-gaia-red-admin-20230419-bootstrap


# -----------------------------------------------------
# Transfer a copy of the config
#[root@ansibler]

    scp /opt/aglais/aglais-status.yml \
        bootstrap:/tmp/aglais-status.yml

    ssh bootstrap \
        '
        sudo mkdir -p /opt/aglais
        sudo mv /tmp/aglais-status.yml \
            /opt/aglais
        '


# -----------------------------------------------------
# -----------------------------------------------------
# Login to the bootstrap node as root.
#[user@desktop]

    podman exec \
        -it \
        ansibler-red \
            bash

        ssh bootstrap

            sudo su -

    #
    # We could prefix everything with sudo, but it gets very boring.
    #

# -----------------------------------------------------
# Install Docker.
# https://docs.docker.com/engine/install/fedora/#install-using-the-repository
#[root@bootstrap]

    dnf -y install dnf-plugins-core

    dnf config-manager \
        --add-repo \
        https://download.docker.com/linux/fedora/docker-ce.repo

    dnf install \
        -y \
        docker-ce \
        docker-ce-cli \
        containerd.io \
        docker-compose-plugin

    systemctl enable docker

    systemctl start docker

    systemctl status docker --no-pager

    >   Created symlink /etc/systemd/system/multi-user.target.wants/docker.service â†’ /usr/lib/systemd/system/docker.service.
    >   â— docker.service - Docker Application Container Engine
    >        Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
    >        Active: active (running) since Wed 2023-04-19 04:52:46 UTC; 10ms ago
    >   TriggeredBy: â— docker.socket
    >          Docs: https://docs.docker.com
    >      Main PID: 8572 (dockerd)
    >         Tasks: 8
    >        Memory: 29.3M
    >           CPU: 202ms
    >        CGroup: /system.slice/docker.service
    >                â””â”€8572 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
    >   ....
    >   ....


    docker --version

    >   Docker version 20.10.17, build 100c701


# -----------------------------------------------------
# Install kubectl.
# https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-using-native-package-management
#[root@bootstrap]

    cat > '/etc/yum.repos.d/kubernetes.repo' << EOF
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

    dnf install -y 'kubectl'

    kubectl version --output json

    >   {
    >     "clientVersion": {
    >       "major": "1",
    >       "minor": "27",
    >       "gitVersion": "v1.27.1",
    >       "gitCommit": "4c9411232e10168d7b050c49a1b59f6df9d7ea4b",
    >       "gitTreeState": "clean",
    >       "buildDate": "2023-04-14T13:21:19Z",
    >       "goVersion": "go1.20.3",
    >       "compiler": "gc",
    >       "platform": "linux/amd64"
    >     },
    >     "kustomizeVersion": "v5.0.1"
    >   }


# -----------------------------------------------------
# Install kind on the bootstrap node.
# https://kind.sigs.k8s.io/docs/user/quick-start/#installing-from-release-binaries
#[root@bootstrap]

    kindversion=0.17.0
    kindbinary=kind-${kindversion:?}
    kindtemp=/tmp/${kindbinary:?}

    curl \
        --location \
        --no-progress-meter \
        --output "${kindtemp:?}" \
        "https://kind.sigs.k8s.io/dl/v${kindversion:?}/kind-linux-amd64"

    pushd /usr/local/bin
        mv "${kindtemp:?}" .
        chown 'root:root' "${kindbinary:?}"
        chmod 'u=rwx,g=rx,o=rx' "${kindbinary:?}"
        ln -s "${kindbinary:?}" 'kind'
    popd

    kind --version

    >   kind version 0.17.0


# -----------------------------------------------------
# Install Helm on the bootstrap node.
# https://helm.sh/docs/intro/install/
# https://github.com/helm/helm/releases
#[root@bootstrap]

    helmarch=linux-amd64
    helmversion=3.11.2
    helmtarfile=helm-v${helmversion}-${helmarch}.tar.gz
    helmtmpfile=/tmp/${helmtarfile:?}
    helmbinary=helm-${helmversion:?}

    curl \
        --location \
        --no-progress-meter \
        --output "${helmtmpfile:?}" \
        "https://get.helm.sh/${helmtarfile:?}"

    tar \
        --gzip \
        --extract \
        --directory /tmp \
        --file "${helmtmpfile:?}"

    pushd /usr/local/bin
        mv "/tmp/${helmarch:?}/helm" "${helmbinary:?}"
        chown 'root:root' "${helmbinary:?}"
        chmod 'u=rwx,g=rx,o=rx' "${helmbinary:?}"
        ln -s "${helmbinary:?}" 'helm'
    popd

    helm version

    >   version.BuildInfo{
    >       Version:"v3.11.2",
    >       GitCommit:"912ebc1cd10d38d340f048efaf0abda047c3468e",
    >       GitTreeState:"clean",
    >       GoVersion:"go1.18.10"
    >       }


# -----------------------------------------------------
# Install clusterctl
# The clusterctl CLI tool handles the lifecycle of a Cluster-API management cluster.
# https://cluster-api-openstack.sigs.k8s.io/getting-started.html#install-clusterctl
#[root@bootstrap]

    clusterctlversion=1.4.1
    clusterctlbinary=clusterctl-${clusterctlversion:?}

    curl \
        --location \
        --no-progress-meter \
        --output "/tmp/${clusterctlbinary:?}" \
        "https://github.com/kubernetes-sigs/cluster-api/releases/download/v${clusterctlversion:?}/clusterctl-linux-amd64"

    pushd /usr/local/bin
        mv "/tmp/${clusterctlbinary:?}" "${clusterctlbinary:?}"
        chown 'root:root' "${clusterctlbinary:?}"
        chmod 'u=rwx,g=rx,o=rx' "${clusterctlbinary:?}"
        ln -s "${clusterctlbinary:?}" 'clusterctl'
    popd

    clusterctl version

    >   clusterctl version: &version.Info{
    >       Major:"1",
    >       Minor:"4",
    >       GitVersion:"v1.4.1",
    >       GitCommit:"39d87e91080088327c738c43f39e46a7f557d03b",
    >       GitTreeState:"clean",
    >       BuildDate:"2023-04-04T17:31:43Z",
    >       GoVersion:"go1.19.6",
    >       Compiler:"gc",
    >       Platform:"linux/amd64"
    >       }


# -----------------------------------------------------
# Create our initial Kind cluster.
# https://github.com/kubernetes-sigs/kind/pull/2478#issuecomment-1214656908
#[root@bootstrap]

    kind create cluster --retain

    >   Creating cluster "kind" ...
    >    âœ“ Ensuring node image (kindest/node:v1.25.3) ðŸ–¼
    >    âœ“ Preparing nodes ðŸ“¦
    >    âœ“ Writing configuration ðŸ“œ
    >    âœ“ Starting control-plane ðŸ•¹ï¸
    >    âœ“ Installing CNI ðŸ”Œ
    >    âœ“ Installing StorageClass ðŸ’¾
    >   ....
    >   ....


    kubectl cluster-info --context kind-kind

    >   Kubernetes control plane is running at https://127.0.0.1:43791
    >   CoreDNS is running at https://127.0.0.1:43791/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
    >   ....


    kubectl get pods --all-namespaces

    >   NAMESPACE            NAME                                         READY   STATUS    RESTARTS   AGE
    >   kube-system          coredns-565d847f94-k9sfp                     1/1     Running   0          16s
    >   kube-system          coredns-565d847f94-x72hv                     1/1     Running   0          16s
    >   kube-system          etcd-kind-control-plane                      1/1     Running   0          30s
    >   kube-system          kindnet-6lrpd                                1/1     Running   0          16s
    >   kube-system          kube-apiserver-kind-control-plane            1/1     Running   0          29s
    >   kube-system          kube-controller-manager-kind-control-plane   1/1     Running   0          29s
    >   kube-system          kube-proxy-fc7tn                             1/1     Running   0          16s
    >   kube-system          kube-scheduler-kind-control-plane            1/1     Running   0          29s
    >   local-path-storage   local-path-provisioner-684f458cdd-pwz8m      1/1     Running   0          16s


# -----------------------------------------------------
# Initialize the Openstack management cluster
# https://cluster-api-openstack.sigs.k8s.io/getting-started.html#initialization-for-common-providers
#[root@bootstrap]

    clusterctl init --infrastructure openstack

    >   Fetching providers
    >   Installing cert-manager Version="v1.11.0"
    >   Waiting for cert-manager to be available...
    >   Installing Provider="cluster-api" Version="v1.4.1" TargetNamespace="capi-system"
    >   Installing Provider="bootstrap-kubeadm" Version="v1.4.1" TargetNamespace="capi-kubeadm-bootstrap-system"
    >   Installing Provider="control-plane-kubeadm" Version="v1.4.1" TargetNamespace="capi-kubeadm-control-plane-system"
    >   Installing Provider="infrastructure-openstack" Version="v0.7.1" TargetNamespace="capo-system"
    >   ....
    >   ....


    kubectl get pods --all-namespaces

    >   NAMESPACE                           NAME                                                             READY   STATUS    RESTARTS   AGE
    >   capi-kubeadm-bootstrap-system       capi-kubeadm-bootstrap-controller-manager-8654485994-2fwvp       1/1     Running   0          22s
    >   capi-kubeadm-control-plane-system   capi-kubeadm-control-plane-controller-manager-5d9d9494d5-z25q2   1/1     Running   0          21s
    >   capi-system                         capi-controller-manager-746b4f5db4-9ml59                         1/1     Running   0          23s
    >   capo-system                         capo-controller-manager-775d744795-p5jjk                         1/1     Running   0          19s
    >   cert-manager                        cert-manager-99bb69456-gjk6b                                     1/1     Running   0          48s
    >   cert-manager                        cert-manager-cainjector-ffb4747bb-zwsgq                          1/1     Running   0          48s
    >   cert-manager                        cert-manager-webhook-545bd5d7d8-cxlnf                            1/1     Running   0          48s
    >   kube-system                         coredns-565d847f94-k9sfp                                         1/1     Running   0          90s
    >   kube-system                         coredns-565d847f94-x72hv                                         1/1     Running   0          90s
    >   kube-system                         etcd-kind-control-plane                                          1/1     Running   0          104s
    >   kube-system                         kindnet-6lrpd                                                    1/1     Running   0          90s
    >   kube-system                         kube-apiserver-kind-control-plane                                1/1     Running   0          103s
    >   kube-system                         kube-controller-manager-kind-control-plane                       1/1     Running   0          103s
    >   kube-system                         kube-proxy-fc7tn                                                 1/1     Running   0          90s
    >   kube-system                         kube-scheduler-kind-control-plane                                1/1     Running   0          103s
    >   local-path-storage                  local-path-provisioner-684f458cdd-pwz8m                          1/1     Running   0          90s


# -----------------------------------------------------
# -----------------------------------------------------
# List the available VM flavors and images.
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
        flavor list

    >   +--------------------------------------+-----------------------------+--------+------+-----------+-------+-----------+
    >   | ID                                   | Name                        |    RAM | Disk | Ephemeral | VCPUs | Is Public |
    >   +--------------------------------------+-----------------------------+--------+------+-----------+-------+-----------+
    >   | ....                                 | ....                        |   .... | .... |      .... |  .... | ....      |
    >   | 2e5dc624-1d3b-4da7-8107-cc2dd4cb5073 | vm.v1.large                 |  32768 |   60 |         0 |     8 | True      |
    >   | 6793b213-5efa-4b51-96bf-1340ff066499 | vm.v1.xsmall                |   2048 |   20 |         0 |     1 | True      |
    >   | 698a8d46-eceb-4c55-91ff-38286bf9eabb | vm.v1.tiny                  |   1024 |   10 |         0 |     1 | True      |
    >   | 6b56d6e9-5397-4543-87fb-e0c0b6d47961 | vm.v1.small                 |  16384 |   20 |         0 |     4 | True      |
    >   | ....                                 | ....                        |   .... | .... |      .... |  .... | ....      |
    >   +--------------------------------------+-----------------------------+--------+------+-----------+-------+-----------+


    #
    # The ubuntu-2004-kube images are hidden with 'community' visibility.
    # https://wiki.openstack.org/wiki/Glance-v2-community-image-visibility-design
    #
    # We have uploaded our own copy of the ubuntu-2004-kube image.
    #

    openstack \
        --os-cloud "${cloudname:?}" \
        image list \
            --shared

    >   +--------------------------------------+-----------------------------------+--------+
    >   | ID                                   | Name                              | Status |
    >   +--------------------------------------+-----------------------------------+--------+
    >   | 686c415b-c5a6-419e-8c46-4732498582e8 | gaia-dmp-ubuntu-2004-kube-v1.25.4 | active |
    >   +--------------------------------------+-----------------------------------+--------+


    openstack \
        --os-cloud "${cloudname:?}" \
        availability zone list \
            --compute

    >   +-----------+-------------+
    >   | Zone Name | Zone Status |
    >   +-----------+-------------+
    >   | nova      | available   |
    >   +-----------+-------------+


    #
    # There is more than one external network, so we would have to filter to select the right one.
    openstack \
        --os-cloud "${cloudname:?}" \
        network list \
            --external

    >   +--------------------------------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
    >   | ID                                   | Name          | Subnets                                                                                                                                                |
    >   +--------------------------------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
    >   | 410920fb-5714-4447-b26a-e7b06092fc62 | cephfs        | 5699fb5d-8316-4b88-b889-b05c8a1ec975                                                                                                                   |
    >   | 57add367-d205-4030-a929-d75617a7c63e | CUDN-Internet | 1847b14d-b974-4f78-959d-44d18d4485b8, 3fcaa5a5-ba8e-49a9-bf94-d87fbb0afc42, 5f1388b3-a0c7-463e-bb58-5532c38e4b40, a79eb610-eca3-4ee8-aaf1-88f4fef5a4e7 |
    >   +--------------------------------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+


    openstack \
        --os-cloud "${cloudname:?}" \
        keypair list

    >   +--------------------------------------+-------------------------------------------------+------+
    >   | Name                                 | Fingerprint                                     | Type |
    >   +--------------------------------------+-------------------------------------------------+------+
    >   | iris-gaia-blue-20230209-keypair      | 2e:84:98:98:df:70:06:0e:4c:ed:bd:d4:d6:6b:eb:16 | ssh  |
    >   | iris-gaia-green-20230203-keypair     | 2e:84:98:98:df:70:06:0e:4c:ed:bd:d4:d6:6b:eb:16 | ssh  |
    >   | iris-gaia-red-20230418-keypair       | 2e:84:98:98:df:70:06:0e:4c:ed:bd:d4:d6:6b:eb:16 | ssh  |
    >   | iris-gaia-red-admin-20230419-keypair | 2e:84:98:98:df:70:06:0e:4c:ed:bd:d4:d6:6b:eb:16 | ssh  |
    >   +--------------------------------------+-------------------------------------------------+------+


# -----------------------------------------------------
# Extract the settings we need.
#[root@ansibler]

    nodenodeflavor=vm.v1.large
    ctrlnodeflavor=vm.v1.small

    keypair=$(
        yq '.aglais.openstack.keypairs.team.name' /opt/aglais/aglais-status.yml
        )

    externalnet=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            network list \
                --external \
                --format json \
        | jq -r ".[] | select(.Name == \"CUDN-Internet\") | .ID"
        )

    cat > /tmp/openstack-settings.env << EOF
export OPENSTACK_CLOUD=${cloudname:?}
export OPENSTACK_SSH_KEY_NAME=${keypair:?}
export OPENSTACK_EXTERNAL_NETWORK_ID=${externalnet:?}

export OPENSTACK_NODE_MACHINE_FLAVOR=${nodenodeflavor}
export OPENSTACK_CONTROL_PLANE_MACHINE_FLAVOR=${ctrlnodeflavor}

export KUBERNETES_VERSION=1.25.4
export OPENSTACK_IMAGE_NAME=gaia-dmp-ubuntu-2004-kube-v1.25.4

export OPENSTACK_FAILURE_DOMAIN=nova

# Use the Cambridge DNS servers.
# https://www.dns.cam.ac.uk/servers/rec.html
export OPENSTACK_DNS_NAMESERVERS=131.111.8.42

EOF


# -----------------------------------------------------
# Transfer the Openstack settings to our bootstrap node.
#[root@ansibler]

    scp \
        /tmp/openstack-settings.env \
        bootstrap:/tmp/openstack-settings.env

    ssh bootstrap \
        '
        sudo mkdir -p \
            /etc/aglais
        sudo install \
            /tmp/openstack-settings.env \
            /etc/aglais/openstack-settings.env
        '

    >   ....
    >   ....


# -----------------------------------------------------
# Transfer a copy of our clouds.yaml file.
#[root@ansibler]

    scp \
        /etc/openstack/clouds.yaml \
        bootstrap:/tmp/openstack-clouds.yaml

    ssh bootstrap \
        '
        sudo mkdir -p \
            /etc/aglais
        sudo install \
            /tmp/openstack-clouds.yaml \
            /etc/aglais/openstack-clouds.yaml
        '

    >   ....
    >   ....


# -----------------------------------------------------
# -----------------------------------------------------
# Install yq on the bootstrap node.
#[root@bootstrap]

    yqversion=4.33.3
    yqbinary=yq-${yqversion:?}

    curl \
        --location \
        --no-progress-meter \
        --output "/tmp/${yqbinary:?}" \
        "https://github.com/mikefarah/yq/releases/download/v${yqversion}/yq_linux_amd64"

    pushd /usr/local/bin
        mv "/tmp/${yqbinary:?}" "${yqbinary:?}"
        chown 'root:root' "${yqbinary:?}"
        chmod 'u=rwx,g=rx,o=rx' "${yqbinary:?}"
        ln -s "${yqbinary:?}" 'yq'
    popd

    yq --version

    >   yq (https://github.com/mikefarah/yq/) version v4.33.3


# -----------------------------------------------------
# Edit our clouds.yaml file to disable TLS certificate checks.
# https://docs.openstack.org/os-client-config/latest/user/configuration.html#ssl-settings
#[root@bootstrap]

    vi /etc/aglais/openstack-clouds.yaml

          iris-gaia-red-admin:
            auth:
              auth_url: https://arcus.openstack.hpc.cam.ac.uk:5000
              ....
              ....
            region_name: "RegionOne"
            interface: "public"
            identity_api_version: 3
            auth_type: "v3applicationcredential"
       +    verify: false


# -----------------------------------------------------
# Load our Openstack settings.
#[root@bootstrap]

    source /etc/aglais/openstack-settings.env

cat << EOF
OPENSTACK_CLOUD [${OPENSTACK_CLOUD}]
OPENSTACK_IMAGE_NAME [${OPENSTACK_IMAGE_NAME}]
EOF

    >   OPENSTACK_CLOUD [iris-gaia-red-admin]
    >   OPENSTACK_IMAGE_NAME [gaia-dmp-ubuntu-2004-kube-v1.25.4]


# -----------------------------------------------------
# Use the script provided by cluster-api-provider-openstack to parse our clouds.yaml file.
# https://cluster-api-openstack.sigs.k8s.io/clusteropenstack/configuration.html#generate-credentials
# https://github.com/kubernetes-sigs/cluster-api-provider-openstack/blob/main/docs/book/src/clusteropenstack/configuration.md#generate-credentials
#[root@bootstrap]

    curl \
        --location \
        --no-progress-meter \
        --output '/tmp/env.rc' \
        'https://raw.githubusercontent.com/kubernetes-sigs/cluster-api-provider-openstack/master/templates/env.rc'

    source '/tmp/env.rc' '/etc/aglais/openstack-clouds.yaml' "${OPENSTACK_CLOUD:?}"

cat << EOF
OPENSTACK_CLOUD_YAML_B64   [${OPENSTACK_CLOUD_YAML_B64}]
OPENSTACK_CLOUD_CACERT_B64 [${OPENSTACK_CLOUD_CACERT_B64}]
OPENSTACK_CLOUD_PROVIDER_CONF_B64 [${OPENSTACK_CLOUD_PROVIDER_CONF_B64}]
EOF

    >   OPENSTACK_CLOUD_YAML_B64   [Y2xvdWRz....mYWxzZQo=]
    >   OPENSTACK_CLOUD_CACERT_B64 [Cg==]
    >   OPENSTACK_CLOUD_PROVIDER_CONF_B64 [W0dsb2Jh....xdTJ2Igo=]


# -----------------------------------------------------
# Generate our external cluster config.
# https://cluster-api.sigs.k8s.io/clusterctl/commands/generate-cluster.html
# https://cluster-api-openstack.sigs.k8s.io/getting-started.html#generating-the-cluster-configuration
#[root@bootstrap]

    CLUSTER_NAME=brown-toad

    clusterctl generate cluster \
        "${CLUSTER_NAME:?}" \
        --flavor external-cloud-provider \
        --kubernetes-version "${KUBERNETES_VERSION:?}" \
        --control-plane-machine-count 3 \
        --worker-machine-count 3 \
    | tee "/tmp/${CLUSTER_NAME:?}.yaml"


    >   apiVersion: v1
    >   data:
    >     cacert: Cg==
    >     clouds.yaml: Y2xvdWRz....mYWxzZQo=
    >   kind: Secret
    >   metadata:
    >     labels:
    >       clusterctl.cluster.x-k8s.io/move: "true"
    >     name: brown-toad-cloud-config
    >     namespace: default
    >   ....
    >   ....
    >   apiVersion: infrastructure.cluster.x-k8s.io/v1alpha6
    >   kind: OpenStackMachineTemplate
    >   metadata:
    >     name: brown-toad-md-0
    >     namespace: default
    >   spec:
    >     template:
    >       spec:
    >         cloudName: iris-gaia-red-admin
    >         flavor: vm.v1.large
    >         identityRef:
    >           kind: Secret
    >           name: brown-toad-cloud-config
    >         image: gaia-dmp-ubuntu-2004-kube-v1.25.4
    >         sshKeyName: iris-gaia-red-admin-20230419-keypair


# -----------------------------------------------------
# Apply the cluster config.
# https://cluster-api.sigs.k8s.io/user/quick-start.html#apply-the-workload-cluster
#[root@bootstrap]

    kubectl apply \
        -f "/tmp/${CLUSTER_NAME:?}.yaml"

    >   secret/brown-toad-cloud-config created
    >   kubeadmconfigtemplate.bootstrap.cluster.x-k8s.io/brown-toad-md-0 created
    >   cluster.cluster.x-k8s.io/brown-toad created
    >   machinedeployment.cluster.x-k8s.io/brown-toad-md-0 created
    >   kubeadmcontrolplane.controlplane.cluster.x-k8s.io/brown-toad-control-plane created
    >   openstackcluster.infrastructure.cluster.x-k8s.io/brown-toad created
    >   openstackmachinetemplate.infrastructure.cluster.x-k8s.io/brown-toad-control-plane created
    >   openstackmachinetemplate.infrastructure.cluster.x-k8s.io/brown-toad-md-0 created


    kubectl get cluster

    >   NAME         PHASE          AGE   VERSION
    >   brown-toad   Provisioning   12s


    clusterctl describe cluster "${CLUSTER_NAME:?}"

    >   NAME                                                           READY  SEVERITY  REASON                           SINCE  MESSAGE
    >   Cluster/brown-toad                                             False  Warning   ScalingUp                        40s    Scaling up control plane to 3 replicas (actual 0)
    >   â”œâ”€ClusterInfrastructure - OpenStackCluster/brown-toad
    >   â”œâ”€ControlPlane - KubeadmControlPlane/brown-toad-control-plane  False  Warning   ScalingUp                        40s    Scaling up control plane to 3 replicas (actual 0)
    >   â””â”€Workers
    >     â””â”€MachineDeployment/brown-toad-md-0                          False  Warning   WaitingForAvailableMachines      40s    Minimum availability requires 3 replicas, current 0 available
    >       â””â”€3 Machines...                                            False  Info      WaitingForClusterInfrastructure  39s    See brown-toad-md-0-98d489d87x8cnh9-ckd79, brown-toad-md-0-98d489d87x8cnh9-hsgft, ...


    kubectl \
        --namespace capo-system \
        logs \
        -l control-plane=capo-controller-manager \
        -c manager \
        --follow

    >   ....
    >   ....
    >   I0419 05:03:09.631363       1 loadbalancer.go:52] "Reconciling load balancer" controller="openstackcluster" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackCluster" OpenStackCluster="default/brown-toad" namespace="default" reconcileID=1bf3314f-136f-43a5-9b16-2e192fc49078 cluster="brown-toad" name="k8s-clusterapi-cluster-default-brown-toad-kubeapi"
    >   I0419 05:03:09.680419       1 loadbalancer.go:167] "Creating load balancer in subnet: \"3e824bca-0473-4fcd-b49d-a05097a55f77\"" controller="openstackcluster" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackCluster" OpenStackCluster="default/brown-toad" namespace="default" reconcileID=1bf3314f-136f-43a5-9b16-2e192fc49078 cluster="brown-toad" name="k8s-clusterapi-cluster-default-brown-toad-kubeapi"
    >   I0419 05:03:11.165450       1 loadbalancer.go:613] "Waiting for load balancer" controller="openstackcluster" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackCluster" OpenStackCluster="default/brown-toad" namespace="default" name="brown-toad" reconcileID=1bf3314f-136f-43a5-9b16-2e192fc49078 cluster="brown-toad" id="f5f339f9-4a67-46be-9cc4-67d6587a0259" targetStatus="ACTIVE"
    >   I0419 05:03:11.165763       1 recorder.go:103] "events: Created load balancer k8s-clusterapi-cluster-default-brown-toad-kubeapi with id f5f339f9-4a67-46be-9cc4-67d6587a0259" type="Normal" object={Kind:OpenStackCluster Namespace:default Name:brown-toad UID:39740f09-bb5c-46a9-8112-4693fd276e1b APIVersion:infrastructure.cluster.x-k8s.io/v1alpha6 ResourceVersion:2047 FieldPath:} reason="Successfulcreateloadbalancer"
    >   I0419 05:03:17.116720       1 openstackmachine_controller.go:311] "Cluster infrastructure is not ready yet, requeuing machine" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/brown-toad-md-0-8kn8k" namespace="default" name="brown-toad-md-0-8kn8k" reconcileID=ad8a84ae-b3fc-4d89-b694-d473dd469e23 openStackMachine="brown-toad-md-0-8kn8k" machine="brown-toad-md-0-98d489d87x8cnh9-ckd79" cluster="brown-toad" openStackCluster="brown-toad"
    >   ....
    >   ....
    >   I0419 05:04:28.510504       1 openstackmachine_controller.go:446] "Machine not exist, Creating Machine" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/brown-toad-control-plane-4pfgh" namespace="default" name="brown-toad-control-plane-4pfgh" reconcileID=a815086b-a90f-4f1e-b5fe-5ca130ce9263 openStackMachine="brown-toad-control-plane-4pfgh" machine="brown-toad-control-plane-5zm9f" cluster="brown-toad" openStackCluster="brown-toad" Machine="brown-toad-control-plane-4pfgh"
    >   I0419 05:04:29.540177       1 recorder.go:103] "events: Created port brown-toad-control-plane-4pfgh-0 with id 771a2445-a6a3-4beb-9a45-43bb1e866db6" type="Normal" object={Kind:OpenStackMachine Namespace:default Name:brown-toad-control-plane-4pfgh UID:dfaa8e13-1bbc-4ae1-ac6f-a846c1517444 APIVersion:infrastructure.cluster.x-k8s.io/v1alpha6 ResourceVersion:2497 FieldPath:} reason="Successfulcreateport"
    >   ....
    >   ....
    >   I0419 05:04:37.222674       1 http.go:143] "controller-runtime/webhook/webhooks: wrote response" webhook="/mutate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackmachine" code=200 reason= UID=92ed078a-d08d-48cd-8d65-ab777773a91e allowed=true
    >   I0419 05:04:37.225805       1 http.go:96] "controller-runtime/webhook/webhooks: received request" webhook="/validate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackmachine" UID=8110fbb7-b513-4d70-98ef-ba6d54500469 kind="infrastructure.cluster.x-k8s.io/v1alpha6, Kind=OpenStackMachine" resource={Group:infrastructure.cluster.x-k8s.io Version:v1alpha6 Resource:openstackmachines}
    >   I0419 05:04:37.226905       1 http.go:143] "controller-runtime/webhook/webhooks: wrote response" webhook="/validate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackmachine" code=200 reason= UID=8110fbb7-b513-4d70-98ef-ba6d54500469 allowed=true
    >   I0419 05:04:47.371308       1 http.go:96] "controller-runtime/webhook/webhooks: received request" webhook="/mutate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackmachine" UID=6fbae862-8440-4a5b-9f2b-5cecd6383e02 kind="infrastructure.cluster.x-k8s.io/v1alpha6, Kind=OpenStackMachine" resource={Group:infrastructure.cluster.x-k8s.io Version:v1alpha6 Resource:openstackmachines}
    >   I0419 05:04:47.372096       1 http.go:143] "controller-runtime/webhook/webhooks: wrote response" webhook="/mutate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackmachine" code=200 reason= UID=6fbae862-8440-4a5b-9f2b-5cecd6383e02 allowed=true
    >   I0419 05:04:47.375649       1 http.go:96] "controller-runtime/webhook/webhooks: received request" webhook="/validate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackmachine" UID=74d1bc8c-1138-4bfe-a7ea-f43a0362970e kind="infrastructure.cluster.x-k8s.io/v1alpha6, Kind=OpenStackMachine" resource={Group:infrastructure.cluster.x-k8s.io Version:v1alpha6 Resource:openstackmachines}
    >   I0419 05:04:47.376823       1 http.go:143] "controller-runtime/webhook/webhooks: wrote response" webhook="/validate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackmachine" code=200 reason= UID=74d1bc8c-1138-4bfe-a7ea-f43a0362970e allowed=true
    >   ....
    >   ....


    clusterctl describe cluster "${CLUSTER_NAME:?}"

    >   NAME                                                           READY  SEVERITY  REASON                       SINCE  MESSAGE
    >   Cluster/brown-toad                                             False  Warning   ScalingUp                    63s    Scaling up control plane to 3 replicas (actual 1)
    >   â”œâ”€ClusterInfrastructure - OpenStackCluster/brown-toad
    >   â”œâ”€ControlPlane - KubeadmControlPlane/brown-toad-control-plane  False  Warning   ScalingUp                    63s    Scaling up control plane to 3 replicas (actual 1)
    >   â”‚ â””â”€Machine/brown-toad-control-plane-5zm9f                     False  Info      WaitingForBootstrapData      73s    1 of 2 completed
    >   â””â”€Workers
    >     â””â”€MachineDeployment/brown-toad-md-0                          False  Warning   WaitingForAvailableMachines  2m55s  Minimum availability requires 3 replicas, current 0 available
    >       â””â”€3 Machines...                                            False  Info      WaitingForBootstrapData      75s    See brown-toad-md-0-98d489d87x8cnh9-ckd79, brown-toad-md-0-98d489d87x8cnh9-hsgft, ...


    kubectl \
        --namespace capo-system \
        logs \
        -l control-plane=capo-controller-manager \
        -c manager \
        --follow


    >   ....
    >   ....
    >   I0419 05:06:35.592500       1 loadbalancer.go:380] "Reconciling load balancer member" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/brown-toad-control-plane-4pfgh" namespace="default" reconcileID=b0f4719c-f5fe-4037-990a-c4aeb8d4dd3f openStackMachine="brown-toad-control-plane-4pfgh" machine="brown-toad-control-plane-5zm9f" cluster="brown-toad" openStackCluster="brown-toad" name="k8s-clusterapi-cluster-default-brown-toad-kubeapi"
    >   I0419 05:06:35.889466       1 openstackmachine_controller.go:434] "Reconciled Machine create successfully" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/brown-toad-control-plane-4pfgh" namespace="default" name="brown-toad-control-plane-4pfgh" reconcileID=b0f4719c-f5fe-4037-990a-c4aeb8d4dd3f openStackMachine="brown-toad-control-plane-4pfgh" machine="brown-toad-control-plane-5zm9f" cluster="brown-toad" openStackCluster="brown-toad"
    >   I0419 05:06:36.335494       1 openstackmachine_controller.go:326] "Reconciling Machine" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/brown-toad-control-plane-4pfgh" namespace="default" name="brown-toad-control-plane-4pfgh" reconcileID=9a22e1b1-48ee-47f1-91ae-be50ec2212fe openStackMachine="brown-toad-control-plane-4pfgh" machine="brown-toad-control-plane-5zm9f" cluster="brown-toad" openStackCluster="brown-toad"
    >   I0419 05:06:36.971635       1 openstackmachine_controller.go:372] "Machine instance state is ACTIVE" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/brown-toad-control-plane-4pfgh" namespace="default" name="brown-toad-control-plane-4pfgh" reconcileID=9a22e1b1-48ee-47f1-91ae-be50ec2212fe openStackMachine="brown-toad-control-plane-4pfgh" machine="brown-toad-control-plane-5zm9f" cluster="brown-toad" openStackCluster="brown-toad" instance-id="9bb7ba76-a955-4760-8b4c-fd8e17a9b538"
    >   ....
    >   ....
    >   I0419 05:06:36.971792       1 loadbalancer.go:380] "Reconciling load balancer member" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/brown-toad-control-plane-4pfgh" namespace="default" reconcileID=9a22e1b1-48ee-47f1-91ae-be50ec2212fe openStackMachine="brown-toad-control-plane-4pfgh" machine="brown-toad-control-plane-5zm9f" cluster="brown-toad" openStackCluster="brown-toad" name="k8s-clusterapi-cluster-default-brown-toad-kubeapi"
    >   I0419 05:06:37.166523       1 openstackmachine_controller.go:434] "Reconciled Machine create successfully" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/brown-toad-control-plane-4pfgh" namespace="default" name="brown-toad-control-plane-4pfgh" reconcileID=9a22e1b1-48ee-47f1-91ae-be50ec2212fe openStackMachine="brown-toad-control-plane-4pfgh" machine="brown-toad-control-plane-5zm9f" cluster="brown-toad" openStackCluster="brown-toad"
    >   ....
    >   ....
    >   I0419 05:07:26.363301       1 openstackmachine_controller.go:326] "Reconciling Machine" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/brown-toad-md-0-8kn8k" namespace="default" name="brown-toad-md-0-8kn8k" reconcileID=6768c2b7-4d1c-4ae7-8002-048692f8b300 openStackMachine="brown-toad-md-0-8kn8k" machine="brown-toad-md-0-98d489d87x8cnh9-ckd79" cluster="brown-toad" openStackCluster="brown-toad"
    >   I0419 05:07:26.382720       1 openstackmachine_controller.go:372] "Machine instance state is ACTIVE" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/brown-toad-control-plane-4pfgh" namespace="default" name="brown-toad-control-plane-4pfgh" reconcileID=44b0870f-bdde-4750-b072-e51cab25eb20 openStackMachine="brown-toad-control-plane-4pfgh" machine="brown-toad-control-plane-5zm9f" cluster="brown-toad" openStackCluster="brown-toad" instance-id="9bb7ba76-a955-4760-8b4c-fd8e17a9b538"
    >   ....
    >   ....
    >   I0419 05:07:40.119431       1 openstackmachine_controller.go:326] "Reconciling Machine" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/brown-toad-md-0-8kn8k" namespace="default" name="brown-toad-md-0-8kn8k" reconcileID=5436b64b-9151-462a-b4ca-16bb7f0b8264 openStackMachine="brown-toad-md-0-8kn8k" machine="brown-toad-md-0-98d489d87x8cnh9-ckd79" cluster="brown-toad" openStackCluster="brown-toad"
    >   I0419 05:07:40.484464       1 recorder.go:103] "events: Failed to create server brown-toad-md-0-d6lc5: error creating OpenStack instance 4b0457c4-6f3c-478b-993b-d2a467a52201, status changed to error" type="Warning" object={Kind:OpenStackMachine Namespace:default Name:brown-toad-md-0-d6lc5 UID:76ea4ab0-f83f-4ac1-9f20-a329bfb9daf7 APIVersion:infrastructure.cluster.x-k8s.io/v1alpha6 ResourceVersion:2460 FieldPath:} reason="Failedcreateserver"
    >   E0419 05:07:40.514484       1 controller.go:326] "Reconciler error" err="create OpenStack instance: error creating OpenStack instance 4b0457c4-6f3c-478b-993b-d2a467a52201, status changed to error" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/brown-toad-md-0-d6lc5" namespace="default" name="brown-toad-md-0-d6lc5" reconcileID=2fa4147a-8140-4622-9d6c-9eb3daf145bd
    >   I0419 05:07:40.554534       1 recorder.go:103] "events: Failed to create server brown-toad-md-0-dfz2d: error creating OpenStack instance 238a2e0a-37e4-4924-8ddf-fb7d8cfab522, status changed to error" type="Warning" object={Kind:OpenStackMachine Namespace:default Name:brown-toad-md-0-dfz2d UID:d2525ead-1dcb-48bf-9b07-f19caf5c7400 APIVersion:infrastructure.cluster.x-k8s.io/v1alpha6 ResourceVersion:2458 FieldPath:} reason="Failedcreateserver"
    >   E0419 05:07:40.577306       1 controller.go:326] "Reconciler error" err="create OpenStack instance: error creating OpenStack instance 238a2e0a-37e4-4924-8ddf-fb7d8cfab522, status changed to error" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/brown-toad-md-0-dfz2d" namespace="default" name="brown-toad-md-0-dfz2d" reconcileID=b9de05bb-fc9d-455b-99d3-471b4ab51ed2
    >   ....
    >   ....
    >   I0419 05:07:42.220916       1 http.go:96] "controller-runtime/webhook/webhooks: received request" webhook="/mutate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackmachine" UID=b1691bad-e10b-47a1-a027-d825450d63a7 kind="infrastructure.cluster.x-k8s.io/v1alpha6, Kind=OpenStackMachine" resource={Group:infrastructure.cluster.x-k8s.io Version:v1alpha6 Resource:openstackmachines}
    >   I0419 05:07:42.222147       1 http.go:143] "controller-runtime/webhook/webhooks: wrote response" webhook="/mutate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackmachine" code=200 reason= UID=b1691bad-e10b-47a1-a027-d825450d63a7 allowed=true
    >   I0419 05:07:42.225612       1 http.go:96] "controller-runtime/webhook/webhooks: received request" webhook="/validate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackmachine" UID=6dc8a798-86a6-4ad5-b180-21ea118dcccc kind="infrastructure.cluster.x-k8s.io/v1alpha6, Kind=OpenStackMachine" resource={Group:infrastructure.cluster.x-k8s.io Version:v1alpha6 Resource:openstackmachines}
    >   I0419 05:07:42.226973       1 http.go:143] "controller-runtime/webhook/webhooks: wrote response" webhook="/validate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackmachine" code=200 reason= UID=6dc8a798-86a6-4ad5-b180-21ea118dcccc allowed=true
    >   I0419 05:07:42.640287       1 openstackmachine_controller.go:299] "Not reconciling machine in failed state. See openStackMachine.status.failureReason, openStackMachine.status.failureMessage, or previously logged error for details" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/brown-toad-md-0-dfz2d" namespace="default" name="brown-toad-md-0-dfz2d" reconcileID=9700aeb3-4c59-41ac-bea4-0ebb60a67101 openStackMachine="brown-toad-md-0-dfz2d" machine="brown-toad-md-0-98d489d87x8cnh9-hsgft" cluster="brown-toad" openStackCluster="brown-toad"
    >   I0419 05:07:42.743687       1 openstackmachine_controller.go:377] "Machine instance state is ERROR" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/brown-toad-md-0-d6lc5" namespace="default" name="brown-toad-md-0-d6lc5" reconcileID=6da70fb4-98f9-4576-b8d0-38eb0a94a380 openStackMachine="brown-toad-md-0-d6lc5" machine="brown-toad-md-0-98d489d87x8cnh9-vpvc9" cluster="brown-toad" openStackCluster="brown-toad" instance-id="4b0457c4-6f3c-478b-993b-d2a467a52201"
    >   I0419 05:07:43.093907       1 openstackmachine_controller.go:299] "Not reconciling machine in failed state. See openStackMachine.status.failureReason, openStackMachine.status.failureMessage, or previously logged error for details" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/brown-toad-md-0-dfz2d" namespace="default" name="brown-toad-md-0-dfz2d" reconcileID=469051dc-a729-470f-bc56-f903ed28ee52 openStackMachine="brown-toad-md-0-dfz2d" machine="brown-toad-md-0-98d489d87x8cnh9-hsgft" cluster="brown-toad" openStackCluster="brown-toad"
    >   I0419 05:07:43.195597       1 openstackmachine_controller.go:299] "Not reconciling machine in failed state. See openStackMachine.status.failureReason, openStackMachine.status.failureMessage, or previously logged error for details" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/brown-toad-md-0-d6lc5" namespace="default" name="brown-toad-md-0-d6lc5" reconcileID=447d7be3-861b-45c0-9535-cabf0be7b277 openStackMachine="brown-toad-md-0-d6lc5" machine="brown-toad-md-0-98d489d87x8cnh9-vpvc9" cluster="brown-toad" openStackCluster="brown-toad"
    >   ....
    >   ....


    clusterctl describe cluster "${CLUSTER_NAME:?}"

    >   NAME                                                           READY  SEVERITY  REASON                       SINCE  MESSAGE
    >   Cluster/brown-toad                                             False  Warning   ScalingUp                    8m13s  Scaling up control plane to 3 replicas (actual 1)
    >   â”œâ”€ClusterInfrastructure - OpenStackCluster/brown-toad
    >   â”œâ”€ControlPlane - KubeadmControlPlane/brown-toad-control-plane  False  Warning   ScalingUp                    8m13s  Scaling up control plane to 3 replicas (actual 1)
    >   â”‚ â””â”€Machine/brown-toad-control-plane-5zm9f                     True                                          6m26s
    >   â””â”€Workers
    >     â””â”€MachineDeployment/brown-toad-md-0                          False  Warning   WaitingForAvailableMachines  10m    Minimum availability requires 3 replicas, current 0 available
    >       â””â”€3 Machines...                                            False  Error     InstanceStateError           5m10s  See brown-toad-md-0-98d489d87x8cnh9-ckd79, brown-toad-md-0-98d489d87x8cnh9-hsgft, ...


    #
    # Checked the Horizon GUI.
    # Instances are listed, but status is error.


    Message: "No valid host was found."
    Code: "500"
    Created: "19 Apr 2023, 5:07 a.m."

    #
    # My guess is we have run out of resources ?
    # Or run out of hosts to put them on ..
    #

    #
    # Try repeat the process with small workers ?
    #

# -----------------------------------------------------
# Try deleting our cluster.
#[root@bootstrap]

    clusterctl delete --all

    >   Deleting Provider="bootstrap-kubeadm" Version="v1.4.1" Namespace="capi-kubeadm-bootstrap-system"
    >   Deleting Provider="control-plane-kubeadm" Version="v1.4.1" Namespace="capi-kubeadm-control-plane-system"
    >   Deleting Provider="cluster-api" Version="v1.4.1" Namespace="capi-system"
    >   Deleting Provider="infrastructure-openstack" Version="v0.7.1" Namespace="capo-system"

    #
    # Not what I expected ...
    #


    clusterctl describe cluster "${CLUSTER_NAME:?}"

    >   NAME                                                           READY  SEVERITY  REASON                       SINCE  MESSAGE
    >   Cluster/brown-toad                                             False  Warning   ScalingUp                    23m    Scaling up control plane to 3 replicas (actual 1)
    >   â”œâ”€ClusterInfrastructure - OpenStackCluster/brown-toad
    >   â”œâ”€ControlPlane - KubeadmControlPlane/brown-toad-control-plane  False  Warning   ScalingUp                    23m    Scaling up control plane to 3 replicas (actual 1)
    >   â”‚ â””â”€Machine/brown-toad-control-plane-5zm9f                     True                                          22m
    >   â””â”€Workers
    >     â””â”€MachineDeployment/brown-toad-md-0                          False  Warning   WaitingForAvailableMachines  25m    Minimum availability requires 3 replicas, current 0 available
    >       â””â”€3 Machines...                                            False  Error     InstanceStateError           20m    See brown-toad-md-0-98d489d87x8cnh9-ckd79, brown-toad-md-0-98d489d87x8cnh9-hsgft, ...


    kubectl get cluster

    >   NAME         PHASE         AGE   VERSION
    >   brown-toad   Provisioned   26m


    kubectl delete cluster "${CLUSTER_NAME:?}"

    >   cluster.cluster.x-k8s.io "brown-toad" deleted
    >   ....
    >   ....

    #
    # Command hangs .....
    # Doesn't actually delete anything.
    #

    kubectl get cluster

    >   NAME         PHASE         AGE   VERSION
    >   brown-toad   Provisioned   26m


    clusterctl describe cluster "${CLUSTER_NAME:?}"

    >   NAME                                                           READY  SEVERITY  REASON                       SINCE  MESSAGE
    >   !! DELETED !! Cluster/brown-toad                               False  Warning   ScalingUp                    7h16m  Scaling up control plane to 3 replicas (actual 1)
    >   â”œâ”€ClusterInfrastructure - OpenStackCluster/brown-toad
    >   â”œâ”€ControlPlane - KubeadmControlPlane/brown-toad-control-plane  False  Warning   ScalingUp                    7h16m  Scaling up control plane to 3 replicas (actual 1)
    >   â”‚ â””â”€Machine/brown-toad-control-plane-5zm9f                     True                                          7h15m
    >   â””â”€Workers
    >     â””â”€MachineDeployment/brown-toad-md-0                          False  Warning   WaitingForAvailableMachines  7h18m  Minimum availability requires 3 replicas, current 0 available
    >       â””â”€3 Machines...                                            False  Error     InstanceStateError           7h13m  See brown-toad-md-0-98d489d87x8cnh9-ckd79, brown-toad-md-0-98d489d87x8cnh9-hsgft, ...




