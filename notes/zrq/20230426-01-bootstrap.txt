#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2023, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Try using the StackHPC helm charts.

    Result:

        Work in progress ...


# -----------------------------------------------------
# Check which platform is live.
#[user@desktop]

    ssh fedora@live.gaia-dmp.uk \
        '
        date
        hostname
        '

    >   Wed 26 Apr 10:54:37 UTC 2023
    >   iris-gaia-green-20230308-zeppelin


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    #
    # Live is green, selecting red for the deployment.
    # Using the 'admin' credentials to allow access to loadbalancers etc.
    #

    source "${HOME:?}/aglais.env"

    agcolour=red

    clientname=ansibler-${agcolour}
    cloudname=iris-gaia-${agcolour}-admin

    podman run \
        --rm \
        --tty \
        --interactive \
        --name     "${clientname:?}" \
        --hostname "${clientname:?}" \
        --env "cloudname=${cloudname:?}" \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK:?}:/mnt/ssh_auth_sock:rw,z" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        ghcr.io/wfau/atolmis/ansible-client:2022.07.25 \
        bash

    >   ....
    >   ....


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"

    >   real    2m3.560s
    >   user    0m52.685s
    >   sys     0m5.497s


# -----------------------------------------------------
# Add YAML editor role to our client container.
# TODO Add this to the Ansible client.
# https://github.com/wfau/atolmis/issues/30
#[root@ansibler]

    ansible-galaxy install kwoodson.yedit

    >   ....
    >   ....


# -----------------------------------------------------
# Create our deployment settings.
#[root@ansibler]

    deployname=${cloudname:?}-$(date '+%Y%m%d')
    deploydate=$(date '+%Y%m%dT%H%M%S')

    statusyml='/opt/aglais/aglais-status.yml'
    if [ ! -e "$(dirname ${statusyml})" ]
    then
        mkdir "$(dirname ${statusyml})"
    fi
    rm -f "${statusyml}"
    touch "${statusyml}"

    yq eval \
        --inplace \
        "
        .aglais.deployment.type = \"cluster-api\"   |
        .aglais.deployment.name = \"${deployname}\" |
        .aglais.deployment.date = \"${deploydate}\" |
        .aglais.openstack.cloud.name = \"${cloudname}\"
        " "${statusyml}"

    cat /opt/aglais/aglais-status.yml

    >   aglais:
    >     deployment:
    >       type: cluster-api
    >       name: iris-gaia-red-admin-20230426
    >       date: 20230426T121134
    >     openstack:
    >       cloud:
    >         name: iris-gaia-red-admin


# -----------------------------------------------------
# Create our bootstrap components.
#[root@ansibler]

    inventory=/deployments/cluster-api/bootstrap/ansible/config/inventory.yml

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/01-create-keypair.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/02-create-network.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/03-create-bootstrap.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/04-config-ansible.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/05-install-aglais.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/06-install-docker.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/07-install-kubectl.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/08-install-kind.yml'

#   ansible-playbook \
#       --inventory "${inventory:?}" \
#       '/deployments/cluster-api/bootstrap/ansible/09-install-helm.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/10-install-clusterctl.yml'

    ansible-playbook \
        --inventory "${inventory:?}" \
        '/deployments/cluster-api/bootstrap/ansible/11-install-yq.yml'

    cat /opt/aglais/aglais-status.yml

    >   aglais:
    >     deployment:
    >       date: 20230426T121134
    >       name: iris-gaia-red-admin-20230426
    >       type: cluster-api
    >     openstack:
    >       cloud:
    >         name: iris-gaia-red-admin
    >       keypairs:
    >         team:
    >           fingerprint: 2e:84:98:98:df:70:06:0e:4c:ed:bd:d4:d6:6b:eb:16
    >           id: iris-gaia-red-admin-20230426-keypair
    >           name: iris-gaia-red-admin-20230426-keypair
    >       networks:
    >         external:
    >           network:
    >             id: 57add367-d205-4030-a929-d75617a7c63e
    >             name: CUDN-Internet
    >         internal:
    >           network:
    >             id: 2755c2dc-6d1b-4bc5-a345-7529f90a6a60
    >             name: iris-gaia-red-admin-20230426-internal-network
    >           router:
    >             id: 32d33c31-0f16-41f5-8c25-99f176ca2d60
    >             name: iris-gaia-red-admin-20230426-internal-router
    >           subnet:
    >             cidr: 10.10.0.0/16
    >             id: 69df76ec-716c-4ed5-be53-e307ab782221
    >             name: iris-gaia-red-admin-20230426-internal-subnet
    >       servers:
    >         bootstrap:
    >           float:
    >             external: 128.232.227.65
    >             id: 02f293d2-ea04-47e0-8bb7-44e4dc79b8c0
    >             internal: 10.10.0.221
    >           server:
    >             address:
    >               ipv4: 10.10.0.221
    >             flavor:
    >               name: gaia.vm.cclake.2vcpu
    >             hostname: bootstrap
    >             id: c62b274d-ac19-4049-8ad8-3fbcdcd9933c
    >             image:
    >               id: e5c23082-cc34-4213-ad31-ff4684657691
    >               name: Fedora-34.1.2
    >             name: iris-gaia-red-admin-20230426-bootstrap


# -----------------------------------------------------
# -----------------------------------------------------
# Login to the bootstrap node as root.
#[root@ansibler]

    podman exec \
        -it \
        ansibler-red \
            bash

        ssh bootstrap

            sudo su -

    #
    # We could prefix everything with sudo, but it gets very boring.
    #


# -----------------------------------------------------
# Install Helm on the bootstrap node.
# https://helm.sh/docs/intro/install/
# https://github.com/helm/helm/releases
#[root@bootstrap]

    #
    # We still need to do this because our incude task doesn't handle tar files yet.
    #

    helmarch=linux-amd64
    helmversion=3.11.2
    helmtarfile=helm-v${helmversion}-${helmarch}.tar.gz
    helmtmpfile=/tmp/${helmtarfile:?}
    helmbinary=helm-${helmversion:?}

    curl \
        --location \
        --no-progress-meter \
        --output "${helmtmpfile:?}" \
        "https://get.helm.sh/${helmtarfile:?}"

    tar \
        --gzip \
        --extract \
        --directory /tmp \
        --file "${helmtmpfile:?}"

    pushd /usr/local/bin
        mv "/tmp/${helmarch:?}/helm" "${helmbinary:?}"
        chown 'root:root' "${helmbinary:?}"
        chmod 'u=rwx,g=rx,o=rx' "${helmbinary:?}"
        ln -s "${helmbinary:?}" 'helm'
    popd


# -----------------------------------------------------
# Create our initial Kind cluster.
# https://github.com/kubernetes-sigs/kind/pull/2478#issuecomment-1214656908
#[root@bootstrap]

    kind create cluster --retain

    >   Creating cluster "kind" ...
    >    âœ“ Ensuring node image (kindest/node:v1.25.3) ðŸ–¼
    >    âœ“ Preparing nodes ðŸ“¦
    >    âœ“ Writing configuration ðŸ“œ
    >    âœ“ Starting control-plane ðŸ•¹ï¸
    >    âœ“ Installing CNI ðŸ”Œ
    >    âœ“ Installing StorageClass ðŸ’¾
    >   ....
    >   ....


# -----------------------------------------------------
# Check the installed pods.
#[root@bootstrap]

    kubectl get pods --all-namespaces

    >   NAMESPACE            NAME                                         READY   STATUS    RESTARTS   AGE
    >   kube-system          coredns-565d847f94-mpnp6                     1/1     Running   0          23s
    >   kube-system          coredns-565d847f94-z5qk4                     1/1     Running   0          23s
    >   kube-system          etcd-kind-control-plane                      1/1     Running   0          37s
    >   kube-system          kindnet-jzwvc                                1/1     Running   0          23s
    >   kube-system          kube-apiserver-kind-control-plane            1/1     Running   0          37s
    >   kube-system          kube-controller-manager-kind-control-plane   1/1     Running   0          37s
    >   kube-system          kube-proxy-qm2gw                             1/1     Running   0          23s
    >   kube-system          kube-scheduler-kind-control-plane            1/1     Running   0          37s
    >   local-path-storage   local-path-provisioner-684f458cdd-qh8c4      1/1     Running   0          23s


# -----------------------------------------------------
# Create our Openstack management cluster
# https://cluster-api-openstack.sigs.k8s.io/getting-started.html#initialization-for-common-providers
# https://github.com/stackhpc/capi-helm-charts/tree/main/charts/openstack-cluster#prerequisites
#[root@bootstrap]

    clusterctl init --infrastructure openstack

    >   Fetching providers
    >   Installing cert-manager Version="v1.11.0"
    >   Waiting for cert-manager to be available...
    >   Installing Provider="cluster-api" Version="v1.4.1" TargetNamespace="capi-system"
    >   Installing Provider="bootstrap-kubeadm" Version="v1.4.1" TargetNamespace="capi-kubeadm-bootstrap-system"
    >   Installing Provider="control-plane-kubeadm" Version="v1.4.1" TargetNamespace="capi-kubeadm-control-plane-system"
    >   Installing Provider="infrastructure-openstack" Version="v0.7.1" TargetNamespace="capo-system"
    >   ....
    >   ....


# -----------------------------------------------------
# Check the installed pods.
#[root@bootstrap]

    kubectl get pods --all-namespaces

    >   NAMESPACE                           NAME                                                             READY   STATUS    RESTARTS   AGE
    >   capi-kubeadm-bootstrap-system       capi-kubeadm-bootstrap-controller-manager-8654485994-r4qf9       1/1     Running   0          86s
    >   capi-kubeadm-control-plane-system   capi-kubeadm-control-plane-controller-manager-5d9d9494d5-pkmqh   1/1     Running   0          84s
    >   capi-system                         capi-controller-manager-746b4f5db4-mk9gq                         1/1     Running   0          87s
    >   capo-system                         capo-controller-manager-775d744795-lqsn4                         1/1     Running   0          80s
    >   cert-manager                        cert-manager-99bb69456-d8zvg                                     1/1     Running   0          105s
    >   cert-manager                        cert-manager-cainjector-ffb4747bb-rfd9s                          1/1     Running   0          105s
    >   cert-manager                        cert-manager-webhook-545bd5d7d8-ktk2d                            1/1     Running   0          105s
    >   kube-system                         coredns-565d847f94-mpnp6                                         1/1     Running   0          10m
    >   kube-system                         coredns-565d847f94-z5qk4                                         1/1     Running   0          10m
    >   kube-system                         etcd-kind-control-plane                                          1/1     Running   0          11m
    >   kube-system                         kindnet-jzwvc                                                    1/1     Running   0          10m
    >   kube-system                         kube-apiserver-kind-control-plane                                1/1     Running   0          11m
    >   kube-system                         kube-controller-manager-kind-control-plane                       1/1     Running   0          11m
    >   kube-system                         kube-proxy-qm2gw                                                 1/1     Running   0          10m
    >   kube-system                         kube-scheduler-kind-control-plane                                1/1     Running   0          11m
    >   local-path-storage                  local-path-provisioner-684f458cdd-qh8c4                          1/1     Running   0          10m


# -----------------------------------------------------
# -----------------------------------------------------
# Extract the settings we need.
#[root@ansibler]

    openstack \
        --os-cloud "${cloudname:?}" \
        token issue \
            --format json \
    | tee /tmp/ostoken.json   \
    | jq '.'

    >   {
    >     "expires": "2023-04-26T13:17:12+0000",
    >     "id": "gAAAAABk....E2TQzaDh",
    >     "project_id": "0dd8cc5ee5a7455c8748cc06d04c93c3",
    >     "user_id": "5fa0c97a6dd14e01a3c7d91dad5c6b17"
    >   }

    osuserident=$(
        jq -r '.user_id' '/tmp/ostoken.json'
        )

    osprojectid=$(
        jq -r '.project_id' '/tmp/ostoken.json'
        )

    ctrlnodeflavor=gaia.vm.cclake.4vcpu
    nodenodeflavor=gaia.vm.cclake.4vcpu

    keypair=$(
        yq '.aglais.openstack.keypairs.team.name' /opt/aglais/aglais-status.yml
        )

    externalnet=$(
        openstack \
            --os-cloud "${cloudname:?}" \
            network list \
                --external \
                --format json \
        | jq -r ".[] | select(.Name == \"CUDN-Internet\") | .ID"
        )

    cat > /tmp/openstack-settings.env << EOF
export OPENSTACK_CLOUD=${cloudname:?}

export OPENSTACK_USER_ID=${osuserident:?}
export OPENSTACK_PROJECT_ID=${osprojectid:?}

export OPENSTACK_SSH_KEY_NAME=${keypair:?}
export OPENSTACK_EXTERNAL_NETWORK_ID=${externalnet:?}

export OPENSTACK_NODE_MACHINE_FLAVOR=${nodenodeflavor}
export OPENSTACK_CONTROL_PLANE_MACHINE_FLAVOR=${ctrlnodeflavor}

export KUBERNETES_VERSION=1.25.4
export OPENSTACK_IMAGE_NAME=gaia-dmp-ubuntu-2004-kube-v1.25.4

export OPENSTACK_FAILURE_DOMAIN=nova

# Use the Cambridge DNS servers.
# https://www.dns.cam.ac.uk/servers/rec.html
export OPENSTACK_DNS_NAMESERVERS=131.111.8.42

EOF


# -----------------------------------------------------
# Transfer the Openstack settings to our bootstrap node.
#[root@ansibler]

    scp \
        /tmp/openstack-settings.env \
        bootstrap:/tmp/openstack-settings.env

    ssh bootstrap \
        '
        sudo mkdir -p \
            /etc/aglais
        sudo install \
            /tmp/openstack-settings.env \
            /etc/aglais/openstack-settings.env
        '


# -----------------------------------------------------
# Transfer a copy of our clouds.yaml file.
#[root@ansibler]

    scp \
        /etc/openstack/clouds.yaml \
        bootstrap:/tmp/openstack-clouds.yaml

    ssh bootstrap \
        '
        sudo mkdir -p \
            /etc/aglais
        sudo install \
            /tmp/openstack-clouds.yaml \
            /etc/aglais/openstack-clouds.yaml
        '


# -----------------------------------------------------
# -----------------------------------------------------
# Load the Openstack settings from our client.
#[root@bootstrap]

    source /etc/aglais/openstack-settings.env

cat << EOF
OPENSTACK_CLOUD [${OPENSTACK_CLOUD}]
OPENSTACK_USER_ID [${OPENSTACK_USER_ID}]
OPENSTACK_PROJECT_ID [${OPENSTACK_PROJECT_ID}]
OPENSTACK_IMAGE_NAME [${OPENSTACK_IMAGE_NAME}]
EOF

    >   OPENSTACK_CLOUD [iris-gaia-red-admin]
    >   OPENSTACK_USER_ID [5fa0c97a6dd14e01a3c7d91dad5c6b17]
    >   OPENSTACK_PROJECT_ID [0dd8cc5ee5a7455c8748cc06d04c93c3]
    >   OPENSTACK_IMAGE_NAME [gaia-dmp-ubuntu-2004-kube-v1.25.4]

# -----------------------------------------------------
# TODO extract single cloud from clouds.yaml so we can edit it.
#

# -----------------------------------------------------
# Edit our clouds.yaml file to disable TLS certificate checks.
# https://docs.openstack.org/os-client-config/latest/user/configuration.html#ssl-settings
#[root@bootstrap]

    yq eval \
        --inplace \
        "
        .clouds.${OPENSTACK_CLOUD}.verify = \"false\"
        " '/etc/aglais/openstack-clouds.yaml'


# -----------------------------------------------------
# Edit our clouds.yaml file to add our project ID.
# https://github.com/stackhpc/capi-helm-charts/tree/main/charts/openstack-cluster#openstack-credentials
#[root@bootstrap]

    yq eval \
        --inplace \
        "
        .clouds.${OPENSTACK_CLOUD}.auth.project_id = \"${OPENSTACK_PROJECT_ID:?}\"
        " '/etc/aglais/openstack-clouds.yaml'


# -----------------------------------------------------
# Check our clouds.yaml file.
#[root@bootstrap]

    yq ".clouds.${OPENSTACK_CLOUD}" '/etc/aglais/openstack-clouds.yaml'

    >   auth:
    >     auth_url: https://arcus.openstack.hpc.cam.ac.uk:5000
    >     application_credential_id: "...."
    >     application_credential_secret: "...."
    >     project_id: 0dd8cc5ee5a7455c8748cc06d04c93c3
    >   region_name: "RegionOne"
    >   interface: "public"
    >   identity_api_version: 3
    >   auth_type: "v3applicationcredential"
    >   verify: "false"


# -----------------------------------------------------
# Create our cluster config.
# https://github.com/stackhpc/capi-helm-charts/tree/main/charts/openstack-cluster#managing-a-workload-cluster
#[root@bootstrap]

    touch '/etc/aglais/workload-cluster.yaml'

    yq eval \
        --inplace \
        "
        .cloudName = \"${OPENSTACK_CLOUD:?}\" |
        .kubernetesVersion = \"${KUBERNETES_VERSION:?}\" |
        .machineImage = \"${OPENSTACK_IMAGE_NAME:?}\" |
        .machineSSHKeyName = \"${OPENSTACK_SSH_KEY_NAME:?}\" |
        .controlPlane.machineFlavor = \"${OPENSTACK_CONTROL_PLANE_MACHINE_FLAVOR:?}\" |
        .nodeGroups.[0].name = \"md-0\" |
        .nodeGroups.[0].machineFlavor = \"${OPENSTACK_NODE_MACHINE_FLAVOR:?}\" |
        .nodeGroups.[0].machineCount = 4
        " '/etc/aglais/workload-cluster.yaml'

    yq '/etc/aglais/workload-cluster.yaml'

    >   cloudName: iris-gaia-red-admin
    >   kubernetesVersion: 1.25.4
    >   machineImage: gaia-dmp-ubuntu-2004-kube-v1.25.4
    >   machineSSHKeyName: iris-gaia-red-admin-20230426-keypair
    >   controlPlane:
    >     machineFlavor: gaia.vm.cclake.4vcpu
    >   nodeGroups:
    >     - name: md-0
    >       machineFlavor: gaia.vm.cclake.4vcpu
    >       machineCount: 4


# -----------------------------------------------------
# Add the StackHPC Helm repos.
#[root@bootstrap]

    helm repo add \
        capi \
        https://stackhpc.github.io/capi-helm-charts

    >   "capi" has been added to your repositories


    helm repo add \
        capi-addons \
        https://stackhpc.github.io/cluster-api-addon-provider

    >   "capi-addons" has been added to your repositories


    helm install \
        cluster-api-addon-provider \
        capi-addons/cluster-api-addon-provider

    >   NAME: cluster-api-addon-provider
    >   LAST DEPLOYED: Wed Apr 26 12:31:45 2023
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None

# -----------------------------------------------------
# Initialise our cluster ...
#[root@bootstrap]

    helm install \
        my-cluster \
        capi/openstack-cluster \
            --values '/etc/aglais/workload-cluster.yaml' \
            --values '/etc/aglais/openstack-clouds.yaml'

    >   NAME: my-cluster
    >   LAST DEPLOYED: Wed Apr 26 12:34:57 2023
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None

# -----------------------------------------------------
# Check the installed pods.
#[root@bootstrap]

    kubectl get pods --all-namespaces

    >   NAMESPACE                           NAME                                                             READY   STATUS              RESTARTS   AGE
    >   capi-kubeadm-bootstrap-system       capi-kubeadm-bootstrap-controller-manager-8654485994-r4qf9       1/1     Running             0          9m36s
    >   capi-kubeadm-control-plane-system   capi-kubeadm-control-plane-controller-manager-5d9d9494d5-pkmqh   1/1     Running             0          9m34s
    >   capi-system                         capi-controller-manager-746b4f5db4-mk9gq                         1/1     Running             0          9m37s
    >   capo-system                         capo-controller-manager-775d744795-lqsn4                         1/1     Running             0          9m30s
    >   cert-manager                        cert-manager-99bb69456-d8zvg                                     1/1     Running             0          9m55s
    >   cert-manager                        cert-manager-cainjector-ffb4747bb-rfd9s                          1/1     Running             0          9m55s
    >   cert-manager                        cert-manager-webhook-545bd5d7d8-ktk2d                            1/1     Running             0          9m55s
    >   default                             cluster-api-addon-provider-5cb78d8945-48jtf                      1/1     Running             0          3m35s
    >   default                             my-cluster-autoscaler-86c49ddb4b-24pct                           0/1     ContainerCreating   0          21s
    >   kube-system                         coredns-565d847f94-mpnp6                                         1/1     Running             0          18m
    >   kube-system                         coredns-565d847f94-z5qk4                                         1/1     Running             0          18m
    >   kube-system                         etcd-kind-control-plane                                          1/1     Running             0          19m
    >   kube-system                         kindnet-jzwvc                                                    1/1     Running             0          18m
    >   kube-system                         kube-apiserver-kind-control-plane                                1/1     Running             0          19m
    >   kube-system                         kube-controller-manager-kind-control-plane                       1/1     Running             0          19m
    >   kube-system                         kube-proxy-qm2gw                                                 1/1     Running             0          18m
    >   kube-system                         kube-scheduler-kind-control-plane                                1/1     Running             0          19m
    >   local-path-storage                  local-path-provisioner-684f458cdd-qh8c4                          1/1     Running             0          18m
    >   [root@iris-gaia-red-admin-20230426-bootstrap ~]#


    kubectl get cluster

    >   NAME         PHASE          AGE    VERSION
    >   my-cluster   Provisioning   104s


    clusterctl describe cluster 'my-cluster'

    >   NAME                                                           READY  SEVERITY  REASON                       SINCE  MESSAGE
    >   Cluster/my-cluster                                             False  Warning   ScalingUp                    2m30s  Scaling up control plane to 3 replicas (actual 0)
    >   â”œâ”€ClusterInfrastructure - OpenStackCluster/my-cluster
    >   â”œâ”€ControlPlane - KubeadmControlPlane/my-cluster-control-plane  False  Warning   ScalingUp                    2m30s  Scaling up control plane to 3 replicas (actual 0)
    >   â””â”€Workers
    >     â””â”€MachineDeployment/my-cluster-md-0                          False  Warning   WaitingForAvailableMachines  2m30s  Minimum availability requires 3 replicas, current 0 available
    >       â””â”€4 Machines...                                            False  Info      WaitingForInfrastructure     2m30s  See m--END--

    kubectl \
        --namespace capo-system \
        logs \
            -l control-plane=capo-controller-manager \
            -c manager \
            --follow

    >   ....
    >   E0426 12:36:21.015277       1 controller.go:326] "Reconciler error" err="failed to unmarshal clouds credentials stored in secret my-cluster-cloud-credentials: error unmarshaling JSON: while decoding JSON: json: cannot unmarshal string into Go struct field Cloud.clouds.verify of type bool" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/my-cluster-md-0-d21456b6-c2fsb" namespace="default" name="my-cluster-md-0-d21456b6-c2fsb" reconcileID=afa97050-f08f-4194-9d16-f339b1cea457
    >   E0426 12:36:22.338144       1 controller.go:326] "Reconciler error" err="failed to unmarshal clouds credentials stored in secret my-cluster-cloud-credentials: error unmarshaling JSON: while decoding JSON: json: cannot unmarshal string into Go struct field Cloud.clouds.verify of type bool" controller="openstackcluster" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackCluster" OpenStackCluster="default/my-cluster" namespace="default" name="my-cluster" reconcileID=6c9ee371-069a-4734-bbdb-24c2b1cad93e
    >   ....


    >   ....
    >   err="
    >       failed to unmarshal clouds credentials stored in secret my-cluster-cloud-credentials:
    >           error unmarshaling JSON: while decoding JSON:
    >               json: cannot unmarshal string into Go struct field Cloud.clouds.verify of type bool
    >       "
    >   controller="openstackmachine"
    >   controllerGroup="infrastructure.cluster.x-k8s.io"
    >   controllerKind="OpenStackMachine"
    >   OpenStackMachine="default/my-cluster-md-0-d21456b6-c2fsb"
    >   namespace="default" name="my-cluster-md-0-d21456b6-c2fsb"
    >   reconcileID=afa97050-f08f-4194-9d16-f339b1cea457
    >   ....



# -----------------------------------------------------
# Check our clouds.yaml file.
#[root@bootstrap]

    yq "." '/etc/aglais/openstack-clouds.yaml'

    >   clouds:
    >     ....
    >     ....
    >     iris-gaia-red-admin:
    >       auth:
    >         auth_url: https://arcus.openstack.hpc.cam.ac.uk:5000
    >         application_credential_id: "ef074975550a4fba889b807f679346c9"
    >         application_credential_secret: "eefoo2Oh Ik6Kohyu ayaeJi2R Eahaqu2v"
    >         project_id: 0dd8cc5ee5a7455c8748cc06d04c93c3
    >       region_name: "RegionOne"
    >       interface: "public"
    >       identity_api_version: 3
    >       auth_type: "v3applicationcredential"
    >       verify: "false"
    >   ....

    #
    # Gah!
    # Is this as simple as faluse rather than "false" ?
    #

    yq eval \
        --inplace \
        "
        .clouds.${OPENSTACK_CLOUD}.verify = false
        " '/etc/aglais/openstack-clouds.yaml'

    yq ".clouds.${OPENSTACK_CLOUD}" '/etc/aglais/openstack-clouds.yaml'

    >   auth:
    >     auth_url: https://arcus.openstack.hpc.cam.ac.uk:5000
    >     application_credential_id: "ef074975550a4fba889b807f679346c9"
    >     application_credential_secret: "eefoo2Oh Ik6Kohyu ayaeJi2R Eahaqu2v"
    >     project_id: 0dd8cc5ee5a7455c8748cc06d04c93c3
    >   region_name: "RegionOne"
    >   interface: "public"
    >   identity_api_version: 3
    >   auth_type: "v3applicationcredential"
    >   verify: false

# -----------------------------------------------------
# Try again ....
#[root@bootstrap]

    helm upgrade \
        my-cluster \
        capi/openstack-cluster \
            --values '/etc/aglais/workload-cluster.yaml' \
            --values '/etc/aglais/openstack-clouds.yaml'

    >   Release "my-cluster" has been upgraded. Happy Helming!
    >   NAME: my-cluster
    >   LAST DEPLOYED: Wed Apr 26 12:53:53 2023
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 2
    >   TEST SUITE: None


    kubectl \
        --namespace capo-system \
        logs \
            -l control-plane=capo-controller-manager \
            -c manager \
            --follow

    >   ....
    >   E0426 12:45:54.464272       1 controller.go:326] "Reconciler error" err="failed to unmarshal clouds credentials stored in secret my-cluster-cloud-credentials: error unmarshaling JSON: while decoding JSON: json: cannot unmarshal string into Go struct field Cloud.clouds.verify of type bool" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/my-cluster-md-0-d21456b6-c2fsb" namespace="default" name="my-cluster-md-0-d21456b6-c2fsb" reconcileID=b556067b-0922-4e7c-a846-4225c5d52495
    >   E0426 12:45:55.786564       1 controller.go:326] "Reconciler error" err="failed to unmarshal clouds credentials stored in secret my-cluster-cloud-credentials: error unmarshaling JSON: while decoding JSON: json: cannot unmarshal string into Go struct field Cloud.clouds.verify of type bool" controller="openstackcluster" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackCluster" OpenStackCluster="default/my-cluster" namespace="default" name="my-cluster" reconcileID=eff53429-619c-40c0-b764-c2b1ea1ab25c
    >   ....
    >   ....
    >   I0426 12:53:54.629225       1 http.go:96] "controller-runtime/webhook/webhooks: received request" webhook="/mutate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackcluster" UID=2a6a970c-7b93-4afa-bc88-ee9e2f7a0b06 kind="infrastructure.cluster.x-k8s.io/v1alpha6, Kind=OpenStackCluster" resource={Group:infrastructure.cluster.x-k8s.io Version:v1alpha6 Resource:openstackclusters}
    >   I0426 12:53:54.629856       1 http.go:143] "controller-runtime/webhook/webhooks: wrote response" webhook="/mutate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackcluster" code=200 reason= UID=2a6a970c-7b93-4afa-bc88-ee9e2f7a0b06 allowed=true
    >   I0426 12:53:54.636089       1 http.go:96] "controller-runtime/webhook/webhooks: received request" webhook="/validate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackcluster" UID=377adf32-760c-43b9-b3ba-d4a62f8a89f4 kind="infrastructure.cluster.x-k8s.io/v1alpha6, Kind=OpenStackCluster" resource={Group:infrastructure.cluster.x-k8s.io Version:v1alpha6 Resource:openstackclusters}
    >   I0426 12:53:54.636416       1 http.go:143] "controller-runtime/webhook/webhooks: wrote response" webhook="/validate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackcluster" code=200 reason= UID=377adf32-760c-43b9-b3ba-d4a62f8a89f4 allowed=true
    >   I0426 12:53:54.650413       1 http.go:96] "controller-runtime/webhook/webhooks: received request" webhook="/validate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackmachinetemplate" UID=2b35a712-8ed7-4651-8bee-ba9a16eb4eb6 kind="infrastructure.cluster.x-k8s.io/v1alpha6, Kind=OpenStackMachineTemplate" resource={Group:infrastructure.cluster.x-k8s.io Version:v1alpha6 Resource:openstackmachinetemplates}
    >   I0426 12:53:54.650735       1 http.go:143] "controller-runtime/webhook/webhooks: wrote response" webhook="/validate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackmachinetemplate" code=200 reason= UID=2b35a712-8ed7-4651-8bee-ba9a16eb4eb6 allowed=true
    >   I0426 12:53:54.661070       1 http.go:96] "controller-runtime/webhook/webhooks: received request" webhook="/validate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackmachinetemplate" UID=c11c2584-a3fd-488e-8489-0d25acfe465d kind="infrastructure.cluster.x-k8s.io/v1alpha6, Kind=OpenStackMachineTemplate" resource={Group:infrastructure.cluster.x-k8s.io Version:v1alpha6 Resource:openstackmachinetemplates}
    >   I0426 12:53:54.661388       1 http.go:143] "controller-runtime/webhook/webhooks: wrote response" webhook="/validate-infrastructure-cluster-x-k8s-io-v1alpha6-openstackmachinetemplate" code=200 reason= UID=c11c2584-a3fd-488e-8489-0d25acfe465d allowed=true
    >   ....


    kubectl get cluster

    >   NAME         PHASE          AGE   VERSION
    >   my-cluster   Provisioning   20m


    clusterctl describe cluster 'my-cluster'

    >   NAME                                                           READY  SEVERITY  REASON                           SINCE  MESSAGE
    >   Cluster/my-cluster                                             False  Warning   ScalingUp                        20m    Scaling up control plane to 3 replicas (actual 0)
    >   â”œâ”€ClusterInfrastructure - OpenStackCluster/my-cluster
    >   â”œâ”€ControlPlane - KubeadmControlPlane/my-cluster-control-plane  False  Warning   ScalingUp                        20m    Scaling up control plane to 3 replicas (actual 0)
    >   â””â”€Workers
    >     â””â”€MachineDeployment/my-cluster-md-0                          False  Warning   WaitingForAvailableMachines      20m    Minimum availability requires 3 replicas, current 0 available
    >       â””â”€4 Machines...                                            False  Info      WaitingForClusterInfrastructure  52s    See my-cluster-md-0-56cffbb7cbx8dqc4-5kbkq, my-cluster-md-0-56cffbb7cbx8dqc4-k7hfc, ...


    kubectl \
        --namespace capo-system \
        logs \
            -l control-plane=capo-controller-manager \
            -c manager \
            --follow

    >   ....
    >   E0426 12:56:53.312242       1 controller.go:326] "Reconciler error" err="failed to reconcile external network: found 2 external networks, which should not happen" controller="openstackcluster" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackCluster" OpenStackCluster="default/my-cluster" namespace="default" name="my-cluster" reconcileID=a7fc7e23-240d-45fc-a486-152cf2e90cda
    >   ....

    >   ....
    >   err="failed to reconcile external network: found 2 external networks, which should not happen"
    >   controller="openstackcluster"
    >   controllerGroup="infrastructure.cluster.x-k8s.io"
    >   controllerKind="OpenStackCluster"
    >   OpenStackCluster="default/my-cluster"
    >   namespace="default"
    >   name="my-cluster"
    >   reconcileID=a7fc7e23-240d-45fc-a486-152cf2e90cda
    >   ....

# -----------------------------------------------------
# Fix our cluster config.
# https://github.com/stackhpc/capi-helm-charts/tree/main/charts/openstack-cluster#managing-a-workload-cluster
#[root@bootstrap]

    yq eval \
        --inplace \
        "
        .dnsNameservers = \"${OPENSTACK_DNS_NAMESERVERS:?}\" |
        .externalNetworkId = \"${OPENSTACK_EXTERNAL_NETWORK_ID:?}\"
        " '/etc/aglais/workload-cluster.yaml'

    yq '/etc/aglais/workload-cluster.yaml'

    >   cloudName: iris-gaia-red-admin
    >   kubernetesVersion: 1.25.4
    >   machineImage: gaia-dmp-ubuntu-2004-kube-v1.25.4
    >   machineSSHKeyName: iris-gaia-red-admin-20230426-keypair
    >   controlPlane:
    >     machineFlavor: gaia.vm.cclake.4vcpu
    >   nodeGroups:
    >     - name: md-0
    >       machineFlavor: gaia.vm.cclake.4vcpu
    >       machineCount: 4
    >   dnsNameservers: 131.111.8.42
    >   externalNetworkId: 57add367-d205-4030-a929-d75617a7c63e


# -----------------------------------------------------
# Try again ....
#[root@bootstrap]

    helm upgrade \
        my-cluster \
        capi/openstack-cluster \
            --values '/etc/aglais/workload-cluster.yaml' \
            --values '/etc/aglais/openstack-clouds.yaml'

    >   Release "my-cluster" has been upgraded. Happy Helming!
    >   NAME: my-cluster
    >   LAST DEPLOYED: Wed Apr 26 13:05:18 2023
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 3
    >   TEST SUITE: None


    kubectl \
        --namespace capo-system \
        logs \
            -l control-plane=capo-controller-manager \
            -c manager \
            --follow

    >   ....
    >   I0426 13:11:11.374641       1 openstackmachine_controller.go:311] "Cluster infrastructure is not ready yet, requeuing machine" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/my-cluster-md-0-d21456b6-tfktj" namespace="default" name="my-cluster-md-0-d21456b6-tfktj" reconcileID=678bd72f-1585-4e3e-8ca4-506a907dceae openStackMachine="my-cluster-md-0-d21456b6-tfktj" machine="my-cluster-md-0-56cffbb7cbx8dqc4-5kbkq" cluster="my-cluster" openStackCluster="my-cluster"
    >   I0426 13:11:26.350320       1 openstackmachine_controller.go:311] "Cluster infrastructure is not ready yet, requeuing machine" controller="openstackmachine" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackMachine" OpenStackMachine="default/my-cluster-md-0-d21456b6-c2fsb" namespace="default" name="my-cluster-md-0-d21456b6-c2fsb" reconcileID=999aa33f-7435-4fd7-8014-0c827d021266 openStackMachine="my-cluster-md-0-d21456b6-c2fsb" machine="my-cluster-md-0-56cffbb7cbx8dqc4-nzvcg" cluster="my-cluster" openStackCluster="my-cluster"
    >   ....

    #
    # Everything wating for everything else ...
    # No errors, but no progress either.
    #


# -----------------------------------------------------
# Try again ....
#[root@bootstrap]

    helm uninstall \
        my-cluster

    >           my-cluster
    >   These resources were kept due to the resource policy:
    >   [Secret] my-cluster-cloud-credentials
    >   [KubeadmConfigTemplate] my-cluster-md-0-99910806
    >   [KubeadmControlPlane] my-cluster-control-plane
    >   [OpenStackCluster] my-cluster
    >   [OpenStackMachineTemplate] my-cluster-control-plane-d21456b6
    >   [OpenStackMachineTemplate] my-cluster-md-0-d21456b6
    >   
    >   release "my-cluster" uninstalled


    helm install \
        my-cluster \
        capi/openstack-cluster \
            --values '/etc/aglais/workload-cluster.yaml' \
            --values '/etc/aglais/openstack-clouds.yaml'

    >   NAME: my-cluster
    >   LAST DEPLOYED: Wed Apr 26 13:17:31 2023
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None


    kubectl \
        --namespace capo-system \
        logs \
            -l control-plane=capo-controller-manager \
            -c manager \
            --follow

    >   ....
    >   I0426 13:18:13.902905       1 openstackcluster_controller.go:427] "Reconciling network components" controller="openstackcluster" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackCluster" OpenStackCluster="default/my-cluster" namespace="default" name="my-cluster" reconcileID=0ae93921-500c-43d9-901f-9e575e01d42c cluster="my-cluster"
    >   E0426 13:18:14.384293       1 controller.go:326] "Reconciler error" err="failed to reconcile external network: found 2 external networks, which should not happen" controller="openstackcluster" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackCluster" OpenStackCluster="default/my-cluster" namespace="default" name="my-cluster" reconcileID=0ae93921-500c-43d9-901f-9e575e01d42c
    >   ....

    #
    # Guess ... some of the retained resources are blocking ?
    #

# -----------------------------------------------------
# Try again ....
#[root@bootstrap]

    helm install \
        another-cluster \
        capi/openstack-cluster \
            --values '/etc/aglais/workload-cluster.yaml' \
            --values '/etc/aglais/openstack-clouds.yaml'

    >   NAME: another-cluster
    >   LAST DEPLOYED: Wed Apr 26 13:39:27 2023
    >   NAMESPACE: default
    >   STATUS: deployed
    >   REVISION: 1
    >   TEST SUITE: None


    kubectl \
        --namespace capo-system \
        logs \
            -l control-plane=capo-controller-manager \
            -c manager \
            --follow

    >   ....
    >   E0426 13:40:05.912991       1 controller.go:326] "Reconciler error" err="failed to reconcile external network: found 2 external networks, which should not happen" controller="openstackcluster" controllerGroup="infrastructure.cluster.x-k8s.io" controllerKind="OpenStackCluster" OpenStackCluster="default/another-cluster" namespace="default" name="another-cluster" reconcileID=9dbcb3fa-9b48-47b7-ad53-91f4aaee63b8
    >   ....

    #
    # Try using clusterctl to generate a cluster config
    # and see how it treats the external network.
    #

# -----------------------------------------------------
# Load our Openstack settings.
#[root@bootstrap]

    source /etc/aglais/openstack-settings.env

cat << EOF
OPENSTACK_CLOUD [${OPENSTACK_CLOUD}]
OPENSTACK_IMAGE_NAME [${OPENSTACK_IMAGE_NAME}]
OPENSTACK_EXTERNAL_NETWORK_ID [${OPENSTACK_EXTERNAL_NETWORK_ID}]
EOF

    >   OPENSTACK_CLOUD [iris-gaia-red-admin]
    >   OPENSTACK_IMAGE_NAME [gaia-dmp-ubuntu-2004-kube-v1.25.4]
    >   OPENSTACK_EXTERNAL_NETWORK_ID [57add367-d205-4030-a929-d75617a7c63e]


# -----------------------------------------------------
# Use the script provided by cluster-api-provider-openstack to parse our clouds.yaml file.
# https://cluster-api-openstack.sigs.k8s.io/clusteropenstack/configuration.html#generate-credentials
# https://github.com/kubernetes-sigs/cluster-api-provider-openstack/blob/main/docs/book/src/clusteropenstack/configuration.md#generate-credentials
#[root@bootstrap]

    curl \
        --location \
        --no-progress-meter \
        --output '/tmp/env.rc' \
        'https://raw.githubusercontent.com/kubernetes-sigs/cluster-api-provider-openstack/master/templates/env.rc'

    source '/tmp/env.rc' '/etc/aglais/openstack-clouds.yaml' "${OPENSTACK_CLOUD:?}"


# -----------------------------------------------------
# Generate our external cluster config.
# https://cluster-api.sigs.k8s.io/clusterctl/commands/generate-cluster.html
# https://cluster-api-openstack.sigs.k8s.io/getting-started.html#generating-the-cluster-configuration
#[root@bootstrap]

    CLUSTER_NAME=brown-toad

    clusterctl generate cluster \
        "${CLUSTER_NAME:?}" \
        --flavor external-cloud-provider \
        --kubernetes-version "${KUBERNETES_VERSION:?}" \
        --control-plane-machine-count 3 \
        --worker-machine-count 3 \
    | tee "/tmp/${CLUSTER_NAME:?}.yaml"


    >   ....
    >   apiVersion: infrastructure.cluster.x-k8s.io/v1alpha6
    >   kind: OpenStackCluster
    >   metadata:
    >     name: brown-toad
    >     namespace: default
    >   spec:
    >     apiServerLoadBalancer:
    >       enabled: true
    >     cloudName: iris-gaia-red-admin
    >     dnsNameservers:
    >     - 131.111.8.42
    >     externalNetworkId: 57add367-d205-4030-a929-d75617a7c63e
    >     identityRef:
    >       kind: Secret
    >       name: brown-toad-cloud-config
    >     managedSecurityGroups: true
    >     nodeCidr: 10.6.0.0/24
    >   ....


    kubectl get pods --all-namespaces

    >   NAMESPACE                           NAME                                                             READY   STATUS              RESTARTS   AGE
    >   capi-kubeadm-bootstrap-system       capi-kubeadm-bootstrap-controller-manager-8654485994-r4qf9       1/1     Running             0          91m
    >   capi-kubeadm-control-plane-system   capi-kubeadm-control-plane-controller-manager-5d9d9494d5-pkmqh   1/1     Running             0          91m
    >   capi-system                         capi-controller-manager-746b4f5db4-mk9gq                         1/1     Running             0          91m
    >   capo-system                         capo-controller-manager-775d744795-lqsn4                         1/1     Running             0          91m
    >   cert-manager                        cert-manager-99bb69456-d8zvg                                     1/1     Running             0          91m
    >   cert-manager                        cert-manager-cainjector-ffb4747bb-rfd9s                          1/1     Running             0          91m
    >   cert-manager                        cert-manager-webhook-545bd5d7d8-ktk2d                            1/1     Running             0          91m
    >   default                             another-cluster-autoscaler-7ff7cb86cc-tp4qq                      0/1     ContainerCreating   0          17m
    >   default                             cluster-api-addon-provider-5cb78d8945-48jtf                      1/1     Running             0          85m
    >   default                             my-cluster-autoscaler-86c49ddb4b-mpv2b                           0/1     ContainerCreating   0          39m
    >   kube-system                         coredns-565d847f94-mpnp6                                         1/1     Running             0          100m
    >   kube-system                         coredns-565d847f94-z5qk4                                         1/1     Running             0          100m
    >   kube-system                         etcd-kind-control-plane                                          1/1     Running             0          100m
    >   kube-system                         kindnet-jzwvc                                                    1/1     Running             0          100m
    >   kube-system                         kube-apiserver-kind-control-plane                                1/1     Running             0          100m
    >   kube-system                         kube-controller-manager-kind-control-plane                       1/1     Running             0          100m
    >   kube-system                         kube-proxy-qm2gw                                                 1/1     Running             0          100m
    >   kube-system                         kube-scheduler-kind-control-plane                                1/1     Running             0          100m
    >   local-path-storage                  local-path-provisioner-684f458cdd-qh8c4                          1/1     Running             0          100m

